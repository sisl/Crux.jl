var documenterSearchIndex = {"docs":
[{"location":"interface/#Library/Interface","page":"Library/Interface","title":"Library/Interface","text":"","category":"section"},{"location":"interface/","page":"Library/Interface","title":"Library/Interface","text":"This section details the interface and functions provided by Crux.jl.","category":"page"},{"location":"interface/#Contents","page":"Library/Interface","title":"Contents","text":"","category":"section"},{"location":"interface/","page":"Library/Interface","title":"Library/Interface","text":"Pages = [\"interface.md\"]","category":"page"},{"location":"interface/#Index","page":"Library/Interface","title":"Index","text":"","category":"section"},{"location":"interface/","page":"Library/Interface","title":"Library/Interface","text":"Pages = [\"interface.md\"]","category":"page"},{"location":"interface/#Reinforcement-Learning","page":"Library/Interface","title":"Reinforcement Learning","text":"","category":"section"},{"location":"interface/","page":"Library/Interface","title":"Library/Interface","text":"RL interfaces are categorized by on-policy and off-policy below.","category":"page"},{"location":"interface/#On-Policy","page":"Library/Interface","title":"On-Policy","text":"","category":"section"},{"location":"interface/#Crux.OnPolicySolver","page":"Library/Interface","title":"Crux.OnPolicySolver","text":"On policy solver type.\n\nFields\n\nagent::PolicyParams Policy parameters (PolicyParams)\nS::AbstractSpace State space\nN::Int = 1000 Number of environment interactions\nŒîN::Int = 200 Number of interactions between updates\nmax_steps::Int = 100 Maximum number of steps per episode\nlog::Union{Nothing, LoggerParams} = nothing The logging parameters\ni::Int = 0 The current number of environment interactions\nparam_optimizers::Dict{Any, TrainingParams} = Dict() Training parameters for the parameters\na_opt::TrainingParams Training parameters for the actor\nc_opt::Union{Nothing, TrainingParams} = nothing Training parameters for the critic\nùí´::NamedTuple = (;) Parameters of the algorithm\ninteraction_storage = nothing If this is initialized to an array then it will store all interactions\npost_sample_callback = (ùíü; kwargs...) -> nothing Callback that that happens after sampling experience\npost_batch_callback = (ùíü; kwargs...) -> nothing Callback that that happens after sampling a batch\n\nOn-policy-specific parameters\n\nŒª_gae::Float32 = 0.95 Generalized advantage estimation parameter\nrequired_columns = Symbol[] Extra data columns to store\n\nParameters specific to cost constraints (a separate value network)\n\nVc::Union{ContinuousNetwork, Nothing} = nothing Cost value approximator\ncost_opt::Union{Nothing, TrainingParams} = nothing Training parameters for the cost value\n\n\n\n\n\n","category":"type"},{"location":"interface/#Crux.a2c_loss","page":"Library/Interface","title":"Crux.a2c_loss","text":"A2C loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.A2C","page":"Library/Interface","title":"Crux.A2C","text":"Advantage actor critic (A2C) solver.\n\nA2C(;\n    œÄ::ActorCritic, \n    a_opt::NamedTuple=(;), \n    c_opt::NamedTuple=(;), \n    log::NamedTuple=(;), \n    Œªp::Float32=1f0, \n    Œªe::Float32=0.1f0, \n    required_columns=[],\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.ppo_loss","page":"Library/Interface","title":"Crux.ppo_loss","text":"PPO loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.PPO","page":"Library/Interface","title":"Crux.PPO","text":"Proximal policy optimization (PPO) solver.\n\nPPO(;\n    œÄ::ActorCritic,\n    œµ::Float32 = 0.2f0,\n    Œªp::Float32 = 1f0,\n    Œªe::Float32 = 0.1f0,\n    target_kl = 0.012f0,\n    a_opt::NamedTuple=(;),\n    c_opt::NamedTuple=(;),\n    log::NamedTuple=(;),\n    required_columns=[],\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.lagrange_ppo_loss","page":"Library/Interface","title":"Crux.lagrange_ppo_loss","text":"PPO loss with a penalty.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.LagrangePPO","page":"Library/Interface","title":"Crux.LagrangePPO","text":"Lagrange-Constrained PPO solver.\n\nLagrangePPO(;\n    œÄ::ActorCritic,\n    Vc::ContinuousNetwork, # value network for estimating cost\n    œµ::Float32 = 0.2f0,\n    Œªp::Float32 = 1f0,\n    Œªe::Float32 = 0.1f0,\n    Œª_gae = 0.95f0,\n    target_kl = 0.012f0,\n    target_cost = 0.025f0,\n    penalty_scale = 1f0,\n    penalty_max = Inf32,\n    Ki_max = 10f0,\n    Ki = 1f-3,\n    Kp = 1,\n    Kd = 0,\n    ema_Œ± = 0.95,\n    a_opt::NamedTuple=(;),\n    c_opt::NamedTuple=(;),\n    cost_opt::NamedTuple=(;),\n    log::NamedTuple=(;),\n    required_columns=[],\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.reinforce_loss","page":"Library/Interface","title":"Crux.reinforce_loss","text":"REINFORCE loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.REINFORCE","page":"Library/Interface","title":"Crux.REINFORCE","text":"REINFORCE solver.\n\nREINFORCE(;\n    œÄ,\n    a_opt::NamedTuple=(;), \n    log::NamedTuple=(;),\n    required_columns=[],\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Off-Policy","page":"Library/Interface","title":"Off-Policy","text":"","category":"section"},{"location":"interface/#Crux.OffPolicySolver","page":"Library/Interface","title":"Crux.OffPolicySolver","text":"Off policy solver type.\n\nFields\n\nagent::PolicyParams # Policy parameters (PolicyParams)\nS::AbstractSpace # State space\nN::Int = 1000 # Number of environment interactions\nŒîN::Int = 4 # Number of interactions between updates\nmax_steps::Int = 100 # Maximum number of steps per episode\nlog::Union{Nothing, LoggerParams} = nothing # The logging parameters\ni::Int = 0 # The current number of environment interactions\nparam_optimizers::Dict{Any, TrainingParams} = Dict() # Training parameters for the parameters\na_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the actor\nc_opt::TrainingParams # Training parameters for the critic\nùí´::NamedTuple = (;) # Parameters of the algorithm\ninteraction_storage = nothing # If this is initialized to an array then it will store all interactions\npost_sample_callback = (ùíü; kwargs...) -> nothing # Callback that that happens after sampling experience\n\nOff-policy-specific parameters\n\npost_batch_callback = (ùíü; kwargs...) -> nothing Callback that that happens after sampling a batch\npre_train_callback = (ùíÆ; kwargs...) -> nothing callback that gets called once prior to training\ntarget_update = (œÄ‚Åª, œÄ; kwargs...) -> polyak_average!(œÄ‚Åª, œÄ, 0.005f0) Function for updating the target network\ntarget_fn Target for critic regression with input signature (œÄ‚Åª, ùíü, Œ≥; i)\nbuffer_size = 1000 Size of the buffer\nrequired_columns = Symbol[]\nbuffer = ExperienceBuffer(S, agent.space, buffer_size, required_columns) The replay buffer\npriority_fn = td_error function for prioritized replay\nbuffer_init::Int = max(c_opt.batch_size, 200) Number of observations to initialize the buffer with\nextra_buffers = [] extra buffers (i.e. for experience replay in continual learning)\nbuffer_fractions = [1.0] Fraction of the minibatch devoted to each buffer\n\n\n\n\n\n","category":"type"},{"location":"interface/#Crux.ddpg_target","page":"Library/Interface","title":"Crux.ddpg_target","text":"DDPG target function.\n\nSet y·µ¢ = r·µ¢ + Œ≥Q‚Ä≤(s·µ¢‚Çä‚ÇÅ, Œº‚Ä≤(s·µ¢‚Çä‚ÇÅ | Œ∏·µò‚Ä≤) | Œ∏·∂ú‚Ä≤)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.smoothed_ddpg_target","page":"Library/Interface","title":"Crux.smoothed_ddpg_target","text":"Smooth DDPG target.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.ddpg_actor_loss","page":"Library/Interface","title":"Crux.ddpg_actor_loss","text":"DDPG actor loss function.\n\n‚àá_Œ∏·µò ùêΩ ‚âà 1/ùëÅ Œ£·µ¢ ‚àá‚ÇêQ(s, a | Œ∏·∂ú)|‚Çõ‚Çå‚Çõ·µ¢, ‚Çê‚Çå·µ§‚Çç‚Çõ·µ¢‚Çé ‚àá_Œ∏·µò Œº(s | Œ∏·µò)|‚Çõ·µ¢\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.DDPG","page":"Library/Interface","title":"Crux.DDPG","text":"Deep deterministic policy gradient (DDPG) solver.\n\nT. P. Lillicrap, et al., \"Continuous control with deep reinforcement learning\", ICLR 2016.\n\nDDPG(;\n    œÄ::ActorCritic, \n    ŒîN=50, \n    œÄ_explore=GaussianNoiseExplorationPolicy(0.1f0),  \n    a_opt::NamedTuple=(;), \n    c_opt::NamedTuple=(;),\n    a_loss=ddpg_actor_loss,\n    c_loss=td_loss(),\n    target_fn=ddpg_target,\n    prefix=\"\",\n    log::NamedTuple=(;), \n    œÄ_smooth=GaussianNoiseExplorationPolicy(0.1f0, œµ_min=-0.5f0, œµ_max=0.5f0), kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.dqn_target","page":"Library/Interface","title":"Crux.dqn_target","text":"DQN target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.DQN","page":"Library/Interface","title":"Crux.DQN","text":"Deep Q-learning (DQN) solver.\n\nV. Mnih, et al., \"Human-level control through deep reinforcement learning\", Nature 2015.\n\nDQN(;\n      œÄ::DiscreteNetwork, \n      N::Int, \n      ŒîN=4, \n      œÄ_explore=œµGreedyPolicy(LinearDecaySchedule(1., 0.1, floor(Int, N/2)), œÄ.outputs), \n      c_opt::NamedTuple=(;), \n      log::NamedTuple=(;),\n      c_loss=td_loss(),\n      target_fn=dqn_target,\n      prefix=\"\",\n      kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.sac_target","page":"Library/Interface","title":"Crux.sac_target","text":"SAC target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.sac_deterministic_target","page":"Library/Interface","title":"Crux.sac_deterministic_target","text":"Deterministic SAC target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.sac_max_q_target","page":"Library/Interface","title":"Crux.sac_max_q_target","text":"Max-Q SAC target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.sac_actor_loss","page":"Library/Interface","title":"Crux.sac_actor_loss","text":"SAC actor loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.sac_temp_loss","page":"Library/Interface","title":"Crux.sac_temp_loss","text":"SAC temp-based loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.SAC","page":"Library/Interface","title":"Crux.SAC","text":"Soft Actor Critic (SAC) solver.\n\nSAC(;\n    œÄ::ActorCritic{T, DoubleNetwork{ContinuousNetwork, ContinuousNetwork}},\n    ŒîN=50,\n    SAC_Œ±::Float32=1f0,\n    SAC_H_target::Float32 = Float32(-prod(dim(action_space(œÄ)))),\n    œÄ_explore=GaussianNoiseExplorationPolicy(0.1f0),\n    SAC_Œ±_opt::NamedTuple=(;),\n    a_opt::NamedTuple=(;),\n    c_opt::NamedTuple=(;),\n    a_loss=sac_actor_loss,\n    c_loss=double_Q_loss(),\n    target_fn=sac_target(œÄ),\n    prefix=\"\",\n    log::NamedTuple=(;),\n    ùí´::NamedTuple=(;),\n    param_optimizers=Dict(),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.softq_target","page":"Library/Interface","title":"Crux.softq_target","text":"Soft Q-learning target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.SoftQ","page":"Library/Interface","title":"Crux.SoftQ","text":"Soft Q-learning solver.\n\nSoftQ(;\n    œÄ::DiscreteNetwork, \n    N::Int, \n    ŒîN=4, \n    c_opt::NamedTuple=(;epochs=4), \n    log::NamedTuple=(;),\n    c_loss=td_loss(),\n    Œ±=Float32(1.),\n    prefix=\"\",\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.td3_target","page":"Library/Interface","title":"Crux.td3_target","text":"TD3 target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.td3_actor_loss","page":"Library/Interface","title":"Crux.td3_actor_loss","text":"TD3 actor loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.TD3","page":"Library/Interface","title":"Crux.TD3","text":"Twin Delayed DDPG (TD3) solver.\n\nTD3(;\n    œÄ,\n    ŒîN=50,\n    œÄ_smooth::Policy=GaussianNoiseExplorationPolicy(0.1f0, œµ_min=-0.5f0, œµ_max=0.5f0),\n    œÄ_explore=GaussianNoiseExplorationPolicy(0.1f0),\n    a_opt::NamedTuple=(;),\n    c_opt::NamedTuple=(;),\n    a_loss=td3_actor_loss,\n    c_loss=double_Q_loss(),\n    target_fn=td3_target,\n    prefix=\"\",\n    log::NamedTuple=(;),\n    ùí´::NamedTuple=(;),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Imitation-Learning","page":"Library/Interface","title":"Imitation Learning","text":"","category":"section"},{"location":"interface/#Crux.AdRIL","page":"Library/Interface","title":"Crux.AdRIL","text":"Adversarial Reward-moment Imitation Learning (AdRIL) solver.\n\nAdRIL(;\n    œÄ, \n    S,\n    ŒîN=50,\n    solver=SAC, \n    ùíü_demo, \n    normalize_demo::Bool=true,\n    expert_frac=0.5, \n    buffer_size = 1000, \n    buffer_init=0,\n    log::NamedTuple=(;),\n    buffer::ExperienceBuffer = ExperienceBuffer(S, action_space(œÄ), buffer_size, [:i]), \n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.AdVIL","page":"Library/Interface","title":"Crux.AdVIL","text":"Adversarial Value Moment Imitation Learning (AdVIL) solver.\n\nAdVIL(;\n    œÄ, \n    S,\n    ùíü_demo, \n    normalize_demo::Bool=true, \n    Œª_GP::Float32=10f0, \n    Œª_orth::Float32=1f-4, \n    Œª_BC::Float32=2f-1, \n    a_opt::NamedTuple=(;), \n    c_opt::NamedTuple=(;), \n    log::NamedTuple=(;), \n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.ASAF","page":"Library/Interface","title":"Crux.ASAF","text":"Adversarial Soft Advantage Fitting (ASAF) solver.\n\nASAF(;\n    œÄ,\n    S,\n    ùíü_demo,\n    normalize_demo::Bool=true,\n    ŒîN=50,\n    Œª_orth=1f-4,\n    a_opt::NamedTuple=(;),\n    c_opt::NamedTuple=(;),\n    log::NamedTuple=(;),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.BC","page":"Library/Interface","title":"Crux.BC","text":"Behavioral cloning solver.\n\nBC(;\n    œÄ,\n    S,\n    ùíü_demo,\n    normalize_demo::Bool=true,\n    loss=nothing,\n    validation_fraction=0.3,\n    window=100,\n    Œªe::Float32=1f-3,\n    opt::NamedTuple=(;),\n    log::NamedTuple=(;),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.OnlineIQLearn","page":"Library/Interface","title":"Crux.OnlineIQLearn","text":"Online Inverse Q-Learning solver.\n\nOnlineIQLearn(;\n    œÄ, \n    S, \n    ùíü_demo, \n    Œ≥=Float32(0.9),\n    normalize_demo::Bool=true, \n    solver=SoftQ, # or SAC for continuous states \n    log::NamedTuple=(;period=500), \n    reg::Bool=true,\n    Œ±_reg=Float32(0.5),\n    gp::Bool=true,\n    Œª_gp=Float32(10.),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.OffPolicyGAIL","page":"Library/Interface","title":"Crux.OffPolicyGAIL","text":"Off-policy generative adversarial imitation learning (GAIL) solver.\n\nOffPolicyGAIL(;\n    œÄ,\n    S, \n    ùíü_demo, \n    ùíü_ndas::Array{ExperienceBuffer} = ExperienceBuffer[], \n    normalize_demo::Bool=true, \n    D::ContinuousNetwork, \n    solver=SAC, \n    d_opt::NamedTuple=(epochs=5,), \n    log::NamedTuple=(;), \n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.OnPolicyGAIL","page":"Library/Interface","title":"Crux.OnPolicyGAIL","text":"On-policy generative adversarial imitation learning (GAIL) solver.\n\nOnPolicyGAIL(;\n    œÄ,\n    S,\n    Œ≥,\n    Œª_gae::Float32 = 0.95f0,\n    ùíü_demo,\n    Œ±r::Float32 = 0.5f0,\n    normalize_demo::Bool=true,\n    D::ContinuousNetwork,\n    solver=PPO,\n    gan_loss::GANLoss=GAN_BCELoss(),\n    d_opt::NamedTuple=(;),\n    log::NamedTuple=(;),\n    Rscale=1f0,\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.SQIL","page":"Library/Interface","title":"Crux.SQIL","text":"Soft Q Imitation Learning (SQIL) solver.\n\nSQIL(;\n    œÄ, \n    S, \n    ùíü_demo, \n    normalize_demo::Bool=true, \n    solver=SAC, \n    log::NamedTuple=(;), \n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Adversarial-RL","page":"Library/Interface","title":"Adversarial RL","text":"","category":"section"},{"location":"interface/#Crux.AdversarialOffPolicySolver","page":"Library/Interface","title":"Crux.AdversarialOffPolicySolver","text":"Adversarial off-policy solver.\n\nùíÆ_pro::OffPolicySolver Solver parameters for the protagonist\nùíÆ_ant::OffPolicySolver Solver parameters for the antagonist\npx::PolicyParams Nominal disturbance policy\ntrain_pro_every::Int = 1\ntrain_ant_every::Int = 1\nlog::Union{Nothing, LoggerParams} = nothing The logging parameters\ni::Int = 0 The current number of environment interactions\n\n\n\n\n\n","category":"type"},{"location":"interface/#Crux.RARL_DQN","page":"Library/Interface","title":"Crux.RARL_DQN","text":"Robust Adversarial RL (RARL) deep Q-learning solver.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.RARL_TD3","page":"Library/Interface","title":"Crux.RARL_TD3","text":"Robust Adversarial RL (RARL) TD3 solver.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.RARL","page":"Library/Interface","title":"Crux.RARL","text":"Robust Adversarial RL (RARL) solver.\n\nRARL(;\n    ùíÆ_pro,\n    ùíÆ_ant,\n    px,\n    log::NamedTuple=(;), \n    train_pro_every::Int=1,\n    train_ant_every::Int=1,\n    buffer_size=1000, # Size of the buffer\n    required_columns=Symbol[:x, :fail],\n    buffer::ExperienceBuffer=ExperienceBuffer(ùíÆ_pro.S, ùíÆ_pro.agent.space, buffer_size, required_columns), # The replay buffer\n    buffer_init::Int=max(max(ùíÆ_pro.c_opt.batch_size, ùíÆ_ant.c_opt.batch_size), 200) # Number of observations to initialize the buffer with\n)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Continual-Learning","page":"Library/Interface","title":"Continual Learning","text":"","category":"section"},{"location":"interface/#Crux.ExperienceReplay","page":"Library/Interface","title":"Crux.ExperienceReplay","text":"Experience replay buffer.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Batched-RL","page":"Library/Interface","title":"Batched RL","text":"","category":"section"},{"location":"interface/#Crux.BatchSolver","page":"Library/Interface","title":"Crux.BatchSolver","text":"Batch solver type.\n\nFields\n\nagent::PolicyParams Policy parameters (PolicyParams)\nS::AbstractSpace State space\nmax_steps::Int = 100 Maximum number of steps per episode\nùíü_train Training data\nparam_optimizers::Dict{Any, TrainingParams} = Dict() Training parameters for the parameters\na_opt::TrainingParams Training parameters for the actor\nc_opt::Union{Nothing, TrainingParams} = nothing Training parameters for the discriminator\ntarget_fn = nothing the target function for value-based methods\ntarget_update = (œÄ‚Åª, œÄ; kwargs...) -> polyak_average!(œÄ‚Åª, œÄ, 0.005f0) Function for updating the target network\nùí´::NamedTuple = (;) Parameters of the algorithm\nlog::Union{Nothing, LoggerParams} = nothing The logging parameters\nrequired_columns = Symbol[] Extra columns to sample\nepoch = 0 Number of epochs of training\n\n\n\n\n\n","category":"type"},{"location":"interface/#Crux.CQL","page":"Library/Interface","title":"Crux.CQL","text":"Conservative Q-Learning (CQL) solver.\n\nCQL(;\n    œÄ::ActorCritic{T, DoubleNetwork{ContinuousNetwork, ContinuousNetwork}},\n    solver_type=BatchSAC,\n    CQL_Œ±::Float32=1f0,\n    CQL_is_distribution=DistributionPolicy(product_distribution([Uniform(-1,1) for i=1:dim(action_space(œÄ))[1]])),\n    CQL_Œ±_thresh::Float32=10f0,\n    CQL_n_action_samples::Int=10,\n    CQL_Œ±_opt::NamedTuple=(;),\n    a_opt::NamedTuple=(;), \n    c_opt::NamedTuple=(;), \n    log::NamedTuple=(;),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.BatchSAC","page":"Library/Interface","title":"Crux.BatchSAC","text":"Batched soft actor critic (SAC) solver.\n\nBatchSAC(;\n    œÄ::ActorCritic{T, DoubleNetwork{ContinuousNetwork, ContinuousNetwork}}, \n    S,\n    ŒîN=50, \n    SAC_Œ±::Float32=1f0, \n    SAC_H_target::Float32 = Float32(-prod(dim(action_space(œÄ)))), \n    ùíü_train, \n    SAC_Œ±_opt::NamedTuple=(;), \n    a_opt::NamedTuple=(;), \n    c_opt::NamedTuple=(;), \n    log::NamedTuple=(;), \n    ùí´::NamedTuple=(;), \n    param_optimizers=Dict(), \n    normalize_training_data = true, \n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Policies","page":"Library/Interface","title":"Policies","text":"","category":"section"},{"location":"interface/#Crux.PolicyParams","page":"Library/Interface","title":"Crux.PolicyParams","text":"Struct for combining useful policy parameters together\n\n    œÄ::Pol\n    space::T2 = action_space(œÄ)\n    œÄ_explore = œÄ\n    œÄ‚Åª = nothing\n    pa = nothing # nominal action distribution\n\n\n\n\n\n","category":"type"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"For a full set of examples, please see the examples/ directory.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Reinforcement learning examples:\nAtari\nCart Pole\nHalf Cheetah (MuJoCo)\nHalf Cheetah (PyBullet)\nPendulum\nImitation learning examples:\nCart Pole\nHalf Cheetah (MuJoCo)\nLava World\nPendulum\nAdversarial RL examples:\nCart Pole\nAircraft Collision Avoidance\nContinuous Bandit\nContinuous Pendulum\nDiscrete Pendulum\nContinuum World\nSafety Gym\nOffline RL examples:\nHopper Medium (MuJoCo)","category":"page"},{"location":"examples/#Minimal-RL-Example","page":"Examples","title":"Minimal RL Example","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"As a minimal example, we'll show how to set up a cart-pole problem and solve it with a simple Flux network using the REINFORCE algorithm.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Crux, POMDPGym\n\n# Problem setup\nmdp = GymPOMDP(:CartPole)\nas = actions(mdp)\nS = state_space(mdp)\n\n# Flux network: Map states to actions\nA() = DiscreteNetwork(Chain(Dense(dim(S)..., 64, relu), Dense(64, length(as))), as)\n\n# Setup REINFORCE solver\nsolver_reinforce = REINFORCE(S=S, œÄ=A())\n\n# Solve the `mdp` to get the `policy`\npolicy_reinforce = solve(solver_reinforce, mdp)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"You can run other algorithms, such as A2C and PPO, to generate different policies:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Set up the critic network for actor-critic algorithms\nV() = ContinuousNetwork(Chain(Dense(dim(S)..., 64, relu), Dense(64, 1)))\n\nsolver_a2c = A2C(S=S, œÄ=ActorCritic(A(), V()))\npolicy_a2c = solve(solver_a2c, mdp)\n\nsolver_ppo = PPO(S=S, œÄ=ActorCritic(A(), V()))\npolicy_ppo = solve(solver_ppo, mdp)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"You also may want to adjust the number of environment interactions N or the number of interactions between updates ŒîN:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"solver_reinforce = REINFORCE(S=S, œÄ=A(), N=10_000, ŒîN=500)\npolicy_reinforce = solve(solver_reinforce, mdp)\n\nsolver_a2c = A2C(S=S, œÄ=ActorCritic(A(), V()), N=10_000, ŒîN=500)\npolicy_a2c = solve(solver_a2c, mdp)\n\nsolver_ppo = PPO(S=S, œÄ=ActorCritic(A(), V()), N=10_000, ŒîN=500)\npolicy_ppo = solve(solver_ppo, mdp)","category":"page"},{"location":"examples/#Plotting-and-Animations","page":"Examples","title":"Plotting and Animations","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"You can take the above results and plot the learning curves:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"p = plot_learning([solver_reinforce, solver_a2c, solver_ppo],\n                  title=\"CartPole Training Curves\",\n                  labels=[\"REINFORCE\", \"A2C\", \"PPO\"])\nCrux.savefig(p, \"cartpole_training.pdf\")","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"You can also create an animated gif of the final policy:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"gif(mdp, policy_ppo, \"cartpole_policy.gif\", max_steps=100)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Note: You may need to install pygame via pip install \"gymnasium[classic-control]\"","category":"page"},{"location":"install/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"install/","page":"Installation","title":"Installation","text":"To install the package, run:","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"] add Crux","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"To edit or contribute use ] dev Crux and the repo will be cloned to ~/.julia/dev/Crux","category":"page"},{"location":"install/#Usage-with-POMDPGym","page":"Installation","title":"Usage with POMDPGym","text":"","category":"section"},{"location":"install/","page":"Installation","title":"Installation","text":"The POMDPGym package provides a wrapper for Gymnasium environments for reinforcement learning to work with POMDPs.jl. Includes options to get the observation space from pixels.","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"Install POMDPGym via:","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"] add https://github.com/ancorso/POMDPGym.jl","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"The Python dependencies gymnasium and pygame will be automatically installed during the build step of this package.","category":"page"},{"location":"install/#Atari-and-other-environments","page":"Installation","title":"Atari and other environments","text":"","category":"section"},{"location":"install/","page":"Installation","title":"Installation","text":"Currently, the automatic installation using Conda.jl does not install the Atari environments of Gymnasium. To do this, install Atari environments in a custom Python environment manually and ask PyCall.jl to use it. To elaborate, create a new Python virtual environment and run","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"pip install gymnasium[classic-control] gymnasium[atari] pygame\npip install autorom","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"Then run the shell command AutoROM and accept the Atari ROM license. Now you can configure PyCall.jl to use your Python environment following the instructions here.","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"Optionally, you can also install MuJoCo.","category":"page"},{"location":"contrib/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contrib/","page":"Contributing","title":"Contributing","text":"We welcome all contributions!","category":"page"},{"location":"contrib/","page":"Contributing","title":"Contributing","text":"Please fork the repository and submit a new Pull Request\nReport issues through our GitHub issue tracker","category":"page"},{"location":"contrib/#Style-Guide","page":"Contributing","title":"Style Guide","text":"","category":"section"},{"location":"contrib/","page":"Contributing","title":"Contributing","text":"(Image: Code Style: Blue)","category":"page"},{"location":"contrib/","page":"Contributing","title":"Contributing","text":"We follow the Blue style guide for Julia.","category":"page"},{"location":"#Crux","page":"Home","title":"Crux","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Build Status) (Image: Code Coverage)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Deep RL library with concise implementations of popular algorithms. Implemented using Flux.jl and fits into the POMDPs.jl interface.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Supports CPU and GPU computation and implements the following algorithms:","category":"page"},{"location":"#Reinforcement-Learning","page":"Home","title":"Reinforcement Learning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Deep Q-Learning (DQN)\nPrioritized Experience Replay\nSoft Q-Learning\nREINFORCE\nProximal Policy Optimization (PPO)\nLagrange-Constrained PPO\nAdvantage Actor Critic (A2C)\nDeep Deterministic Policy Gradient (DDPG)\nTwin Delayed DDPG (TD3)\nSoft Actor Critic (SAC)","category":"page"},{"location":"#Imitation-Learning","page":"Home","title":"Imitation Learning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Behavioral Cloning\nGenerative Adversarial Imitation Learning (GAIL) w/ On-Policy and Off-Policy Versions\nAdversarial Value Moment Imitation Learning (AdVIL)\nAdversarial Reward-moment Imitation Learning (AdRIL)\nSoft Q Imitation Learning (SQIL)\nAdversarial Soft Advantage Fitting (ASAF)\nInverse Q-Learning (IQLearn)","category":"page"},{"location":"#Batch-RL","page":"Home","title":"Batch RL","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Batch Soft Actor Critic (BatchSAC)\nConservative Q-Learning (CQL)","category":"page"},{"location":"#Adversarial-RL","page":"Home","title":"Adversarial RL","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Robust Adversarial RL (RARL)","category":"page"},{"location":"#Continual-Learning","page":"Home","title":"Continual Learning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Experience Replay","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In progress.","category":"page"}]
}
