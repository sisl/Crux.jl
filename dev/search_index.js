var documenterSearchIndex = {"docs":
[{"location":"interface/#Library/Interface","page":"Library/Interface","title":"Library/Interface","text":"This section details the interface and functions provided by Crux.jl.","category":"section"},{"location":"interface/#Contents","page":"Library/Interface","title":"Contents","text":"Pages = [\"interface.md\"]","category":"section"},{"location":"interface/#Index","page":"Library/Interface","title":"Index","text":"Pages = [\"interface.md\"]","category":"section"},{"location":"interface/#Reinforcement-Learning","page":"Library/Interface","title":"Reinforcement Learning","text":"RL interfaces are categorized by on-policy and off-policy below.","category":"section"},{"location":"interface/#On-Policy","page":"Library/Interface","title":"On-Policy","text":"","category":"section"},{"location":"interface/#Off-Policy","page":"Library/Interface","title":"Off-Policy","text":"","category":"section"},{"location":"interface/#Imitation-Learning","page":"Library/Interface","title":"Imitation Learning","text":"","category":"section"},{"location":"interface/#Adversarial-RL","page":"Library/Interface","title":"Adversarial RL","text":"","category":"section"},{"location":"interface/#Continual-Learning","page":"Library/Interface","title":"Continual Learning","text":"","category":"section"},{"location":"interface/#Batched-RL","page":"Library/Interface","title":"Batched RL","text":"","category":"section"},{"location":"interface/#Policies","page":"Library/Interface","title":"Policies","text":"","category":"section"},{"location":"interface/#Crux.OnPolicySolver","page":"Library/Interface","title":"Crux.OnPolicySolver","text":"On policy solver type.\n\nFields\n\nagent::PolicyParams Policy parameters (PolicyParams)\nS::AbstractSpace State space\nN::Int = 1000 Number of environment interactions\nŒîN::Int = 200 Number of interactions between updates\nmax_steps::Int = 100 Maximum number of steps per episode\nlog::Union{Nothing, LoggerParams} = nothing The logging parameters\ni::Int = 0 The current number of environment interactions\nparam_optimizers::Dict{Any, TrainingParams} = Dict() Training parameters for the parameters\na_opt::TrainingParams Training parameters for the actor\nc_opt::Union{Nothing, TrainingParams} = nothing Training parameters for the critic\nùí´::NamedTuple = (;) Parameters of the algorithm\ninteraction_storage = nothing If this is initialized to an array then it will store all interactions\npost_sample_callback = (ùíü; kwargs...) -> nothing Callback that that happens after sampling experience\npost_batch_callback = (ùíü; kwargs...) -> nothing Callback that that happens after sampling a batch\n\nOn-policy-specific parameters\n\nŒª_gae::Float32 = 0.95 Generalized advantage estimation parameter\nrequired_columns = Symbol[] Extra data columns to store\n\nParameters specific to cost constraints (a separate value network)\n\nVc::Union{ContinuousNetwork, Nothing} = nothing Cost value approximator\ncost_opt::Union{Nothing, TrainingParams} = nothing Training parameters for the cost value\n\n\n\n\n\n","category":"type"},{"location":"interface/#Crux.a2c_loss","page":"Library/Interface","title":"Crux.a2c_loss","text":"A2C loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.A2C","page":"Library/Interface","title":"Crux.A2C","text":"Advantage actor critic (A2C) solver.\n\nA2C(;\n    œÄ::ActorCritic, \n    a_opt::NamedTuple=(;), \n    c_opt::NamedTuple=(;), \n    log::NamedTuple=(;), \n    Œªp::Float32=1f0, \n    Œªe::Float32=0.1f0, \n    required_columns=[],\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.ppo_loss","page":"Library/Interface","title":"Crux.ppo_loss","text":"PPO loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.PPO","page":"Library/Interface","title":"Crux.PPO","text":"Proximal policy optimization (PPO) solver.\n\nPPO(;\n    œÄ::ActorCritic,\n    œµ::Float32 = 0.2f0,\n    Œªp::Float32 = 1f0,\n    Œªe::Float32 = 0.1f0,\n    target_kl = 0.012f0,\n    a_opt::NamedTuple=(;),\n    c_opt::NamedTuple=(;),\n    log::NamedTuple=(;),\n    required_columns=[],\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.lagrange_ppo_loss","page":"Library/Interface","title":"Crux.lagrange_ppo_loss","text":"PPO loss with a penalty.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.LagrangePPO","page":"Library/Interface","title":"Crux.LagrangePPO","text":"Lagrange-Constrained PPO solver.\n\nLagrangePPO(;\n    œÄ::ActorCritic,\n    Vc::ContinuousNetwork, # value network for estimating cost\n    œµ::Float32 = 0.2f0,\n    Œªp::Float32 = 1f0,\n    Œªe::Float32 = 0.1f0,\n    Œª_gae = 0.95f0,\n    target_kl = 0.012f0,\n    target_cost = 0.025f0,\n    penalty_scale = 1f0,\n    penalty_max = Inf32,\n    Ki_max = 10f0,\n    Ki = 1f-3,\n    Kp = 1,\n    Kd = 0,\n    ema_Œ± = 0.95,\n    a_opt::NamedTuple=(;),\n    c_opt::NamedTuple=(;),\n    cost_opt::NamedTuple=(;),\n    log::NamedTuple=(;),\n    required_columns=[],\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.reinforce_loss","page":"Library/Interface","title":"Crux.reinforce_loss","text":"REINFORCE loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.REINFORCE","page":"Library/Interface","title":"Crux.REINFORCE","text":"REINFORCE solver.\n\nREINFORCE(;\n    œÄ,\n    a_opt::NamedTuple=(;), \n    log::NamedTuple=(;),\n    required_columns=[],\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.OffPolicySolver","page":"Library/Interface","title":"Crux.OffPolicySolver","text":"Off policy solver type.\n\nFields\n\nagent::PolicyParams # Policy parameters (PolicyParams)\nS::AbstractSpace # State space\nN::Int = 1000 # Number of environment interactions\nŒîN::Int = 4 # Number of interactions between updates\nmax_steps::Int = 100 # Maximum number of steps per episode\nlog::Union{Nothing, LoggerParams} = nothing # The logging parameters\ni::Int = 0 # The current number of environment interactions\nparam_optimizers::Dict{Any, TrainingParams} = Dict() # Training parameters for the parameters\na_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the actor\nc_opt::TrainingParams # Training parameters for the critic\nùí´::NamedTuple = (;) # Parameters of the algorithm\ninteraction_storage = nothing # If this is initialized to an array then it will store all interactions\npost_sample_callback = (ùíü; kwargs...) -> nothing # Callback that that happens after sampling experience\n\nOff-policy-specific parameters\n\npost_batch_callback = (ùíü; kwargs...) -> nothing Callback that that happens after sampling a batch\npre_train_callback = (ùíÆ; kwargs...) -> nothing callback that gets called once prior to training\ntarget_update = (œÄ‚Åª, œÄ; kwargs...) -> polyak_average!(œÄ‚Åª, œÄ, 0.005f0) Function for updating the target network\ntarget_fn Target for critic regression with input signature (œÄ‚Åª, ùíü, Œ≥; i)\nbuffer_size = 1000 Size of the buffer\nrequired_columns = Symbol[]\nbuffer = ExperienceBuffer(S, agent.space, buffer_size, required_columns) The replay buffer\npriority_fn = td_error function for prioritized replay\nbuffer_init::Int = max(c_opt.batch_size, 200) Number of observations to initialize the buffer with\nextra_buffers = [] extra buffers (i.e. for experience replay in continual learning)\nbuffer_fractions = [1.0] Fraction of the minibatch devoted to each buffer\n\n\n\n\n\n","category":"type"},{"location":"interface/#Crux.ddpg_target","page":"Library/Interface","title":"Crux.ddpg_target","text":"DDPG target function.\n\nSet y·µ¢ = r·µ¢ + Œ≥Q‚Ä≤(s·µ¢‚Çä‚ÇÅ, Œº‚Ä≤(s·µ¢‚Çä‚ÇÅ | Œ∏·µò‚Ä≤) | Œ∏·∂ú‚Ä≤)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.smoothed_ddpg_target","page":"Library/Interface","title":"Crux.smoothed_ddpg_target","text":"Smooth DDPG target.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.ddpg_actor_loss","page":"Library/Interface","title":"Crux.ddpg_actor_loss","text":"DDPG actor loss function.\n\n‚àá_Œ∏·µò ùêΩ ‚âà 1/ùëÅ Œ£·µ¢ ‚àá‚ÇêQ(s, a | Œ∏·∂ú)|‚Çõ‚Çå‚Çõ·µ¢, ‚Çê‚Çå·µ§‚Çç‚Çõ·µ¢‚Çé ‚àá_Œ∏·µò Œº(s | Œ∏·µò)|‚Çõ·µ¢\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.DDPG","page":"Library/Interface","title":"Crux.DDPG","text":"Deep deterministic policy gradient (DDPG) solver.\n\nT. P. Lillicrap, et al., \"Continuous control with deep reinforcement learning\", ICLR 2016.\n\nDDPG(;\n    œÄ::ActorCritic, \n    ŒîN=50, \n    œÄ_explore=GaussianNoiseExplorationPolicy(0.1f0),  \n    a_opt::NamedTuple=(;), \n    c_opt::NamedTuple=(;),\n    a_loss=ddpg_actor_loss,\n    c_loss=td_loss(),\n    target_fn=ddpg_target,\n    prefix=\"\",\n    log::NamedTuple=(;), \n    œÄ_smooth=GaussianNoiseExplorationPolicy(0.1f0, œµ_min=-0.5f0, œµ_max=0.5f0), kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.dqn_target","page":"Library/Interface","title":"Crux.dqn_target","text":"DQN target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.DQN","page":"Library/Interface","title":"Crux.DQN","text":"Deep Q-learning (DQN) solver.\n\nV. Mnih, et al., \"Human-level control through deep reinforcement learning\", Nature 2015.\n\nDQN(;\n      œÄ::DiscreteNetwork, \n      N::Int, \n      ŒîN=4, \n      œÄ_explore=œµGreedyPolicy(LinearDecaySchedule(1., 0.1, floor(Int, N/2)), œÄ.outputs), \n      c_opt::NamedTuple=(;), \n      log::NamedTuple=(;),\n      c_loss=td_loss(),\n      target_fn=dqn_target,\n      prefix=\"\",\n      kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.sac_target","page":"Library/Interface","title":"Crux.sac_target","text":"SAC target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.sac_deterministic_target","page":"Library/Interface","title":"Crux.sac_deterministic_target","text":"Deterministic SAC target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.sac_max_q_target","page":"Library/Interface","title":"Crux.sac_max_q_target","text":"Max-Q SAC target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.sac_actor_loss","page":"Library/Interface","title":"Crux.sac_actor_loss","text":"SAC actor loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.sac_temp_loss","page":"Library/Interface","title":"Crux.sac_temp_loss","text":"SAC temp-based loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.SAC","page":"Library/Interface","title":"Crux.SAC","text":"Soft Actor Critic (SAC) solver.\n\nSAC(;\n    œÄ::ActorCritic{T, DoubleNetwork{ContinuousNetwork, ContinuousNetwork}},\n    ŒîN=50,\n    SAC_Œ±::Float32=1f0,\n    SAC_H_target::Float32 = Float32(-prod(dim(action_space(œÄ)))),\n    œÄ_explore=GaussianNoiseExplorationPolicy(0.1f0),\n    SAC_Œ±_opt::NamedTuple=(;),\n    a_opt::NamedTuple=(;),\n    c_opt::NamedTuple=(;),\n    a_loss=sac_actor_loss,\n    c_loss=double_Q_loss(),\n    target_fn=sac_target(œÄ),\n    prefix=\"\",\n    log::NamedTuple=(;),\n    ùí´::NamedTuple=(;),\n    param_optimizers=Dict(),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.softq_target","page":"Library/Interface","title":"Crux.softq_target","text":"Soft Q-learning target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.SoftQ","page":"Library/Interface","title":"Crux.SoftQ","text":"Soft Q-learning solver.\n\nSoftQ(;\n    œÄ::DiscreteNetwork, \n    N::Int, \n    ŒîN=4, \n    c_opt::NamedTuple=(;epochs=4), \n    log::NamedTuple=(;),\n    c_loss=td_loss(),\n    Œ±=Float32(1.),\n    prefix=\"\",\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.td3_target","page":"Library/Interface","title":"Crux.td3_target","text":"TD3 target function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.td3_actor_loss","page":"Library/Interface","title":"Crux.td3_actor_loss","text":"TD3 actor loss function.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.TD3","page":"Library/Interface","title":"Crux.TD3","text":"Twin Delayed DDPG (TD3) solver.\n\nTD3(;\n    œÄ,\n    ŒîN=50,\n    œÄ_smooth::Policy=GaussianNoiseExplorationPolicy(0.1f0, œµ_min=-0.5f0, œµ_max=0.5f0),\n    œÄ_explore=GaussianNoiseExplorationPolicy(0.1f0),\n    a_opt::NamedTuple=(;),\n    c_opt::NamedTuple=(;),\n    a_loss=td3_actor_loss,\n    c_loss=double_Q_loss(),\n    target_fn=td3_target,\n    prefix=\"\",\n    log::NamedTuple=(;),\n    ùí´::NamedTuple=(;),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.AdRIL","page":"Library/Interface","title":"Crux.AdRIL","text":"Adversarial Reward-moment Imitation Learning (AdRIL) solver.\n\nAdRIL(;\n    œÄ, \n    S,\n    ŒîN=50,\n    solver=SAC, \n    ùíü_demo, \n    normalize_demo::Bool=true,\n    expert_frac=0.5, \n    buffer_size = 1000, \n    buffer_init=0,\n    log::NamedTuple=(;),\n    buffer::ExperienceBuffer = ExperienceBuffer(S, action_space(œÄ), buffer_size, [:i]), \n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.AdVIL","page":"Library/Interface","title":"Crux.AdVIL","text":"Adversarial Value Moment Imitation Learning (AdVIL) solver.\n\nAdVIL(;\n    œÄ, \n    S,\n    ùíü_demo, \n    normalize_demo::Bool=true, \n    Œª_GP::Float32=10f0, \n    Œª_orth::Float32=1f-4, \n    Œª_BC::Float32=2f-1, \n    a_opt::NamedTuple=(;), \n    c_opt::NamedTuple=(;), \n    log::NamedTuple=(;), \n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.ASAF","page":"Library/Interface","title":"Crux.ASAF","text":"Adversarial Soft Advantage Fitting (ASAF) solver.\n\nASAF(;\n    œÄ,\n    S,\n    ùíü_demo,\n    normalize_demo::Bool=true,\n    ŒîN=50,\n    Œª_orth=1f-4,\n    a_opt::NamedTuple=(;),\n    c_opt::NamedTuple=(;),\n    log::NamedTuple=(;),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.BC","page":"Library/Interface","title":"Crux.BC","text":"Behavioral cloning solver.\n\nBC(;\n    œÄ,\n    S,\n    ùíü_demo,\n    normalize_demo::Bool=true,\n    loss=nothing,\n    validation_fraction=0.3,\n    window=100,\n    Œªe::Float32=1f-3,\n    opt::NamedTuple=(;),\n    log::NamedTuple=(;),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.OnlineIQLearn","page":"Library/Interface","title":"Crux.OnlineIQLearn","text":"Online Inverse Q-Learning solver.\n\nOnlineIQLearn(;\n    œÄ, \n    S, \n    ùíü_demo, \n    Œ≥=Float32(0.9),\n    normalize_demo::Bool=true, \n    solver=SoftQ, # or SAC for continuous states \n    log::NamedTuple=(;period=500), \n    reg::Bool=true,\n    Œ±_reg=Float32(0.5),\n    gp::Bool=true,\n    Œª_gp=Float32(10.),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.OffPolicyGAIL","page":"Library/Interface","title":"Crux.OffPolicyGAIL","text":"Off-policy generative adversarial imitation learning (GAIL) solver.\n\nOffPolicyGAIL(;\n    œÄ,\n    S, \n    ùíü_demo, \n    ùíü_ndas::Array{ExperienceBuffer} = ExperienceBuffer[], \n    normalize_demo::Bool=true, \n    D::ContinuousNetwork, \n    solver=SAC, \n    d_opt::NamedTuple=(epochs=5,), \n    log::NamedTuple=(;), \n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.OnPolicyGAIL","page":"Library/Interface","title":"Crux.OnPolicyGAIL","text":"On-policy generative adversarial imitation learning (GAIL) solver.\n\nOnPolicyGAIL(;\n    œÄ,\n    S,\n    Œ≥,\n    Œª_gae::Float32 = 0.95f0,\n    ùíü_demo,\n    Œ±r::Float32 = 0.5f0,\n    normalize_demo::Bool=true,\n    D::ContinuousNetwork,\n    solver=PPO,\n    gan_loss::GANLoss=GAN_BCELoss(),\n    d_opt::NamedTuple=(;),\n    log::NamedTuple=(;),\n    Rscale=1f0,\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.SQIL","page":"Library/Interface","title":"Crux.SQIL","text":"Soft Q Imitation Learning (SQIL) solver.\n\nSQIL(;\n    œÄ, \n    S, \n    ùíü_demo, \n    normalize_demo::Bool=true, \n    solver=SAC, \n    log::NamedTuple=(;), \n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.AdversarialOffPolicySolver","page":"Library/Interface","title":"Crux.AdversarialOffPolicySolver","text":"Adversarial off-policy solver.\n\nùíÆ_pro::OffPolicySolver Solver parameters for the protagonist\nùíÆ_ant::OffPolicySolver Solver parameters for the antagonist\npx::PolicyParams Nominal disturbance policy\ntrain_pro_every::Int = 1\ntrain_ant_every::Int = 1\nlog::Union{Nothing, LoggerParams} = nothing The logging parameters\ni::Int = 0 The current number of environment interactions\n\n\n\n\n\n","category":"type"},{"location":"interface/#Crux.RARL_DQN","page":"Library/Interface","title":"Crux.RARL_DQN","text":"Robust Adversarial RL (RARL) deep Q-learning solver.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.RARL_TD3","page":"Library/Interface","title":"Crux.RARL_TD3","text":"Robust Adversarial RL (RARL) TD3 solver.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.RARL","page":"Library/Interface","title":"Crux.RARL","text":"Robust Adversarial RL (RARL) solver.\n\nRARL(;\n    ùíÆ_pro,\n    ùíÆ_ant,\n    px,\n    log::NamedTuple=(;), \n    train_pro_every::Int=1,\n    train_ant_every::Int=1,\n    buffer_size=1000, # Size of the buffer\n    required_columns=Symbol[:x, :fail],\n    buffer::ExperienceBuffer=ExperienceBuffer(ùíÆ_pro.S, ùíÆ_pro.agent.space, buffer_size, required_columns), # The replay buffer\n    buffer_init::Int=max(max(ùíÆ_pro.c_opt.batch_size, ùíÆ_ant.c_opt.batch_size), 200) # Number of observations to initialize the buffer with\n)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.ExperienceReplay","page":"Library/Interface","title":"Crux.ExperienceReplay","text":"Experience replay buffer.\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.BatchSolver","page":"Library/Interface","title":"Crux.BatchSolver","text":"Batch solver type.\n\nFields\n\nagent::PolicyParams Policy parameters (PolicyParams)\nS::AbstractSpace State space\nmax_steps::Int = 100 Maximum number of steps per episode\nùíü_train Training data\nparam_optimizers::Dict{Any, TrainingParams} = Dict() Training parameters for the parameters\na_opt::TrainingParams Training parameters for the actor\nc_opt::Union{Nothing, TrainingParams} = nothing Training parameters for the discriminator\ntarget_fn = nothing the target function for value-based methods\ntarget_update = (œÄ‚Åª, œÄ; kwargs...) -> polyak_average!(œÄ‚Åª, œÄ, 0.005f0) Function for updating the target network\nùí´::NamedTuple = (;) Parameters of the algorithm\nlog::Union{Nothing, LoggerParams} = nothing The logging parameters\nrequired_columns = Symbol[] Extra columns to sample\nepoch = 0 Number of epochs of training\n\n\n\n\n\n","category":"type"},{"location":"interface/#Crux.CQL","page":"Library/Interface","title":"Crux.CQL","text":"Conservative Q-Learning (CQL) solver.\n\nCQL(;\n    œÄ::ActorCritic{T, DoubleNetwork{ContinuousNetwork, ContinuousNetwork}},\n    solver_type=BatchSAC,\n    CQL_Œ±::Float32=1f0,\n    CQL_is_distribution=DistributionPolicy(product_distribution([Uniform(-1,1) for i=1:dim(action_space(œÄ))[1]])),\n    CQL_Œ±_thresh::Float32=10f0,\n    CQL_n_action_samples::Int=10,\n    CQL_Œ±_opt::NamedTuple=(;),\n    a_opt::NamedTuple=(;), \n    c_opt::NamedTuple=(;), \n    log::NamedTuple=(;),\n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.BatchSAC","page":"Library/Interface","title":"Crux.BatchSAC","text":"Batched soft actor critic (SAC) solver.\n\nBatchSAC(;\n    œÄ::ActorCritic{T, DoubleNetwork{ContinuousNetwork, ContinuousNetwork}}, \n    S,\n    ŒîN=50, \n    SAC_Œ±::Float32=1f0, \n    SAC_H_target::Float32 = Float32(-prod(dim(action_space(œÄ)))), \n    ùíü_train, \n    SAC_Œ±_opt::NamedTuple=(;), \n    a_opt::NamedTuple=(;), \n    c_opt::NamedTuple=(;), \n    log::NamedTuple=(;), \n    ùí´::NamedTuple=(;), \n    param_optimizers=Dict(), \n    normalize_training_data = true, \n    kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"interface/#Crux.PolicyParams","page":"Library/Interface","title":"Crux.PolicyParams","text":"Struct for combining useful policy parameters together\n\n    œÄ::Pol\n    space::T2 = action_space(œÄ)\n    œÄ_explore = œÄ\n    œÄ‚Åª = nothing\n    pa = nothing # nominal action distribution\n\n\n\n\n\n","category":"type"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"For a full set of examples, please see the examples/ directory.\n\nReinforcement learning examples:\nAtari\nCart Pole\nHalf Cheetah (MuJoCo)\nHalf Cheetah (PyBullet)\nPendulum\nImitation learning examples:\nCart Pole\nHalf Cheetah (MuJoCo)\nLava World\nPendulum\nAdversarial RL examples:\nCart Pole\nAircraft Collision Avoidance\nContinuous Bandit\nContinuous Pendulum\nDiscrete Pendulum\nContinuum World\nSafety Gym\nOffline RL examples:\nHopper Medium (MuJoCo)","category":"section"},{"location":"examples/#Minimal-RL-Example","page":"Examples","title":"Minimal RL Example","text":"As a minimal example, we'll show how to set up a cart-pole problem and solve it with a simple Flux network using the REINFORCE algorithm.\n\nusing Crux, POMDPGym\n\n# Problem setup\nmdp = GymPOMDP(:CartPole)\nas = actions(mdp)\nS = state_space(mdp)\n\n# Flux network: Map states to actions\nA() = DiscreteNetwork(Chain(Dense(dim(S)..., 64, relu), Dense(64, length(as))), as)\n\n# Setup REINFORCE solver\nsolver_reinforce = REINFORCE(S=S, œÄ=A())\n\n# Solve the `mdp` to get the `policy`\npolicy_reinforce = solve(solver_reinforce, mdp)\n\nYou can run other algorithms, such as A2C and PPO, to generate different policies:\n\n# Set up the critic network for actor-critic algorithms\nV() = ContinuousNetwork(Chain(Dense(dim(S)..., 64, relu), Dense(64, 1)))\n\nsolver_a2c = A2C(S=S, œÄ=ActorCritic(A(), V()))\npolicy_a2c = solve(solver_a2c, mdp)\n\nsolver_ppo = PPO(S=S, œÄ=ActorCritic(A(), V()))\npolicy_ppo = solve(solver_ppo, mdp)\n\nYou also may want to adjust the number of environment interactions N or the number of interactions between updates ŒîN:\n\nsolver_reinforce = REINFORCE(S=S, œÄ=A(), N=10_000, ŒîN=500)\npolicy_reinforce = solve(solver_reinforce, mdp)\n\nsolver_a2c = A2C(S=S, œÄ=ActorCritic(A(), V()), N=10_000, ŒîN=500)\npolicy_a2c = solve(solver_a2c, mdp)\n\nsolver_ppo = PPO(S=S, œÄ=ActorCritic(A(), V()), N=10_000, ŒîN=500)\npolicy_ppo = solve(solver_ppo, mdp)","category":"section"},{"location":"examples/#Plotting-and-Animations","page":"Examples","title":"Plotting and Animations","text":"You can take the above results and plot the learning curves:\n\np = plot_learning([solver_reinforce, solver_a2c, solver_ppo],\n                  title=\"CartPole Training Curves\",\n                  labels=[\"REINFORCE\", \"A2C\", \"PPO\"])\nCrux.savefig(p, \"cartpole_training.pdf\")\n\nHere's an example for the half cheetah MuJoCo problem, comparing four RL algorithms from examples/rl/half_cheetah_mujoco.jl.\n\n(Image: mujoco)\n\nYou can also create an animated gif of the final policy:\n\ngif(mdp, policy_ppo, \"cartpole_policy.gif\", max_steps=100)\n\nNote: You may need to install pygame via pip install \"gymnasium[classic-control]\"","category":"section"},{"location":"install/#Installation","page":"Installation","title":"Installation","text":"To install the package, run:\n\n] add Crux\n\nTo edit or contribute use ] dev Crux and the repo will be cloned to ~/.julia/dev/Crux","category":"section"},{"location":"install/#Usage-with-POMDPGym","page":"Installation","title":"Usage with POMDPGym","text":"The POMDPGym package provides a wrapper for Gymnasium environments for reinforcement learning to work with POMDPs.jl. Includes options to get the observation space from pixels.\n\nInstall POMDPGym via:\n\n] add https://github.com/ancorso/POMDPGym.jl\n\nThe Python dependencies gymnasium and pygame will be automatically installed during the build step of this package.","category":"section"},{"location":"install/#Atari-and-other-environments","page":"Installation","title":"Atari and other environments","text":"Currently, the automatic installation using Conda.jl does not install the Atari environments of Gymnasium. To do this, install Atari environments in a custom Python environment manually and ask PyCall.jl to use it. To elaborate, create a new Python virtual environment and run\n\npip install gymnasium[classic-control] gymnasium[atari] pygame\npip install autorom\n\nThen run the shell command AutoROM and accept the Atari ROM license. Now you can configure PyCall.jl to use your Python environment following the instructions here.\n\nOptionally, you can also install MuJoCo.","category":"section"},{"location":"contrib/#Contributing","page":"Contributing","title":"Contributing","text":"We welcome all contributions!\n\nPlease fork the repository and submit a new Pull Request\nReport issues through our GitHub issue tracker","category":"section"},{"location":"contrib/#Style-Guide","page":"Contributing","title":"Style Guide","text":"(Image: Code Style: Blue)\n\nWe follow the Blue style guide for Julia.","category":"section"},{"location":"#Crux","page":"Home","title":"Crux","text":"(Image: Build Status) (Image: Code Coverage)\n\nDeep RL library with concise implementations of popular algorithms. Implemented using Flux.jl and fits into the POMDPs.jl interface.\n\nSupports CPU and GPU computation and implements the following algorithms:","category":"section"},{"location":"#Reinforcement-Learning","page":"Home","title":"Reinforcement Learning","text":"Deep Q-Learning (DQN)\nPrioritized Experience Replay\nSoft Q-Learning\nREINFORCE\nProximal Policy Optimization (PPO)\nLagrange-Constrained PPO\nAdvantage Actor Critic (A2C)\nDeep Deterministic Policy Gradient (DDPG)\nTwin Delayed DDPG (TD3)\nSoft Actor Critic (SAC)","category":"section"},{"location":"#Imitation-Learning","page":"Home","title":"Imitation Learning","text":"Behavioral Cloning\nGenerative Adversarial Imitation Learning (GAIL) w/ On-Policy and Off-Policy Versions\nAdversarial Value Moment Imitation Learning (AdVIL)\nAdversarial Reward-moment Imitation Learning (AdRIL)\nSoft Q Imitation Learning (SQIL)\nAdversarial Soft Advantage Fitting (ASAF)\nInverse Q-Learning (IQLearn)","category":"section"},{"location":"#Batch-RL","page":"Home","title":"Batch RL","text":"Batch Soft Actor Critic (BatchSAC)\nConservative Q-Learning (CQL)","category":"section"},{"location":"#Adversarial-RL","page":"Home","title":"Adversarial RL","text":"Robust Adversarial RL (RARL)","category":"section"},{"location":"#Continual-Learning","page":"Home","title":"Continual Learning","text":"Experience Replay\n\n","category":"section"},{"location":"#Citation","page":"Home","title":"Citation","text":"In progress.","category":"section"}]
}
