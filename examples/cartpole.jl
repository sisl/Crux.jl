using POMDPs, Crux, Flux, POMDPGym

## Cartpole - V0
mdp = GymPOMDP(:CartPole, version = :v0)
as = actions(mdp)
S = state_space(mdp)

Q() = Chain(Dense(dim(S)..., 64, relu), Dense(64, 64, relu), Dense(64, length(as)))
V() = Chain(Dense(dim(S)..., 64, relu), Dense(64, 64, relu), Dense(64, 1))
A() = Chain(Dense(dim(S)..., 64, relu), Dense(64, 64, relu), Dense(64, length(as)), softmax)

# Solve with REINFORCE
ğ’®_reinforce = PGSolver(Ï€ = CategoricalPolicy(A = A(), actions = as),
                S = S, N=10000, Î”N = 500, loss = reinforce())
Ï€_reinforce = solve(ğ’®_reinforce, mdp)

# Solve with A2C
ğ’®_a2c = PGSolver(Ï€ = ActorCritic(CategoricalPolicy(A = A(), actions = as), V()), 
                S = S, N=10000, Î”N = 500, loss = a2c())
Ï€_a2c = solve(ğ’®_a2c, mdp)

# Solve with PPO
ğ’®_ppo = PGSolver(Ï€ = ActorCritic(CategoricalPolicy(A = A(), actions = as), V()), 
                S = S, N=10000, Î”N = 500, loss = ppo())
Ï€_ppo = solve(ğ’®_ppo, mdp)

# Solve with DQN
ğ’®_dqn = DQNSolver(Ï€ = DQNPolicy(Q = Q(), actions = as), S = S, N=10000)
Ï€_dqn = solve(ğ’®_dqn, mdp)

# Plot the learning curve
p = plot_learning([ğ’®_reinforce, ğ’®_a2c, ğ’®_ppo, ğ’®_dqn], title = "CartPole-V0 Training Curves", labels = ["REINFORCE", "A2C", "PPO", "DQN"])

# Produce a gif with the final policy
gif(mdp, Ï€_ppo, "cartpole_policy.gif")