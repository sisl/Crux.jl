using Debugger, Revise #remove
using Crux, Flux, POMDPGym, Random, POMDPs, BSON


## Cartpole
mdp = GymPOMDP(:CartPole, version = :v0)
as = actions(mdp)
S = state_space(mdp)
Î³ = Float32(discount(mdp))

Disc() = ContinuousNetwork(Chain(Dense(6, 64, relu), Dense(64, 64, relu), Dense(64, 1)))
V() = ContinuousNetwork(Chain(Dense(4, 64, relu), Dense(64, 64, relu), Dense(64, 1)))
A() = DiscreteNetwork(Chain(Dense(4, 64, relu), Dense(64, 64, relu), Dense(64, length(as))), as)
SA() = SoftDiscreteNetwork(Chain(Dense(4, 64, relu), Dense(64, 64, relu), Dense(64, length(as))), as;Î±=Float32(1.))


# Fill a buffer with expert trajectories
expert_trajectories = BSON.load("examples/il/expert_data/cartpole.bson")[:data]

# Solve with PPO-GAIL
ğ’®_gail = OnPolicyGAIL(D=Disc(), Î³=Î³, gan_loss=GAN_BCELoss(), ğ’Ÿ_demo=expert_trajectories, solver=PPO, Ï€=ActorCritic(A(), V()), S=S, 
    N=40000, Î”N=1024, d_opt=(batch_size=1024, epochs=80))
solve(ğ’®_gail, mdp)

# Solve with Behavioral Cloning
ğ’®_bc = BC(Ï€=A(), ğ’Ÿ_demo=expert_trajectories, S=S, opt=(epochs=100,), log=(period=10,))
solve(ğ’®_bc, mdp)

# Solve with IQ-Learn
ğ’®_iql = OnlineIQLearn(Ï€=SA(), ğ’Ÿ_demo=expert_trajectories, S=S, Î³=Î³, N=40000, Î”N=200, log=(;period=50))
solve(ğ’®_iql, mdp)

# Plot true rewards
p = plot_learning([ğ’®_gail, ğ’®_bc, ğ’®_iql], title = "CartPole-V0 Imitation Training Curves", 
    labels = ["GAIL", "BC", "IQLearn"])
Crux.savefig(p, "examples/il/cartpole_training.pdf")