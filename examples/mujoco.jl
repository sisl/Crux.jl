# Note: This setup should get at least the performance of the openai-spinning up benchmarks
using Crux, Flux, POMDPs, POMDPGym, Distributions
init_mujoco_render() # Required for visualization

# Construct the Mujoco environment
mdp = GymPOMDP(:HalfCheetah, version = :v3)
S = state_space(mdp)
adim = length(POMDPs.actions(mdp)[1])

# Initializations that match the default PyTorch initializations
Winit(out, in) = rand(Uniform(Float32(-sqrt(1/in)), Float32(sqrt(1/in))), out, in)
binit(in) = (out) -> rand(Uniform(Float32(-sqrt(1/in)), Float32(sqrt(1/in))), out)

# Build the networks
Î¼() = Chain(Dense(S.dims[1], 64, tanh, initW = Winit, initb = binit(S.dims[1])), Dense(64, 32, tanh, initW = Winit, initb = binit(64)), Dense(32, adim, initW = Winit, initb = binit(32)))
V() = Chain(Dense(S.dims[1], 64, tanh, initW = Winit, initb = binit(S.dims[1])), Dense(64, 32, tanh, initW = Winit, initb = binit(64)), Dense(32, 1, initW = Winit, initb = binit(32)))
log_std() = -0.5*ones(Float32, adim)

# Solve with ppo
ğ’®_ppo = PGSolver(Ï€ = ActorCritic(GaussianPolicy(Î¼(), log_std()), V()), 
                 S = S, 
                 max_steps = 1000, 
                 loss = ppo(Î»â‚‘ = 0f0),
                 Î”N = 4000,
                 Î»_gae = 0.97,
                 batch_size = 4000,
                 epochs = 80,
                 early_stopping = (info) -> info[:kl] > 0.015,
                 N = 3000000, 
                 opt = ADAM(3e-4),
                 opt_v = ADAM(1e-3)
                 )
solve(ğ’®_ppo, mdp)

# Plot the learning curve
p = plot_learning(ğ’®_ppo, title = "HalfCheetah Training Curves")

# Produce a gif with the final policy
gif(mdp, ğ’®_ppo.Ï€, "mujoco.gif")

