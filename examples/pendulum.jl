using POMDPs, Crux, Flux, POMDPGym

## Pendulum
mdp = PendulumMDP(actions = [-2., -0.5, 0, 0.5, 2.])
as = [actions(mdp)...]
S = state_space(mdp)

# Define the networks we will use
Q() = Chain(x -> x ./ [6.3f0, 8f0], Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, length(as)), x -> 200f0.*x .- 200f0)
A() = Chain(x -> x ./ [6.3f0, 8f0], Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, length(as)), softmax)
Î¼() = Chain(x -> x ./ [6.3f0, 8f0], Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, 1))
V() = Chain(x -> x ./ [6.3f0, 8f0], Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, 1), x -> 200f0.*x .- 200f0)

# Solve with REINFORCE
ğ’®_reinforce = PGSolver(Ï€ = GaussianPolicy(Î¼ = Î¼(), logÎ£ = zeros(Float32, 1)),
                S = S, N=100000, Î”N = 2048, loss = reinforce(), opt = Flux.Optimiser(ClipNorm(1f0), ADAM(1e-4)), batch_size = 512, epochs = 10)
Ï€_reinforce = solve(ğ’®_reinforce, mdp)

# Solve with A2C
ğ’®_a2c = PGSolver(Ï€ = ActorCritic(GaussianPolicy(Î¼ = Î¼(), logÎ£ = zeros(Float32, 1)), V()), 
                S = S, N=100000, Î”N = 2048, loss = a2c(), opt = Flux.Optimiser(ClipNorm(1f0), ADAM(1e-4)), batch_size = 512, epochs = 10)
Ï€_a2c = solve(ğ’®_a2c, mdp)

# Solve with PPO
ğ’®_ppo = PGSolver(Ï€ = ActorCritic(GaussianPolicy(Î¼ = Î¼(), logÎ£ = zeros(Float32, 1)), V()), 
                S = S, N=100000, Î”N = 2048, loss = ppo(), opt = Flux.Optimiser(ClipNorm(1f0), ADAM(1e-3)), batch_size = 512, epochs = 100)
Ï€_ppo = solve(ğ’®_ppo, mdp)

# Solve with DQN
ğ’®_dqn = DQNSolver(Ï€ = DQNPolicy(Q = Q(), actions = as), S = S, N=100000)
Ï€_dqn = solve(ğ’®_dqn, mdp)


# Plot the learning curve
p = plot_learning([ğ’®_reinforce, ğ’®_a2c, ğ’®_ppo, ğ’®_dqn], title = "Pendulum Swingup Training Curves", labels = ["REINFORCE", "A2C", "PPO", "DQN"])

# Produce a gif with the final policy
gif(mdp, Ï€_dqn, "pendulum.gif", max_steps = 200)
