using POMDPs, Crux, Flux, POMDPGym

## Pendulum
mdp = PendulumMDP(actions=[-2., -0.5, 0, 0.5, 2.])
as = [actions(mdp)...]
S = state_space(mdp)

# Define the networks we will use
QSA() = ContinuousNetwork(Chain(x -> x ./ [6.3f0, 8f0, 2f0], Dense(3, 64, relu), Dense(64, 64, relu), Dense(64, 1)))
QS() = DiscreteNetwork(Chain(x -> x ./ [6.3f0, 8f0], Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, length(as))), as)
V() = ContinuousNetwork(Chain(x -> x ./ [6.3f0, 8f0], Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, 1)))
A() = ContinuousNetwork(Chain(x -> x ./ [6.3f0, 8f0], Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, 1, tanh), x -> 2f0 * x), 1)

G() = GaussianPolicy(A(), zeros(Float32, 1))


# Solve with REINFORCE
ğ’®_reinforce = PGSolver(Ï€=G(), S=S, N=100000, Î”N=2048, loss=reinforce(), batch_size=512)
Ï€_reinforce = solve(ğ’®_reinforce, mdp)

# Solve with A2C
ğ’®_a2c = PGSolver(Ï€=ActorCritic(G(), V()), S=S, N=100000, Î”N=2048, loss=a2c(), batch_size=512)
Ï€_a2c = solve(ğ’®_a2c, mdp)

# Solve with PPO
ğ’®_ppo = PGSolver(Ï€=ActorCritic(G(), V()), S=S, N=100000, Î”N=2048, loss=ppo(), batch_size=512)
Ï€_ppo = solve(ğ’®_ppo, mdp)

# Solve with DQN
ğ’®_dqn = DQNSolver(Ï€=QS(), S=S, N=100000)
Ï€_dqn = solve(ğ’®_dqn, mdp)

# Solve with DDPG
ğ’®_ddpg = DDPGSolver(Ï€=ActorCritic(A(), QSA()), S=S, N=100000)
Ï€_ddpg = solve(ğ’®_ddpg, mdp)


# Plot the learning curve
p = plot_learning([ğ’®_reinforce, ğ’®_a2c, ğ’®_ppo, ğ’®_dqn, ğ’®_ddpg], title="Pendulum Swingup Training Curves", labels=["REINFORCE", "A2C", "PPO", "DQN", "DDPG"])

# Produce a gif with the final policy
gif(mdp, Ï€_dqn, "pendulum.gif", max_steps=200)

