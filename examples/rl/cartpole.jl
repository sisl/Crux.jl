using POMDPs, Crux, Flux, POMDPGym

## Cartpole - V0
mdp = GymPOMDP(:CartPole, version = :v1)
as = actions(mdp)
S = state_space(mdp)

A() = DiscreteNetwork(Chain(Dense(Crux.dim(S)..., 64, relu), Dense(64, 64, relu), Dense(64, length(as))), as)
V() = ContinuousNetwork(Chain(Dense(Crux.dim(S)..., 64, relu), Dense(64, 64, relu), Dense(64, 1)))
SoftA(Î±::Float32) = SoftDiscreteNetwork(Chain(Dense(Crux.dim(S)..., 64, relu), Dense(64, 64, relu), Dense(64, length(as))), as;Î±=Î±)

# collection and c_opt_epoch optimization
Î”Ns=[1,2,4]
epochs = [1,5,10,50]
mix = Iterators.product(Î”Ns,epochs)  
ğ’®_sqls_2 = [SoftQ(Ï€=SoftA(Float32(0.5)), S=S, N=10000, 
    Î”N=dn, c_opt=(;epochs=e), interaction_storage=[]) for (dn,e) in mix]
Ï€_sqls_2 = [@time solve(x, mdp) for x in ğ’®_sqls_2]
p = plot_learning(ğ’®_sqls_2, title = "CartPole-V0 SoftQ Tradeoff Curves", 
    labels = ["SQL Î”N=($dn),ep=($e)" for (dn,e) in mix])
Crux.savefig(p, "examples/rl/cartpole_soft_q_tradeoffs.pdf")


# Solve with REINFORCE (~2 seconds)
ğ’®_reinforce = REINFORCE(Ï€=A(), S=S, N=10000, Î”N=500, a_opt=(epochs=5,), interaction_storage=[])
@time Ï€_reinforce = solve(ğ’®_reinforce, mdp)

# Solve with A2C (~8 seconds)
ğ’®_a2c = A2C(Ï€=ActorCritic(A(), V()), S=S, N=10000, Î”N=500)
@time Ï€_a2c = solve(ğ’®_a2c, mdp)

# Solve with PPO (~15 seconds)
ğ’®_ppo = PPO(Ï€=ActorCritic(A(), V()), S=S, N=10000, Î”N=500)
@time Ï€_ppo = solve(ğ’®_ppo, mdp)

# Solve with DQN (~12 seconds)
ğ’®_dqn = DQN(Ï€=A(), S=S, N=10000, interaction_storage=[])
@time Ï€_dqn = solve(ğ’®_dqn, mdp)

# Solve with SoftQLearning w/ varying Î± (~12 seconds)
Î±s = Vector{Float32}([1,0.5,0.2,0.1])
ğ’®_sqls = [SoftQ(Ï€=SoftA(Î±), S=S, N=10000, interaction_storage=[]) for Î± in Î±s]
Ï€_sqls = [@time solve(ğ’®_sqls[i], mdp) for i=1:length(Î±s)]

# Plot the learning curve
p = plot_learning([ğ’®_reinforce, ğ’®_a2c, ğ’®_ppo, ğ’®_dqn, ğ’®_sqls...], title = "CartPole-V0 Training Curves", 
    labels = ["REINFORCE", "A2C", "PPO", "DQN", ["SQL ($i)" for i in Î±s]...])
Crux.savefig(p, "examples/rl/cartpole_training.pdf")

# Produce a gif with the final policy
gif(mdp, Ï€_ppo, "cartpole_policy.gif", max_steps=100)






## Optional - Save data for imitation learning
# using BSON
# s = Sampler(mdp, ğ’®_dqn.Ï€, max_steps=100, required_columns=[:t])
# 
# data = steps!(s, Nsteps=10000)
# sum(data[:r])/100
# data[:expert_val] = ones(Float32, 1, 10000)
# data[:a]
# 
# data = ExperienceBuffer(data)
# BSON.@save "examples/il/expert_data/cartpole.bson" data

