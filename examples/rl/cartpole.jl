using POMDPs, Crux, Flux, POMDPGym

## Cartpole - V0
mdp = GymPOMDP(:CartPole, version = :v0)
as = actions(mdp)
S = state_space(mdp)

A() = DiscreteNetwork(Chain(Dense(Crux.dim(S)..., 64, relu), Dense(64, 64, relu), Dense(64, length(as))), as)
V() = ContinuousNetwork(Chain(Dense(Crux.dim(S)..., 64, relu), Dense(64, 64, relu), Dense(64, 1)))

# Solve with REINFORCE (~2 seconds)
ğ’®_reinforce = REINFORCE(Ï€=A(), S=S, N=10000, Î”N=500, a_opt=(epochs=5,), interaction_storage=[])
@time Ï€_reinforce = solve(ğ’®_reinforce, mdp)

# Solve with A2C (~8 seconds)
ğ’®_a2c = A2C(Ï€=ActorCritic(A(), V()), S=S, N=10000, Î”N=500)
@time Ï€_a2c = solve(ğ’®_a2c, mdp)

# Solve with PPO (~15 seconds)
ğ’®_ppo = PPO(Ï€=ActorCritic(A(), V()), S=S, N=10000, Î”N=500)
@time Ï€_ppo = solve(ğ’®_ppo, mdp)

# Solve with DQN (~12 seconds)
ğ’®_dqn = DQN(Ï€=A(), S=S, N=10000, interaction_storage=[])
@time Ï€_dqn = solve(ğ’®_dqn, mdp)

# Plot the learning curve
p = plot_learning([ğ’®_reinforce, ğ’®_a2c, ğ’®_ppo, ğ’®_dqn], title = "CartPole-V0 Training Curves", labels = ["REINFORCE", "A2C", "PPO", "DQN"])

# Produce a gif with the final policy
gif(mdp, Ï€_ppo, "cartpole_policy.gif", max_steps=100)

## Optional - Save data for imitation learning
# using BSON
# s = Sampler(mdp, ğ’®_dqn.Ï€, max_steps=100, required_columns=[:t])
# 
# data = steps!(s, Nsteps=10000)
# sum(data[:r])/100
# data[:expert_val] = ones(Float32, 1, 10000)
# data[:a]
# 
# data = ExperienceBuffer(data)
# BSON.@save "examples/il/expert_data/cartpole.bson" data

