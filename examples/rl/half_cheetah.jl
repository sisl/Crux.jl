# Note: This setup should get at least the performance of the openai-spinning up benchmarks
using Crux, Flux, POMDPs, POMDPGym, Distributions
import POMDPPolicies:FunctionPolicy
using Random
init_mujoco_render() # Required for visualization

# Construct the Mujoco environment
mdp = GymPOMDP(:HalfCheetah, version = :v3)
logmdp = GymPOMDP(:HalfCheetah, version = :v3)
S = state_space(mdp)
adim = length(POMDPs.actions(mdp)[1])
amin = -1*ones(Float32, adim)
amax = 1*ones(Float32, adim)
rand_policy = FunctionPolicy((s) -> Float32.(rand.(Uniform.(amin, amax))))

# Initializations that match the default PyTorch initializations
Winit(out, in) = Float32.(rand(Uniform(-sqrt(1/in), sqrt(1/in)), out, in))
binit(in) = (out) -> Float32.(rand(Uniform(-sqrt(1/in), sqrt(1/in)), out))

# Build the networks
idim = S.dims[1] + adim
# Networks for on-policy algorithms
Î¼() = ContinuousNetwork(Chain(Dense(S.dims[1], 64, tanh, initW = Winit, initb = binit(S.dims[1])), 
                              Dense(64, 32, tanh, initW = Winit, initb = binit(64)), 
                              Dense(32, adim, initW = Winit, initb = binit(32))))
V() = ContinuousNetwork(Chain(Dense(S.dims[1], 64, tanh, initW = Winit, initb = binit(S.dims[1])), 
                              Dense(64, 32, initW = Winit, initb = binit(64)), 
                             Dense(32, 1, initW = Winit, initb = binit(32))))
log_std() = -0.5f0*ones(Float32, adim)

# Networks for off-policy algorithms
Q() = ContinuousNetwork(Chain(Dense(idim, 256, relu, initW = Winit, initb = binit(idim)), 
            Dense(256, 256, relu, initW = Winit, initb = binit(256)), 
            Dense(256, 1, initW = Winit, initb = binit(256)))) 
A() = ContinuousNetwork(Chain(Dense(S.dims[1], 256, relu, initW = Winit, initb = binit(S.dims[1])), 
            Dense(256, 256, relu, initW = Winit, initb = binit(256)), 
            Dense(256, 6, tanh, initW = Winit, initb = binit(256)))) 
function SAC_A()
    base = Chain(Dense(S.dims[1], 256, relu, initW = Winit, initb = binit(S.dims[1])), 
                Dense(256, 256, relu, initW = Winit, initb = binit(256)))
    mu = ContinuousNetwork(Chain(base..., Dense(256, 6, initW = Winit, initb = binit(256))))
    logÎ£ = ContinuousNetwork(Chain(base..., Dense(256, 6, initW = Winit, initb = binit(256))))
    SquashedGaussianPolicy(mu, logÎ£)
end

## Setup params
shared = (max_steps=1000, N=Int(1e6), S=S)
on_policy = (Î”N=4000, 
             Î»_gae=0.97, 
             a_opt=(batch_size=4000, epochs=80, optimizer=ADAM(3e-4)), 
             c_opt=(batch_size=4000, epochs=80, optimizer=ADAM(1e-3)))
off_policy = (Î”N=50,
              max_steps=1000,
              log=(period=4000, fns=[log_undiscounted_return(3)]),
              buffer_size=Int(1e6), 
              buffer_init=1000, 
              c_opt=(batch_size=100, optimizer=ADAM(1e-3)),
              a_opt=(batch_size=100, optimizer=ADAM(1e-3)), 
              Ï€_explore=FirstExplorePolicy(10000, rand_policy, GaussianNoiseExplorationPolicy(0.1f0, a_min=amin, a_max=amax)))

## Run solvers 
ğ’®_ppo = PPO(;Ï€=ActorCritic(GaussianPolicy(Î¼(), log_std()), V()), Î»â‚‘=0f0, shared..., on_policy...)
solve(ğ’®_ppo, mdp)

# Solve with DDPG
ğ’®_ddpg = DDPG(;Ï€=ActorCritic(A(), Q()) |> gpu, shared..., off_policy...)  
solve(ğ’®_ddpg, mdp)

# Solve with TD3
ğ’®_td3 = TD3(;Ï€=ActorCritic(A(), DoubleNetwork(Q(), Q())) |> gpu, shared..., off_policy..., 
                  Ï€_smooth = GaussianNoiseExplorationPolicy(0.2f0, Ïµ_min = -0.5f0, Ïµ_max = 0.5f0, a_min = amin, a_max = amax))
solve(ğ’®_td3, mdp)

# Solve with SAC
ğ’®_sac = SAC(;Ï€=ActorCritic(SAC_A(), DoubleNetwork(Q(), Q())) |> gpu, shared..., off_policy...)
solve(ğ’®_sac, mdp)

using BSON
s = Sampler(mdp, ğ’®_sac.Ï€, S, max_steps=1000, required_columns=[:t])

data = steps!(s, Nsteps=10000)
sum(data[:r])/10
data[:expert_val] = ones(Float32, 1, 10000)

data = ExperienceBuffer(data)
BSON.@save "examples/il/expert_data/half_cheetah.bson" data

# Plot the learning curve
p = plot_learning([ğ’®_ppo, ğ’®_ddpg, ğ’®_td3, ğ’®_sac], title = "HalfCheetah Training Curves", labels = ["PPO", "DDPG", "TD3", "SAC"])
Crux.savefig("examples/rl/half_cheetah_benchmark.pdf")

# Produce a gif with the final policy
gif(mdp, ğ’®_ddpg.Ï€, "mujoco.gif")

