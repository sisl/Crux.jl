# Note: This setup should get at least the performance of the openai-spinning up benchmarks
using Crux, Flux, POMDPs, POMDPGym, Distributions
import POMDPPolicies:FunctionPolicy
using Random
init_mujoco_render() # Required for visualization

# Construct the Mujoco environment
mdp = GymPOMDP(:HalfCheetah, version = :v3)
S = state_space(mdp)
adim = length(POMDPs.actions(mdp)[1])
amin = -1*ones(Float32, adim)
amax = 1*ones(Float32, adim)
rand_policy = FunctionPolicy((s) -> Float32.(rand.(Uniform.(amin, amax))))

# Initializations that match the default PyTorch initializations
Winit(out, in) = Float32.(rand(Uniform(-sqrt(1/in), sqrt(1/in)), out, in))
binit(out, in) = Float32.(rand(Uniform(-sqrt(1/in), sqrt(1/in)), out))

# Build the networks
idim = S.dims[1] + adim
# Networks for on-policy algorithms
渭() = ContinuousNetwork(Chain(Dense(S.dims[1], 64, tanh, init=Winit, bias=binit(64, S.dims[1])), 
                              Dense(64, 32, tanh, init=Winit, bias=binit(32, 64)), 
                              Dense(32, adim, init=Winit, bias=binit(adim, 32))))
V() = ContinuousNetwork(Chain(Dense(S.dims[1], 64, tanh, init=Winit, bias=binit(64, S.dims[1])), 
                              Dense(64, 32, init=Winit, bias=binit(32, 64)), 
                             Dense(32, 1, init=Winit, bias=binit(1, 32))))
log_std() = -0.5f0*ones(Float32, adim)

# Networks for off-policy algorithms
Q() = ContinuousNetwork(Chain(Dense(idim, 256, relu, init=Winit, bias=binit(256, idim)), 
            Dense(256, 256, relu, init=Winit, bias=binit(256, 256)), 
            Dense(256, 1, init=Winit, bias=binit(1,256)))) 
A() = ContinuousNetwork(Chain(Dense(S.dims[1], 256, relu, init=Winit, bias=binit(256, S.dims[1])), 
            Dense(256, 256, relu, init=Winit, bias=binit(256, 256)), 
            Dense(256, 6, tanh, init=Winit, bias=binit(6, 256)))) 
function SAC_A()
    base = Chain(Dense(S.dims[1], 256, relu, init=Winit, bias=binit(256, S.dims[1])), 
                Dense(256, 256, relu, init=Winit, bias=binit(256, 256)))
    mu = ContinuousNetwork(Chain(base..., Dense(256, 6, init=Winit, bias=binit(6, 256))))
    log危 = ContinuousNetwork(Chain(base..., Dense(256, 6, init=Winit, bias=binit(6, 256))))
    SquashedGaussianPolicy(mu, log危)
end

## Setup params
shared = (max_steps=1000, N=Int(3e6), S=S)
on_policy = (N=4000, 
             位_gae=0.97, 
             a_opt=(batch_size=4000, epochs=80, optimizer=ADAM(3e-4)), 
             c_opt=(batch_size=4000, epochs=80, optimizer=ADAM(1e-3)))
off_policy = (N=50,
              max_steps=1000,
              log=(period=4000, fns=[log_undiscounted_return(3)]),
              buffer_size=Int(1e6), 
              buffer_init=1000, 
              c_opt=(batch_size=100, optimizer=ADAM(1e-3)),
              a_opt=(batch_size=100, optimizer=ADAM(1e-3)), 
              _explore=FirstExplorePolicy(10000, rand_policy, GaussianNoiseExplorationPolicy(0.1f0, a_min=amin, a_max=amax)))

## Run solvers 
_ppo = PPO(;=ActorCritic(GaussianPolicy(渭(), log_std()), V()), 位e=0f0, shared..., on_policy...)
solve(_ppo, mdp)

# Solve with DDPG
_ddpg = DDPG(;=ActorCritic(A(), Q()) |> gpu, shared..., off_policy...)  
solve(_ddpg, mdp)

# Solve with TD3
_td3 = TD3(;=ActorCritic(A(), DoubleNetwork(Q(), Q())) |> gpu, shared..., off_policy..., 
                  _smooth = GaussianNoiseExplorationPolicy(0.2f0, 系_min = -0.5f0, 系_max = 0.5f0, a_min = amin, a_max = amax))
solve(_td3, mdp)

# Solve with SAC
_sac = SAC(;=ActorCritic(SAC_A(), DoubleNetwork(Q(), Q())) |> gpu, shared..., off_policy...)
solve(_sac, mdp)

# Plot the learning curve
p = plot_learning([_ppo, _ddpg, _td3, _sac], title = "HalfCheetah Mujoco Training Curves", labels = ["PPO", "DDPG", "TD3", "SAC"])
Crux.savefig("examples/rl/half_cheetah_mujoco_benchmark.pdf")

# Produce a gif with the final policy
gif(mdp, _ddpg., "half_cheetah_mujoco.gif")

## Save trajectories for imitation learning
# using BSON
# s = Sampler(mdp, _ddpg., max_steps=1000, required_columns=[:t])
# 
# data = steps!(s, Nsteps=10000)
# sum(data[:r])/10
# data[:expert_val] = ones(Float32, 1, 10000)
# 
# data = ExperienceBuffer(data)
# BSON.@save "examples/il/expert_data/half_cheetah_mujoco.bson" data

