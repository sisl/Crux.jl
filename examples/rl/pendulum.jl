using POMDPs, Crux, Flux, POMDPGym
import POMDPPolicies:FunctionPolicy
import Distributions:Uniform
using Random
using Distributions

## Pendulum
mdp = PendulumPOMDP(actions=[-2., -0.5, 0, 0.5, 2.])
as = [actions(mdp)...]
amin = [-2f0]
amax = [2f0]
rand_policy = FunctionPolicy((s) -> Float32.(rand.(Uniform.(amin, amax))))
S = state_space(mdp, Ïƒ=[3.14f0, 8f0])

# Define the networks we will use
QSA() = ContinuousNetwork(Chain(Dense(3, 64, relu), Dense(64, 64, relu), Dense(64, 1)))
QS() = DiscreteNetwork(Chain(Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, length(as))), as)
V() = ContinuousNetwork(Chain(Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, 1)))
A() = ContinuousNetwork(Chain(Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, 1, tanh), x -> 2f0 * x), 1)
SG() = SquashedGaussianPolicy(ContinuousNetwork(Chain(Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, 1))), zeros(Float32, 1), 2f0)


# Solve with REINFORCE (Generally doesn't learn much, ~15 secs)
ğ’®_reinforce = REINFORCE(Ï€=SG(), S=S, N=100000, Î”N=2048, a_opt=(batch_size=512,))
@time Ï€_reinforce = solve(ğ’®_reinforce, mdp)

# Solve with A2C (Generally doesn't learn much, ~1 min)
ğ’®_a2c = A2C(Ï€=ActorCritic(SG(), V()), S=S, N=100000, Î”N=2048, a_opt=(batch_size=512,))
@time Ï€_a2c = solve(ğ’®_a2c, mdp)

# Solve with PPO (gets to > -200 reward, ~1.5 min)
ğ’®_ppo = PPO(Ï€=ActorCritic(SG(), V()), S=S, N=100000, Î”N=2048, a_opt=(batch_size=512,), Î»e=0f0)
@time Ï€_ppo = solve(ğ’®_ppo, mdp)

# Solve with DQN (gets to > -200 reward, ~30 sec)
ğ’®_dqn = DQN(Ï€=QS(), S=S, N=30000)
@time Ï€_dqn = solve(ğ’®_dqn, mdp)

off_policy = (S=S,
              Î”N=50,
              N=30000,
              buffer_size=Int(5e5),
              buffer_init=1000,
              c_opt=(batch_size=100, optimizer=ADAM(1e-3)),
              a_opt=(batch_size=100, optimizer=ADAM(1e-3)),
              Ï€_explore=FirstExplorePolicy(1000, rand_policy, GaussianNoiseExplorationPolicy(0.5f0, a_min=[-2.0], a_max=[2.0])))
              
# Solver with DDPG
ğ’®_ddpg = DDPG(;Ï€=ActorCritic(A(), QSA()), off_policy...)
@time Ï€_ddpg = solve(ğ’®_ddpg, mdp)

# Solve with TD3
ğ’®_td3 = TD3(;Ï€=ActorCritic(A(), DoubleNetwork(QSA(), QSA())), off_policy...)
@time Ï€_td3 = solve(ğ’®_td3, mdp)

# Solve with SAC
ğ’®_sac = SAC(;Ï€=ActorCritic(SG(), DoubleNetwork(QSA(), QSA())), off_policy...)
@time Ï€_sac = solve(ğ’®_sac, mdp)


# Plot the learning curve
p = plot_learning([ğ’®_reinforce, ğ’®_a2c, ğ’®_ppo, ğ’®_dqn, ğ’®_ddpg, ğ’®_td3, ğ’®_sac], title="Pendulum Swingup Training Curves", labels=["REINFORCE", "A2C", "PPO", "DQN", "DDPG", "TD3", "SAC"], legend=:right)
Crux.savefig("examples/rl/pendulum_benchmark.pdf")

# Produce a gif with the final policy
gif(mdp, Ï€_dqn, "pendulum.gif", max_steps=200)

## Save data for imitation learning
# using BSON
# s = Sampler(mdp, ğ’®_ppo.Ï€, max_steps=200, required_columns=[:t])
# 
# data = steps!(s, Nsteps=10000)
# sum(data[:r])/50
# data[:expert_val] = ones(Float32, 1, 10000)
# 
# data = ExperienceBuffer(data)
# BSON.@save "examples/il/expert_data/pendulum.bson" data

