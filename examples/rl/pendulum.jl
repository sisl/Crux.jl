using POMDPs, Crux, Flux, POMDPGym
import POMDPPolicies:FunctionPolicy
import Distributions:Uniform
using Random
using Distributions

## Pendulum
mdp = PendulumMDP(actions=[-2., -0.5, 0, 0.5, 2.])
as = [actions(mdp)...]
amin = [-1f0]
amax = [1f0]
rand_policy = FunctionPolicy((s) -> Float32.(rand.(Uniform.(amin, amax))))
S = state_space(mdp)

# Define the networks we will use
QSA() = ContinuousNetwork(Chain(x -> x ./ [6.3f0, 8f0, 2f0], Dense(3, 64, relu), Dense(64, 64, relu), Dense(64, 1)))
QS() = DiscreteNetwork(Chain(x -> x ./ [6.3f0, 8f0], Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, length(as))), as)
V() = ContinuousNetwork(Chain(x -> x ./ [6.3f0, 8f0], Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, 1)))
A() = ContinuousNetwork(Chain(x -> x ./ [6.3f0, 8f0], Dense(2, 64, relu), Dense(64, 64, relu), Dense(64, 1, tanh), x -> 2f0 * x), 1)

G() = GaussianPolicy(A(), zeros(Float32, 1))
function SAC_A()
    base = Chain(x -> x ./ [6.3f0, 8f0], Dense(2, 64, relu), Dense(64, 64, relu))
    mu = ContinuousNetwork(Chain(base..., Dense(64, 1)))
    logÎ£ = ContinuousNetwork(Chain(base..., Dense(64, 1)))
    SquashedGaussianPolicy(mu, logÎ£)
end


# Solve with REINFORCE (Generally doesn't learn much, ~15 secs)
ğ’®_reinforce = REINFORCE(Ï€=G(), S=S, N=100000, Î”N=2048, a_opt=(batch_size=512,))
@time Ï€_reinforce = solve(ğ’®_reinforce, mdp)

# Solve with A2C (Generally doesn't learn much, ~1 min)
ğ’®_a2c = A2C(Ï€=ActorCritic(G(), V()), S=S, N=100000, Î”N=2048, a_opt=(batch_size=512,))
@time Ï€_a2c = solve(ğ’®_a2c, mdp)

# Solve with PPO (gets to > -200 reward, ~1.5 min)
ğ’®_ppo = PPO(Ï€=ActorCritic(G(), V()), S=S, N=100000, Î”N=2048, a_opt=(batch_size=512,))
@time Ï€_ppo = solve(ğ’®_ppo, mdp)

# Solve with DQN (gets to > -200 reward, ~30 sec)
ğ’®_dqn = DQN(Ï€=QS(), S=S, N=30000)
@time Ï€_dqn = solve(ğ’®_dqn, mdp)

off_policy = (S=S,
              Î”N=50,
              N=30000,
              buffer_size=Int(5e5),
              buffer_init=1000,
              c_opt=(batch_size=100, optimizer=ADAM(1e-3)),
              a_opt=(batch_size=100, optimizer=ADAM(1e-3)),
              Ï€_explore=FirstExplorePolicy(1000, rand_policy, GaussianNoiseExplorationPolicy(0.5f0, a_min=[-2.0], a_max=[2.0])))
              
# Solver with DDPG (gets to > -200 reward, ~1 min)
ğ’®_ddpg = DDPG(;Ï€=ActorCritic(A(), QSA()), off_policy...)
@time Ï€_ddpg = solve(ğ’®_ddpg, mdp)

# Solve with TD3 (didn't learn much, ~1.5 min)
ğ’®_td3 = TD3(;Ï€=ActorCritic(A(), DoubleNetwork(QSA(), QSA())), off_policy...)
@time Ï€_td3 = solve(ğ’®_td3, mdp)

# Solve with TD3 (didn't learn much, ~1.5 min)
ğ’®_sac = SAC(;Ï€=ActorCritic(SAC_A(), DoubleNetwork(QSA(), QSA())), off_policy...)
@time Ï€_sac = solve(ğ’®_sac, mdp)

# Plot the learning curve
p = plot_learning([ğ’®_reinforce, ğ’®_a2c, ğ’®_ppo, ğ’®_dqn, ğ’®_ddpg, ğ’®_td3, ğ’®_sac], title="Pendulum Swingup Training Curves", labels=["REINFORCE", "A2C", "PPO", "DQN", "DDPG", "TD3", "SAC"], legend=:right)

# Produce a gif with the final policy
gif(mdp, Ï€_dqn, "pendulum.gif", max_steps=200)

