@with_kw mutable struct DQNSolver <: Solver 
    Ï€::DQNPolicy
    s_dim::Int
    a_dim::Int
    N::Int64 = 1000
    exploration_policy::ExplorationPolicy
    device = cpu
    L::Function = Flux.Losses.huber_loss
    opt = ADAM(1e-3)
    batch_size::Int = 32
    train_period::Int = 4 
    target_update_period::Int = 2000
    buffer_init::Int = max(batch_size, 200)
    log = LoggerParams(dir = "log/dqn", period = 500)
    buffer::ExperienceBuffer = ExperienceBuffer(mdp, 1000, device = device)
    rng::AbstractRNG = Random.GLOBAL_RNG
    i::Int64 = 1
end

target(Qâ», ğ’Ÿ, Î³) = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* maximum(Qâ»(ğ’Ÿ[:sp]), dims=1)

q_predicted(Ï€, ğ’Ÿ) = sum(value(Ï€, ğ’Ÿ[:s]) .* ğ’Ÿ[:a], dims = 1)

td_loss(Ï€, ğ’Ÿ, y, L) = L(q_predicted(Ï€, ğ’Ÿ), y)

td_error(Ï€, ğ’Ÿ, y) = abs.(q_predicted(Ï€, ğ’Ÿ) .- y)

#TODO: Look at RL class DQN pong for inspo on gpu usage and frame processing
function POMDPs.solve(ğ’®::DQNSolver, mdp; explore_offset = 0, extra_buffer = nothing)
    buffer = fill!(ExperienceBuffer, mdp, ğ’®.buffer.init, capacity = ğ’®.buffer.size, rng = ğ’®.rng)
    ğ’Ÿ = ExperienceBuffer(mdp, ğ’®.batch_size, device = ğ’®.device, Nelements = ğ’®.batch_size)
    s, Î³ = rand(ğ’®.rng, initialstate(mdp)) , Float32(discount(mdp))
    s = StepSampler(mdp)
    
    ğ’®.i == 1 && log(ğ’®.log, 0, mdp, ğ’®.Ï€, rng = ğ’®.rng)
    for i = 1:ğ’®.N
        # s = push_step!(buffer, mdp, s, ğ’®.Ï€, ğ’®.exploration_policy, ğ’®.i, rng = ğ’®.rng)
        push!(buffer, step!(sampler))
        
        rand!(ğ’®.rng, ğ’Ÿ, buffer)
        y = target(Qâ», ğ’Ÿ, Î³)
        ğ’®.buffer.prioritied && update_priorities!(buffer, ğ’Ÿ, td_error(ğ’®.Ï€, ğ’Ÿ, y))
        loss, grad = train!(ğ’®.Ï€, () -> td_loss(ğ’®.Ï€, ğ’Ÿ, y, ğ’®.L), ğ’®.opt, ğ’®.device)
        
        elapsed(ğ’®.i, ğ’®.target_update_period) && copyto!(Qâ», ğ’®.Ï€.Q)
        log(ğ’®.log, ğ’®.i, mdp, ğ’®.Ï€, data = [logloss(loss, grad), logexploration(ğ’®.exploration_policy, ğ’®.i)], rng = ğ’®.rng)
    end
    ğ’®.Ï€
end

# 
# function solve_multiple(ğ’®::DQNSolver, mdps...; buffer = nothing)
#     mdp = mdps[1]
#     Qâ» = deepcopy(ğ’®.Ï€.Q) |> ğ’®.device
#     ğ’Ÿ = ExperienceBuffer(mdp, ğ’®.batch_size, device = ğ’®.device, Nelements = ğ’®.batch_size)
#     svec, Î³, loss, grad = [rand(ğ’®.rng, initialstate(mdp)) for mdp in mdps] , Float32(discount(mdp)), NaN, NaN
# 
#     ğ’®.i == 1 && log(ğ’®.log, 0, mdps, ğ’®.Ï€, rng = ğ’®.rng)
#     for ğ’®.i = ğ’®.i : ğ’®.i + ğ’®.N - 1
#         #TODO: Add max steps per episode
#         for j =1:length(mdps)
#             ğ’®.Ï€.mdp = mdps[j]
#             svec[j] = push_step!(buffer, mdps[j], svec[j], ğ’®.Ï€, ğ’®.exploration_policy, ğ’®.i, rng = ğ’®.rng)
#         end
#         rand!(ğ’®.rng, ğ’Ÿ, buffer)
# 
#         if elapsed(ğ’®.i, ğ’®.train_freq)
#             y = target(Qâ», ğ’Ÿ, Î³)
#             loss, grad = train!(ğ’®.Ï€, () -> TDLoss(ğ’®.Ï€, ğ’Ÿ, y, ğ’®.L), ğ’®.opt, ğ’®.device)
#         end
# 
#         elapsed(ğ’®.i, ğ’®.target_update_period) && copyto!(Qâ», ğ’®.Ï€.Q)
#         log(ğ’®.log, ğ’®.i, mdp, ğ’®.Ï€, data = [logloss(loss, grad), logexploration(ğ’®.exploration_policy, ğ’®.i)], rng = ğ’®.rng)
#     end
#     ğ’®.Ï€
# end

