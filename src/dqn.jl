@with_kw mutable struct DQNSolver <: Solver 
    Ï€::DQNPolicy
    s_dim::Int
    a_dim::Int
    N::Int = 1000
    exploration_policy::ExplorationPolicy
    device = cpu
    rng::AbstractRNG = Random.GLOBAL_RNG
    L::Function = Flux.Losses.huber_loss
    opt = ADAM(1e-3)
    batch_size::Int = 32
    Î”train::Int = 4 
    Î”target_update::Int = 2000
    buffer_init::Int = max(batch_size, 200)
    log = LoggerParams(dir = "log/dqn", period = 500, rng = rng)
    buffer::ExperienceBuffer = ExperienceBuffer(mdp, 1000, device = device)
    i::Int = 1
end

target(Qâ», ğ’Ÿ, Î³::Float32) = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* maximum(Qâ»(ğ’Ÿ[:sp]), dims=1)

q_predicted(Ï€, ğ’Ÿ) = sum(value(Ï€, ğ’Ÿ[:s]) .* ğ’Ÿ[:a], dims = 1)

td_loss(Ï€, ğ’Ÿ, y, L) = L(q_predicted(Ï€, ğ’Ÿ), y)

td_error(Ï€, ğ’Ÿ, y) = abs.(q_predicted(Ï€, ğ’Ÿ) .- y)

#TODO: Look at RL class DQN pong for inspo on gpu usage and frame processing
function POMDPs.solve(ğ’®::DQNSolver, mdp)
    # Log the pre-train performance
    ğ’®.i == 1 && log(ğ’®.log, 0, mdp, ğ’®.Ï€)
    
    # Initialize minibatch buffer and sampler
    ğ’Ÿ = ExperienceBuffer(mdp, ğ’®.batch_size, device = ğ’®.device, indices = prioritized(ğ’®.buffer))
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.max_steps, exploration_policy = ğ’®.exploration_policy, rng = ğ’®.rng)
    
    # Fill the buffer as needed
    ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i = ğ’®.i)
    
    for ğ’®.i = range(ğ’®.i, length = ğ’®.N, step = ğ’®.Î”train)
        # Take a step in the environment
        push!(ğ’®.buffer, steps!(s, i = ğ’®.i, Nsteps = ğ’®.Î”train))
        
        # Sample a minibatch
        rand!(ğ’®.rng, ğ’Ÿ, buffer, i = ğ’®.i)
        
        # Compute target, td_error and td_loss for backprop
        y = target(ğ’®.Qâ», ğ’Ÿ, Î³)
        prioritized(ğ’®.buffer) && update_priorities!(buffer, ğ’Ÿ.indices, td_error(ğ’®.Ï€, ğ’Ÿ, y))
        loss, grad = train!(ğ’®.Ï€, () -> td_loss(ğ’®.Ï€, ğ’Ÿ, y, ğ’®.L), ğ’®.opt, ğ’®.device)
        
        # Update target network 
        elapsed(ğ’®.i - ğ’®.Î”train + 1:ğ’®.i, ğ’®.Î”target_update) && copyto!(ğ’®.Qâ», ğ’®.Ï€.Q)
        
        # Log results
        log(ğ’®.log, ğ’®.i, mdp, ğ’®.Ï€, data = [logloss(loss, grad), logexploration(ğ’®.exploration_policy, ğ’®.i)])
    end
    ğ’®.Ï€
end

