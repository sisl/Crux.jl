@with_kw struct DQNSolver <: Solver 
    Q
    N::Int64
    exploration_policy::ExplorationPolicy
    L::Function = Flux.Losses.huber_loss
    opt = ADAM(1e-3)
    batch_size::Int = 32
    target_update_period::Int = 2000
    log = LoggerParams(dir = "log/dqn", period = 500)
    buffer::BufferParams = BufferParams(init = 200, size = 1000)
    device = cpu
    rng::AbstractRNG = Random.GLOBAL_RNG
end

target(Qâ», ğ’Ÿ, Î³) = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* maximum(Qâ»(ğ’Ÿ[:sp]), dims=1)

TDLoss(Q, ğ’Ÿ, y, L) = L(sum(Q(ğ’Ÿ[:s]) .* ğ’Ÿ[:a], dims = 1), y)


function POMDPs.solve(ğ’®::DQNSolver, mdp)
    policy = CategoricalPolicy(ğ’®.Q, mdp, device = ğ’®.device)
    Qâ» = deepcopy(ğ’®.Q) |> ğ’®.device
    buffer = ExperienceBuffer(mdp, RandomPolicy(mdp), ğ’®.buffer.init, ğ’®.buffer.size, rng = ğ’®.rng)
    ğ’Ÿ = ExperienceBuffer(mdp, ğ’®.batch_size, ğ’®.batch_size, device = ğ’®.device)
    s, Î³ = rand(ğ’®.rng, initialstate(mdp)) , Float32(discount(mdp))
    
    log(ğ’®.log, 0, mdp, policy, rng = ğ’®.rng)
    for i=1:ğ’®.N
        s = push!(buffer, mdp, s, policy, ğ’®.exploration_policy, i, rng = ğ’®.rng)
        
        rand!(ğ’®.rng, ğ’Ÿ, buffer)
        println("max reward: ", maximum(ğ’Ÿ[:r]) )
        Î¸ = Flux.params(policy, ğ’®.device)
        y = target(Qâ», ğ’Ÿ, Î³)
        Qin = network(policy, ğ’®.device)
        loss, back = Flux.pullback(() -> TDLoss(Qin, ğ’Ÿ, y, ğ’®.L), Î¸)
        grad = back(1f0)
        Flux.update!(ğ’®.opt, Î¸, grad)
        sync!(policy, ğ’®.device)
        
        elapsed(i, ğ’®.target_update_period) && copyto!(Qâ», policy.Q)
        log(ğ’®.log, i, mdp, policy, data = [logloss(loss, grad, Î¸), logexploration(ğ’®.exploration_policy, i)], rng = ğ’®.rng)
    end
    policy
end

