@with_kw mutable struct AdversarialOffPolicySolver <: Solver
    Ï€ # Policy
    S::AbstractSpace # State space
    A::AbstractSpace = action_space(Ï€) # Action space
    N::Int = 1000 # Number of environment interactions
    Î”N::Int = 4 # Number of interactions between updates
    max_steps::Int = 100 # Maximum number of steps per episode
    log::Union{Nothing, LoggerParams} = nothing # The logging parameters
    i::Int = 0 # The current number of environment interactions
    param_optimizers::Dict{Any, TrainingParams} = Dict() # Training parameters for the parameters
    a_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the actor
    c_opt::TrainingParams # Training parameters for the critic
    x_param_optimizers::Dict{Any, TrainingParams} = Dict() # Training parameters for the parameters
    x_a_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the actor
    x_c_opt::TrainingParams # Training parameters for the critic
    ğ’«::NamedTuple = (;) # Parameters of the algorithm
    desired_AP_ratio = 1 # Desired training ratio of protagonist to antagonist
    
    # Off-policy-specific parameters
    Ï€â» = deepcopy(Ï€)
    Ï€_explore::Policy # exploration noise
    target_update = (Ï€â», Ï€; kwargs...) -> polyak_average!(Ï€â», Ï€, 0.005f0) # Function for updating the target network
    target_fn # Target for critic regression with input signature (Ï€â», ğ’Ÿ, Î³; i)
    x_target_fn # Target for critic regression with input signature (Ï€â», ğ’Ÿ, Î³; i)
    buffer_size = 1000 # Size of the buffer
    required_columns = Symbol[]
    buffer::ExperienceBuffer = ExperienceBuffer(S, A, buffer_size, required_columns) # The replay buffer
    buffer_init::Int = max(c_opt.batch_size, 200) # Number of observations to initialize the buffer with
    extra_buffers = [] # extra buffers (i.e. for experience replay in continual learning)
    buffer_fractions = [1.0] # Fraction of the minibatch devoted to each buffer
end

function POMDPs.solve(ğ’®::AdversarialOffPolicySolver, mdp)
    # Construct the training buffer, constants, and sampler
    ğ’Ÿ = buffer_like(ğ’®.buffer, capacity=ğ’®.c_opt.batch_size, device=device(ğ’®.Ï€))
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€, S=ğ’®.S, A=ğ’®.A, max_steps=ğ’®.max_steps, Ï€_explore=ğ’®.Ï€_explore, required_columns=extra_columns(ğ’®.buffer))
    isnothing(ğ’®.log.sampler) && (ğ’®.log.sampler = s)

    # Log the pre-train performance
    log(ğ’®.log, ğ’®.i)

    # Fill the buffer with initial observations before training
    ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i=ğ’®.i, explore=true)
    
    N_antagonist = 1
    N_protagonist = 1
    
    antagonist_params = (antagonist(ğ’®.Ï€), antagonist(ğ’®.Ï€â»), ğ’®.x_target_fn, ğ’®.x_param_optimizers, ğ’®.x_a_opt, ğ’®.x_c_opt)
    protagonist_params = (protagonist(ğ’®.Ï€), protagonist(ğ’®.Ï€â»), ğ’®.target_fn, ğ’®.param_optimizers, ğ’®.a_opt, ğ’®.c_opt)
    
    # Loop over the desired number of environment interactions
    for ğ’®.i in range(ğ’®.i, stop=ğ’®.i + ğ’®.N - ğ’®.Î”N, step=ğ’®.Î”N)
        # Sample transitions into the replay buffer
        push!(ğ’®.buffer, steps!(s, Nsteps=ğ’®.Î”N, explore=true, i=ğ’®.i))
        
        infos = []
        
        train_over = []
        curr_ratio = N_antagonist / N_protagonist
        if curr_ratio <= ğ’®.desired_AP_ratio
            push!(train_over, antagonist_params)
            N_antagonist += 1
        end
        if curr_ratio >= ğ’®.desired_AP_ratio
            push!(train_over, protagonist_params)
            N_protagonist += 1
        end
        
        ## Loop over the antagonist and protagonist
        for (Ï€, Ï€â», target_fn, param_optimizers, a_opt, c_opt) in train_over
        
            # Loop over the desired number of training steps
            for epoch in 1:c_opt.epochs
                # Sample a batch
                rand!(ğ’Ÿ, ğ’®.buffer, ğ’®.extra_buffers..., fracs=ğ’®.buffer_fractions, i=ğ’®.i)
                
                # initialize the info
                info = Dict()
                
                # Compute the target
                y = target_fn(Ï€â», ğ’®.ğ’«, ğ’Ÿ, Î³, i=ğ’®.i)
                
                # Train parameters
                for (Î¸s, p_opt) in param_optimizers
                    train!(Î¸s, (;kwargs...) -> p_opt.loss(Ï€, ğ’®.ğ’«, ğ’Ÿ; kwargs...), p_opt, info=info)
                end
                
                # Train the critic
                if ((epoch-1) % c_opt.update_every) == 0
                    train!(critic(Ï€), (;kwargs...) -> c_opt.loss(Ï€, ğ’®.ğ’«, ğ’Ÿ, y; weighted=ispri, kwargs...), c_opt, info=info)
                end
                
                # Train the actor 
                if !isnothing(a_opt) && ((epoch-1) % a_opt.update_every) == 0
                    train!(actor(Ï€), (;kwargs...) -> a_opt.loss(Ï€, ğ’®.ğ’«, ğ’Ÿ; kwargs...), a_opt, info=info)
                
                    # Update the target network
                    ğ’®.target_update(Ï€â», Ï€)
                end
                
                # Store the training information
                push!(infos, info)
                
            end
            # If not using a separate actor, update target networks after critic training
            isnothing(a_opt) && ğ’®.target_update(Ï€â», Ï€, i=ğ’®.i + 1:ğ’®.i + ğ’®.Î”N)
            
        end # end loop over A and P
        
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, aggregate_info(infos))
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.Ï€
end

