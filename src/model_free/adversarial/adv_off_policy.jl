@with_kw mutable struct AdversarialOffPolicySolver <: Solver
    ğ’®_pro::OffPolicySolver # Solver parameters for the protagonist
    ğ’®_ant::OffPolicySolver # Solver parameters for the antagonist
    px::PolicyParams # Nominal disturbance policy
    train_pro_every::Int = 1
    train_ant_every::Int = 1
    log::Union{Nothing, LoggerParams} = nothing # The logging parameters
    i::Int = 0 # The current number of environment interactions
end

function POMDPs.solve(ğ’®::AdversarialOffPolicySolver, mdp)
    # make some assertions on the parameters that should be shared
    @assert ğ’®.ğ’®_pro.buffer == ğ’®.ğ’®_ant.buffer
    @assert ğ’®.ğ’®_pro.buffer_init == ğ’®.ğ’®_ant.buffer_init
    @assert ğ’®.ğ’®_pro.required_columns == ğ’®.ğ’®_ant.required_columns
    @assert ğ’®.ğ’®_pro.N == ğ’®.ğ’®_ant.N
    @assert ğ’®.ğ’®_pro.S == ğ’®.ğ’®_ant.S
    @assert ğ’®.ğ’®_pro.max_steps == ğ’®.ğ’®_ant.max_steps
    
    # define shared parameters for easy reference
    buffer = ğ’®.ğ’®_pro.buffer
    buffer_init = ğ’®.ğ’®_pro.buffer_init
    Î”N = ğ’®.ğ’®_pro.Î”N + ğ’®.ğ’®_ant.Î”N
    N = ğ’®.ğ’®_pro.N
    S = ğ’®.ğ’®_pro.S
    max_steps = ğ’®.ğ’®_pro.max_steps
    
    # Construct the training buffern for the protagonist and antagonist
    ğ’Ÿ_pro = buffer_like(buffer, capacity=ğ’®.ğ’®_protagonist.c_opt.batch_size, device=device(ğ’®.ğ’®_protagonist.Ï€))
    ğ’Ÿ_ant = buffer_like(buffer, capacity=ğ’®.ğ’®_protagonist.c_opt.batch_size, device=device(ğ’®.ğ’®_protagonist.Ï€))
    
    # Get the discount factor 
    Î³ = Float32(discount(mdp))
    
    # Two samplers, one for the traditional ap
    s_pro = Sampler(mdp, ğ’®.ğ’®_pro.agent, adversary=ğ’®.px, S=S, max_steps=max_steps, required_columns=extra_columns(buffer))
    s_ant = Sampler(mdp, ğ’®.ğ’®_pro.agent, adversary=ğ’®.ğ’®_ant.agent, S=S, max_steps=max_steps, required_columns=extra_columns(buffer))
    
    s_nom = Sampler(mdp_nom, ğ’®.Ï€, S=ğ’®.S, A=ğ’®.A, max_steps=ğ’®.max_steps, required_columns=setdiff(extra_columns(ğ’®.buffer), [:x]))
    push!(ğ’®.log.fns, (;kwargs...) -> Dict("nominal_undiscounted_return" => undiscounted_return(s_nom, Neps=10)))
    
    isnothing(ğ’®.log.sampler) && (ğ’®.log.sampler = s_pro)

    # Log the pre-train performance
    log(ğ’®.log, ğ’®.i)

    # Fill the buffer with initial observations before training
    ğ’®.i += fillto!(buffer, s_pro, buffer_init, i=ğ’®.i, explore=true)
    ğ’®.ğ’®_pro.i = ğ’®.i
    ğ’®.ğ’®_ant.i = ğ’®.i
    
    # Loop over the desired number of environment interactions
    for ğ’®.i in range(ğ’®.i, stop=ğ’®.i + N - Î”N, step=Î”N)
        # Update the per solver iteration
        ğ’®.ğ’®_pro.i = ğ’®.i
        ğ’®.ğ’®_ant.i = ğ’®.i
        
        # Sample transitions into the replay buffer
        push!(buffer, steps!(s_pro, Nsteps=ğ’®.ğ’®_pro.Î”N, explore=true, i=ğ’®.i))
        push!(buffer, steps!(s_ant, Nsteps=ğ’®.ğ’®_ant.Î”N, explore=true, i=ğ’®.i))
        
        # callback for potentially updating the buffer
        ğ’®.post_experience_callback(buffer) 
        
        # Train the networks
        infos = []
        elapsed(ğ’®.i + 1:ğ’®.i + Î”N, ğ’®.train_pro_every) && merge!(infos, train_step(ğ’®.ğ’®_pro, ğ’Ÿ_pro, Î³))
        elapsed(ğ’®.i + 1:ğ’®.i + Î”N, ğ’®.train_ant_every) && merge!(infos, train_step(ğ’®.ğ’®_ant, ğ’Ÿ_ant, Î³))
        
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + Î”N, aggregate_info(infos))
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.ğ’®_pro.i = ğ’®.i
    ğ’®.ğ’®_ant.i = ğ’®.i
    
    ğ’®.ğ’®_pro.Ï€
end