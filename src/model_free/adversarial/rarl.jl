function RARL_DQN_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
         -ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* maximum(value(Ï€, ğ’Ÿ[:sp]), dims=1)
end

function RARL_TD3_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; i) 
    ap, _ = exploration(ğ’«[:Ï€_smooth], ğ’Ÿ[:sp], Ï€_on=Ï€, i=i)
    y = -ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* min.(value(Ï€, ğ’Ÿ[:sp], ap)...)
end

RARL_DQN(;kwargs...) = DQN(;c_loss=td_loss(name=:x_Qavg, a_key=:x), prefix="x_", target_fn=RARL_DQN_target)
RARL_TD3(;kwargs...) = DQN(;c_loss=double_Q_loss(name=:x_Qavg, a_key=:x), prefix="x_", target_fn=RARL_TD3_target)

function RARL(;ğ’®_pro,
               ğ’®_ant,
               px,
               log::NamedTuple=(;), 
               train_pro_every::Int=1,
               train_ant_every::Int=1,
               buffer_size=1000, # Size of the buffer
               required_columns=Symbol[:x, :fail],
               buffer::ExperienceBuffer=ExperienceBuffer(ğ’®_pro.S, ğ’®_pro.agent.space, buffer_size, required_columns), # The replay buffer
               buffer_init::Int=max(max(ğ’®_pro.c_opt.batch_size, ğ’®_ant.c_opt.batch_size), 200) # Number of observations to initialize the buffer with
               )
        # Set buffer parameters to be consistent between solvers (Since buffer is shared)
        ğ’®_pro.required_columns = required_columns
        ğ’®_pro.buffer = buffer
        ğ’®_pro.buffer_init = buffer_init
        ğ’®_ant.required_columns = required_columns
        ğ’®_ant.buffer = buffer    
        ğ’®_ant.buffer_init = buffer_init    
        AdversarialOffPolicySolver(;ğ’®_pro=ğ’®_pro,
                                    ğ’®_ant=ğ’®_ant,
                                    px=px,
                                    train_pro_every=train_pro_every,
                                    train_ant_every=train_ant_every,
                                    log=LoggerParams(;dir="log/rarl", log...),)
end
