"""
Batch solver type.

Fields
======
- `agent::PolicyParams` Policy parameters ([`PolicyParams`](@ref))
- `S::AbstractSpace` State space
- `max_steps::Int = 100` Maximum number of steps per episode
- `ğ’Ÿ_train` Training data
- `param_optimizers::Dict{Any, TrainingParams} = Dict()` Training parameters for the parameters
- `a_opt::TrainingParams` Training parameters for the actor
- `c_opt::Union{Nothing, TrainingParams} = nothing` Training parameters for the discriminator
- `target_fn = nothing` the target function for value-based methods
- `target_update = (Ï€â», Ï€; kwargs...) -> polyak_average!(Ï€â», Ï€, 0.005f0)` Function for updating the target network
- `ğ’«::NamedTuple = (;)` Parameters of the algorithm
- `log::Union{Nothing, LoggerParams} = nothing` The logging parameters
- `required_columns = Symbol[]` Extra columns to sample
- `epoch = 0` Number of epochs of training
"""
@with_kw mutable struct BatchSolver <: Solver
    agent::PolicyParams
    S::AbstractSpace # State space
    max_steps::Int = 100 # Maximum number of steps per episode
    ğ’Ÿ_train # training data
    param_optimizers::Dict{Any, TrainingParams} = Dict() # Training parameters for the parameters
    a_opt::TrainingParams # Training parameters for the actor
    c_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the discriminator
    target_fn = nothing # the target function for value-based methods
    target_update = (Ï€â», Ï€; kwargs...) -> polyak_average!(Ï€â», Ï€, 0.005f0) # Function for updating the target network
    ğ’«::NamedTuple = (;) # Parameters of the algorithm
    log::Union{Nothing, LoggerParams} = nothing # The logging parameters
    required_columns = Symbol[] # Extra columns to sample
    epoch = 0 # Number of epochs of training
end

function POMDPs.solve(ğ’®::BatchSolver, mdp)
    Î³ = Float32(discount(mdp))
    # Sampler for logging performance
    s = Sampler(mdp, ğ’®.agent, S=ğ’®.S, max_steps=ğ’®.max_steps, required_columns=ğ’®.required_columns)
    isnothing(ğ’®.log.sampler) && (ğ’®.log.sampler = s)
    
    # Log initial performance
    log(ğ’®.log, ğ’®.epoch, ğ’®=ğ’®)
    
    # Loop over the number of epochs
    infos = []
    grad_steps = 0
    for ğ’®.epoch=ğ’®.epoch:ğ’®.epoch + ğ’®.a_opt.epochs
        minibatch_infos = [] # stores the info from each minibatch
        
        # Shuffle the experience buffer
        shuffle!(ğ’®.ğ’Ÿ_train)
        
        # Call train for each minibatch
        batches = partition(1:length(ğ’®.ğ’Ÿ_train), ğ’®.a_opt.batch_size)
        for batch in batches
            mb = minibatch(ğ’®.ğ’Ÿ_train, batch)
            info = Dict()
            
            # Train parameters
            for (Î¸s, p_opt) in ğ’®.param_optimizers
                train!(Î¸s, (;kwargs...) -> p_opt.loss(ğ’®.agent.Ï€, ğ’®.ğ’«, mb; kwargs...), p_opt, info=info)
            end
            
            # Compute target
            y = !isnothing(ğ’®.target_fn) ? ğ’®.target_fn(ğ’®.agent.Ï€â», ğ’®.ğ’«, mb, Î³) : nothing
            
            # Optionally train the critic
            if !isnothing(ğ’®.c_opt)
                train!(critic(ğ’®.agent.Ï€), (;kwargs...)->ğ’®.c_opt.loss(ğ’®.agent.Ï€, ğ’®.ğ’«, mb, y; kwargs...), ğ’®.c_opt, info=info)
                
                if !isnothing(y)
                    ğ’®.target_update(ğ’®.agent.Ï€â», ğ’®.agent.Ï€)
                end
            end 
            
            # Train the actor
            train!(actor(ğ’®.agent.Ï€), (;kwargs...)->ğ’®.a_opt.loss(ğ’®.agent.Ï€, ğ’®.ğ’«, mb; kwargs...), ğ’®.a_opt, info=info)
            
            grad_steps += 1
            log(ğ’®.log, grad_steps, info, ğ’®=ğ’®)
            
            push!(minibatch_infos, info)
        end
        push!(infos, aggregate_info(minibatch_infos))                
        
        # Early stopping
        ğ’®.a_opt.early_stopping(infos) && break
    end
    
    ğ’®.agent.Ï€
end



