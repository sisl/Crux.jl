@with_kw mutable struct BatchSolver <: Solver
    Ï€ # The policy to train
    S::AbstractSpace # State space
    A::AbstractSpace = action_space(Ï€) # Action space
    max_steps::Int = 100 # Maximum number of steps per episode
    ð’Ÿ_train # training data
    a_opt::TrainingParams # Training parameters for the actor
    c_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the discriminator
    log::Union{Nothing, LoggerParams} = nothing # The logging parameters
    required_columns = Symbol[] # Extra columns to sample
    epoch = 0 # Number of epochs of training
end

function POMDPs.solve(ð’®::BatchSolver, mdp)    
    # Sampler for logging performance
    s = Sampler(mdp, ð’®.Ï€, S=ð’®.S, A=ð’®.A, max_steps=ð’®.max_steps, required_columns=ð’®.required_columns)
    isnothing(ð’®.log.sampler) && (ð’®.log.sampler = s)
    
    # Log initial performance
    log(ð’®.log, ð’®.epoch)
    
    # Loop over the number of epochs
    infos = []
    for ð’®.epoch=ð’®.epoch:ð’®.epoch + ð’®.a_opt.epochs
        minibatch_infos = [] # stores the info from each minibatch
        
        # Shuffle the experience buffer
        shuffle!(ð’®.ð’Ÿ_train)
        
        # Call train for each minibatch
        batches = partition(1:length(ð’®.ð’Ÿ_train), ð’®.a_opt.batch_size)
        for batch in batches
            mb = minibatch(ð’®.ð’Ÿ_train, batch)
            # Train the actor
            info = train!(actor(ð’®.Ï€), (;kwargs...)->ð’®.a_opt.loss(ð’®.Ï€, mb; kwargs...), ð’®.a_opt)
            
            # Optionally train the critic
            if !isnothing(ð’®.c_opt)
                info_d = train!(critic(ð’®.Ï€), (;kwargs...)->ð’®.c_opt.loss(ð’®.Ï€, mb; kwargs...), ð’®.c_opt)
                info = merge(info, info_d)
            end 
            push!(minibatch_infos, info)
        end
        push!(infos, aggregate_info(minibatch_infos))                
        
        # Early stopping
        ð’®.a_opt.early_stopping(infos) && break
        
        # Log the results
        log(ð’®.log, ð’®.epoch+1, infos[end])
    end
    
    ð’®.Ï€
end

# Early stopping function that terminates training on validation error increase
function stop_on_validation_increase(Ï€, ð’Ÿ_val, loss; window=5)
    k = "validation_error"
    (infos) -> begin
        ve = loss(Ï€, ð’Ÿ_val) # Compute the validation error
        infos[end][k] = ve # store it
        N = length(infos)
        if length(infos) >= 2*window
            curr_window = mean([infos[i][k] for i=N-window+1:N])
            old_window = mean([infos[i][k] for i=N-2*window+1:N-window])
            return curr_window >= old_window # check if the error has gone up
        end
        false
    end
end
