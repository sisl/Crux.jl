function BatchSAC(;Ï€::ActorCritic{T, DoubleNetwork{ContinuousNetwork, ContinuousNetwork}}, S, A=action_space(Ï€), Î”N=50, SAC_Î±::Float32=1f0, SAC_H_target::Float32 = Float32(-prod(dim(action_space(Ï€)))), ð’Ÿ_train, SAC_Î±_opt::NamedTuple=(;), a_opt::NamedTuple=(;), c_opt::NamedTuple=(;), log::NamedTuple=(;), ð’«::NamedTuple=(;), param_optimizers=Dict(), normalize_training_data = true, kwargs...) where T
    normalize_training_data && (ð’Ÿ_train = normalize!(deepcopy(ð’Ÿ_train), S, A))
    ð’Ÿ_train = ð’Ÿ_train |> device(Ï€)
    
    ð’« = (SAC_log_Î± = [Base.log(SAC_Î±)], SAC_H_target = SAC_H_target, ð’«...)
    BatchSolver(;
        Ï€ = Ï€,
        S=S,
        A=A,
        ð’« = ð’«,
        ð’Ÿ_train=ð’Ÿ_train,
        log = LoggerParams(;dir = "log/batch_sac", log...),
        param_optimizers = Dict(Flux.params(ð’«[:SAC_log_Î±]) => TrainingParams(;loss=SAC_temp_loss, name="temp_", SAC_Î±_opt...), param_optimizers...),
        a_opt = TrainingParams(;loss=SAC_actor_loss, name="actor_", a_opt...),
        c_opt = TrainingParams(;loss=double_Q_loss, name="critic_", epochs=Î”N, c_opt...),
        target_fn = SAC_target(Ï€),
        kwargs...)
end

