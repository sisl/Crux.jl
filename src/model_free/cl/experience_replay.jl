"""
Experience replay buffer.
"""
function ExperienceReplay(;Ï€, 
                          S, 
                          ğ’«=(;),
                          A=action_space(Ï€), 
                          N_experience_replay,
                          ER_frac = 0.5,
                          solver=TD3, 
                          required_columns=Symbol[],  
                          c_opt=(;), 
                          a_opt=(;), 
                          replay_store_weight = (D)->1f0,
                          kwargs...)
                          
    buffer_er = ExperienceBuffer(S, A, N_experience_replay, [:weight, :value, required_columns...])
    ğ’« = (;buffer_er, ğ’«...)
    function cb(D; ğ’®, info=Dict())
        if ğ’®.agent.Ï€ isa LatentConditionedNetwork
            D[:value] = mean(value(critic(ğ’®.agent.Ï€), vcat(D[:s], D[:z]), D[:a]))
        else
            D[:value] = mean(value(ğ’®.agent.Ï€, D[:s], D[:a]))
        end
        D[:weight] .= replay_store_weight(D)
        push_reservoir!(buffer_er, D, weighted=true)
        info["Experience_size"] = length(buffer_er)
    end
    creg = action_value_regularization
    areg = action_regularization
    if Ï€ isa LatentConditionedNetwork
        creg = TIER_action_value_regularization
        areg = TIER_action_regularization
    end
    
    
    
    solver(;  Ï€=Ï€,
              S=S,
              ğ’«=ğ’«,
              required_columns=unique([:weight, required_columns...]),
              post_sample_callback=cb,
              c_opt=(;regularizer=BatchRegularizer(buffers=[buffer_er], batch_size=128, Î»=0.5f0, loss=creg), c_opt...),
              a_opt=(;regularizer=BatchRegularizer(buffers=[buffer_er], batch_size=128, Î»=0.5f0, loss=areg), a_opt...),
              extra_buffers=[buffer_er],
              buffer_fractions=[1.0-ER_frac, ER_frac],
              kwargs...
              )
end

