function TIER_td_loss(;loss=Flux.mse, name=:Qavg, s_key=:s, a_key=:a, weight=nothing)
    (Ï€, ğ’«, ğ’Ÿ, y; info=Dict(), z) -> begin
        Q = value(Ï€, vcat(ğ’Ÿ[s_key], z), ğ’Ÿ[a_key]) 
        
        # Store useful information
        Zygote.ignore() do
            info[name] = mean(Q)
        end
        
        loss(Q, y, agg = isnothing(weight) ? mean : weighted_mean(ğ’Ÿ[weight]))
    end
end

function TIER_double_Q_loss(;name1=:Q1avg, name2=:Q2avg, kwargs...)
    l1 = TIER_td_loss(;name=name1, kwargs...)
    l2 = TIER_td_loss(;name=name2, kwargs...)
    
    (Ï€, ğ’«, ğ’Ÿ, y; info=Dict(), z=ğ’Ÿ[:z]) -> begin
        .5f0*(l1(critic(Ï€).N1, ğ’«, ğ’Ÿ, y, info=info, z=z) + l2(critic(Ï€).N2, ğ’«, ğ’Ÿ, y, info=info,z=z))
    end
end


TIER_TD3_actor_loss(Ï€, ğ’«, ğ’Ÿ; info = Dict()) = -mean(value(critic(Ï€).N1, vcat(ğ’Ÿ[:s], ğ’Ÿ[:z]), action(actor(Ï€), vcat(ğ’Ÿ[:s], ğ’Ÿ[:z]))))

function TIER_TD3_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; i, z=ğ’Ÿ[:z]) 
    ap, _ = exploration(ğ’«[:Ï€_smooth], vcat(ğ’Ÿ[:sp], z), Ï€_on=actor(Ï€), i=i)
    y = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* min.(value(critic(Ï€), vcat(ğ’Ÿ[:sp], z), ap)...)
end

TIER_action_regularization(Ï€, ğ’Ÿ) = Flux.mse(action(actor(Ï€), vcat(ğ’Ÿ[:s], ğ’Ÿ[:z])), ğ’Ÿ[:a])
TIER_action_value_regularization(Ï€, ğ’Ÿ) = begin 
    v = value(critic(Ï€), vcat(ğ’Ÿ[:s], ğ’Ÿ[:z]), ğ’Ÿ[:a])
    v isa Tuple && (v = v[1])
    Flux.mse(v, ğ’Ÿ[:value])
end


function TIER(;Ï€, 
               observation_model,
               S, 
               Î”N,
               ğ’«=(;),
               A=action_space(Ï€), 
               buffer_size=1000, 
               latent_dim, 
               N_experience_replay, 
               N_experience_obs,
               ER_frac = 0.5,
               replay_store_weight = (D)->1f0,
               bayesian_inference,
               solver=TD3, 
               required_columns=Symbol[], 
               c_opt=(;), 
               a_opt=(;), 
               obs_opt=(;),
               a_loss=TIER_TD3_actor_loss,
               c_loss=TIER_double_Q_loss(),
               target_fn=TIER_TD3_target,
               zprior=MvNormal(zeros(latent_dim), I),
               kwargs...)
    # if args["bayesian_inference"]=="mcmc"
   	# 	z_dist = rand(z_dist, args["N_BI_samples"]) # prior for mcmc
    required_columns = unique([:weight, required_columns...])
    buffer = ExperienceBuffer(S, A, buffer_size, required_columns)
    buffer.data[:z] = zeros(Float32, latent_dim, capacity(buffer))
      
    # This experience buffer is for recalling behavior
    buffer_er = ExperienceBuffer(S, A, N_experience_replay, [required_columns..., :value])
    buffer_er.data[:z] = zeros(Float32, latent_dim, capacity(buffer_er))

    # this experience buffer is for learning the latent embedding
    buffer_obs = ExperienceBuffer(S, A, N_experience_obs, [required_columns..., :value])
    buffer_obs.data[:z] = zeros(Float32, latent_dim, capacity(buffer_obs))

    # Buffer used to train for latent
    obs_opt = TrainingParams(;obs_opt...)
    ğ’Ÿobs = buffer_like(buffer_obs, capacity=obs_opt.batch_size)
    
    ğ’« = (;buffer_er, buffer_obs, obs_opt, z_dist=Any[zprior], zs=Any[zprior], observation_model, ğ’«...)

    # Define the solver 
    ğ’® = solver(;Ï€=Ï€,
              S=S,
              ğ’«=ğ’«,
              Î”N=Î”N,
              buffer=buffer,
              required_columns=required_columns,
              c_opt=(;regularizer=BatchRegularizer(buffers=[buffer_er], batch_size=128, Î»=0.5f0, loss=TIER_action_value_regularization), c_opt...),
              a_opt=(;regularizer=BatchRegularizer(buffers=[buffer_er], batch_size=128, Î»=0.5f0, loss=TIER_action_regularization), a_opt...),
              extra_buffers = [buffer_er],
              buffer_fractions = [1.0 - ER_frac, ER_frac],
              a_loss=a_loss,
              c_loss=c_loss,
              target_fn=target_fn,
              kwargs...
              )

    function TIER_cb(D; ğ’®, info=Dict())
        # update the z distribution
        ğ’®.ğ’«.z_dist[1] = bayesian_inference(observation_model, D, ğ’®.ğ’«.z_dist[1], info=info)
        
        # Set the agent's best estimate and record
        zbest = Crux.best_estimate(ğ’®.ğ’«.z_dist[1])
        ğ’®.agent.Ï€.z = zbest
        push!(ğ’®.ğ’«[:zs], deepcopy(ğ’®.ğ’«.z_dist[1]))
        
        # Fill the buffer with latent estimate, value and computed weight
        D[:z] = repeat(zbest, 1, length(D[:r]))
        D[:value] = mean(value(critic(ğ’®.agent.Ï€), vcat(D[:s], D[:z]), D[:a]))
        D[:weight] .= replay_store_weight(D)
        
        # Add this buffer to our experience replay and observation buffers
        push_reservoir!(buffer_er, D, weighted=true)
        push_reservoir!(buffer_obs, D)
        
        # Train the obs model
        for j=1:obs_opt.epochs
                rand!(ğ’Ÿobs, buffer_obs, buffer, fracs=[0.5, 0.5])
                train!(Flux.params(observation_model), (;kwargs...) -> obs_opt.loss(observation_model, ğ’Ÿobs; kwargs...), obs_opt, info=info)
        end
    end

    ğ’®.post_sample_callback = TIER_cb
    ğ’®
end

