function value_estimate(Ï€::DiscreteNetwork, px, s) where {T <: DiscreteNetwork}
        pdfs = exp.(logpdf(px, s))
        sum(value(Ï€, s) .* pdfs, dims=1)
end

#NOTE: Currently all of these assume that we get a reward (cost) ONCE at the end of an episode

function E_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        px = ğ’«[:px]
        if ğ’«[:use_likelihood_weights]
                return ğ’Ÿ[:likelihoodweight] .* (ğ’Ÿ[:done] .* ğ’Ÿ[:r] .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp]))
        else
                return ğ’Ÿ[:done] .* ğ’Ÿ[:r] .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp])
        end
end

function CDF_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        rÎ± = ğ’«[:rÎ±][1]
        px = ğ’«[:px]
        
        if ğ’«[:use_likelihood_weights]
                return ğ’Ÿ[:likelihoodweight] .* (ğ’Ÿ[:done] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp]))
        else
                return ğ’Ÿ[:done] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp])
        end
end

function CVaR_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        rÎ± = ğ’«[:rÎ±][1]
        px = ğ’«[:px]
        if ğ’«[:use_likelihood_weights]
                return ğ’Ÿ[:likelihoodweight] .* (ğ’Ÿ[:done] .* ğ’Ÿ[:r] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp]))
        else
                return ğ’Ÿ[:done] .* ğ’Ÿ[:r] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp])
        end
end

function E_VaR_CVaR_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        [CDF_target(Ï€.networks[1], ğ’«, ğ’Ÿ, Î³; kwargs...), CVaR_target(Ï€.networks[2], ğ’«, ğ’Ÿ, Î³; kwargs...), E_target(Ï€.networks[3], ğ’«, ğ’Ÿ, Î³; kwargs...)]
end 

function VaR_CVaR_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        [CDF_target(Ï€.networks[1], ğ’«, ğ’Ÿ, Î³; kwargs...), CVaR_target(Ï€.networks[2], ğ’«, ğ’Ÿ, Î³; kwargs...)]
end


function CERL_Discrete(;Ï€::MixtureNetwork,
                        S,
                        N, 
                        px,
                        prioritized=true,
                        use_likelihood_weights=true, 
                        Î±,
                        ğ’«=(;),
                        buffer_size=N,
                        Î”N=4,
                        pre_train_callback,
                        Ï€_explore, 
                        c_opt::NamedTuple=(;), 
                        log::NamedTuple=(;),
                        c_loss,
                        kwargs...)
               
                    ğ’« = (;px, rÎ±=[NaN], Î±, use_likelihood_weights, ğ’«...)
                    required_columns=[:logprob, :likelihoodweight]
                    agent = PolicyParams(Ï€=Ï€, Ï€_explore=Ï€_explore, Ï€â»=deepcopy(Ï€), pa=px)
                    OffPolicySolver(;agent=agent,
                                     S=S,
                                     log=LoggerParams(;dir="log/cerl_dqn", period=100, fns=[log_episode_averages([:r], 100)], log...),
                                     ğ’«=ğ’«,
                                     N=N,
                                     Î”N=Î”N,
                                     pre_train_callback=pre_train_callback,
                                     buffer=ExperienceBuffer(S, agent.space, buffer_size, required_columns, prioritized=prioritized),
                                     c_opt = TrainingParams(;loss=c_loss, name="critic_", epochs=Î”N, c_opt...),
                                     target_fn=VaR_CVaR_target,
                                     kwargs...)
end

