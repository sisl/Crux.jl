function discrete_value_estimate(Ï€, px, s)
        pdfs = exp.(logpdf(px, s))
        sum(value(Ï€, s) .* pdfs, dims=1)
end

function CERL_double_loss(;name1=:Q1avg, name2=:Q2avg, kwargs...)
    l1 = td_loss(;name=name1, kwargs...)
    l2 = td_loss(;name=name2, kwargs...)
    
    (Ï€, ğ’«, ğ’Ÿ, y; info=Dict()) -> begin
        .5f0*(l1(Ï€.C.N1, ğ’«, ğ’Ÿ, y[1], info=info) + l2(Ï€.C.N2, ğ’«, ğ’Ÿ, y[2], info=info))
    end
end

#NOTE: Currently all of these assume that we get a reward (cost) ONCE at the end of an episode

function expected_reward_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        px = ğ’«[:px]
        ğ’Ÿ[:done] .* ğ’Ÿ[:r] .+ (1.f0 .- ğ’Ÿ[:done]) .* discrete_value_estimate(Ï€, px, ğ’Ÿ[:sp])
end

function expected_tail_reward_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        rÎ± = ğ’«[:rÎ±]
        px = ğ’«[:px]
        ğ’Ÿ[:done] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* discrete_value_estimate(Ï€.C.N1, px, ğ’Ÿ[:sp]) #CDF
        ğ’Ÿ[:done] .* ğ’Ÿ[:r] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* discrete_value_estimate(Ï€.C.N2, px, ğ’Ÿ[:sp]) #CVAR
end


function CERL_Discrete(;Ï€::DiscreteNetwork,
                        N, 
                        px,
                        ğ’«=(;),
                        buffer_size=N,
                        Î”N=4,
                        Ï€_explore, 
                        c_opt::NamedTuple=(;), 
                        log::NamedTuple=(;),
                        c_loss=CERL_double_loss(),
                        kwargs...)
               
                    ğ’« = (;px, rÎ±=NaN, ğ’«...)
                    OffPolicySolver(;agent=PolicyParams(Ï€=Ï€, Ï€_explore=Ï€_explore, Ï€â»=deepcopy(Ï€)), 
                                     log=LoggerParams(;dir="log/cerl_dqn", log...),
                                     ğ’«=ğ’«,
                                     N=N,
                                     Î”N=Î”N,
                                     buffer_size=buffer_size,
                                     c_opt = TrainingParams(;loss=c_loss, name="critic_", epochs=Î”N, c_opt...),
                                     target_fn=expected_tail_reward_target,
                                     kwargs...)
end

