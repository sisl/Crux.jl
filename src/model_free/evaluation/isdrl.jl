function value_estimate(Ï€::DiscreteNetwork, px, s, ğ’«)
        pdfs = logits(px, s)
        sum(value(Ï€, s) .* pdfs, dims=1)
end

function value_estimate(Ï€::ActorCritic, px, s, ğ’«)
        mean([value(Ï€, s, action(px, s)) for _ in 1:ğ’«[:N_samples]])
end

#NOTE: Currently all of these assume that we get a reward (cost) ONCE at the end of an episode

function E_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        px = ğ’«[:px]
        if ğ’«[:use_likelihood_weights]
                return ğ’Ÿ[:likelihoodweight] .* (ğ’Ÿ[:done] .* ğ’Ÿ[:r] .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp], ğ’«))
        else
                return ğ’Ÿ[:done] .* ğ’Ÿ[:r] .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp], ğ’«)
        end
end

function CDF_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        # rÎ± = ğ’«[:rÎ±][1]
        rs = ğ’«[:rs]
        # stdrÎ± = ğ’«[:std_rÎ±][1]
        # vard = Normal(rÎ±, stdrÎ±)
        px = ğ’«[:px]
        B = length(ğ’Ÿ[:r])
        
        y = hcat([ğ’Ÿ[:done] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€.policy, px, vcat(repeat([rÎ±], 1, B), ğ’Ÿ[:sp]), ğ’«) for rÎ± in rs]...)
        return y
        # return (ğ’Ÿ[:var_prob] .+ y) ./ 2f0
        # return ğ’Ÿ[:var_prob] 
        
        # if ğ’«[:use_likelihood_weights]
        #         return ğ’Ÿ[:likelihoodweight] .* (ğ’Ÿ[:done] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp], ğ’«))
        # else
        #          return ğ’Ÿ[:done] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp], ğ’«)
        #         # return ğ’Ÿ[:done] .* cdf.(vard, ğ’Ÿ[:r]) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp], ğ’«)
        # end
end

function CVaR_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        rÎ± = ğ’«[:rÎ±][1]
        # stdrÎ± = ğ’«[:std_rÎ±][1]
        # vard = Normal(rÎ±, stdrÎ±)
        px = ğ’«[:px]
        
        y = ğ’Ÿ[:done] .* ğ’Ÿ[:r] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp], ğ’«)
        return y 
        # return (ğ’Ÿ[:cvar_prob] .+ y) ./ 2f0
        # return ğ’Ÿ[:cvar_prob]
        
        # if ğ’«[:use_likelihood_weights]
        #         return ğ’Ÿ[:likelihoodweight] .* (ğ’Ÿ[:done] .* ğ’Ÿ[:r] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp], ğ’«))
        # else
        #         return ğ’Ÿ[:done] .* ğ’Ÿ[:r] .* (ğ’Ÿ[:r] .> rÎ±) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp], ğ’«)
        #         # return ğ’Ÿ[:done] .* ğ’Ÿ[:r] .* cdf.(vard, ğ’Ÿ[:r]) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, px, ğ’Ÿ[:sp], ğ’«)
        # end
end

function E_VaR_CVaR_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        [CDF_target(Ï€.networks[1], ğ’«, ğ’Ÿ, Î³; kwargs...), CVaR_target(Ï€.networks[2], ğ’«, ğ’Ÿ, Î³; kwargs...), E_target(Ï€.networks[3], ğ’«, ğ’Ÿ, Î³; kwargs...)]
end 

function VaR_CVaR_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
        [CDF_target(Ï€.networks[1], ğ’«, ğ’Ÿ, Î³; kwargs...), CVaR_target(Ï€.networks[2], ğ’«, ğ’Ÿ, Î³; kwargs...)]
end

function IS_L_KL_log(Ï€, ğ’«, ğ’Ÿ; info=Dict(), kwargs...)
        x, logqx = exploration(Ï€, ğ’Ÿ[:s])
        Q_s = Zygote.ignore() do 
                value_estimate(Ï€, ğ’«[:px], ğ’Ÿ[:s], ğ’«)
        end
        px = exp.(logpdf(ğ’«[:px], ğ’Ÿ[:s], x))
        Q_sx = value(Ï€, ğ’Ÿ[:s], x)
        
        qstar = px .* Q_sx #./ Q_s
        

        -mean(qstar .* logqx ./ exp.(logqx))
end



function fill_probs(D; ğ’®, info=Dict())
        rÎ± = ğ’®.ğ’«[:rÎ±][1]
        epies = !(D isa ExperienceBuffer) ? episodes(ExperienceBuffer(D)) : episodes(D)
        for ep in epies
                episode_range = ep[1]:ep[2]
                r = sum(D[:r][1, episode_range]) # total return
                varprob = r >= rÎ±
                cvarprob = r * varprob
                
                for i in reverse(episode_range)
                        D[:var_prob][:, i] .= varprob
                        D[:cvar_prob][:, i] .= cvarprob
                        
                        varprob = D[:likelihoodweight][1,i] * varprob
                        cvarprob = D[:likelihoodweight][1,i] * cvarprob
                end
        end
end
    






function ISDRL_Discrete(;Ï€,
                        S,
                        N, 
                        px,
                        N_cdf=10,
                        cdf_weights=ones(Float32, N_cdf) ./ N_cdf,
                        prioritized=true,
                        use_likelihood_weights=true, 
                        Î±,
                        target_fn=VaR_CVaR_target,
                        ğ’«=(;),
                        buffer_size=N,
                        Î”N=4,
                        pre_train_callback,
                        Ï€_explore=Ï€, 
                        c_opt::NamedTuple=(;), 
                        log::NamedTuple=(;),
                        c_loss,
                        kwargs...)
               
                    ğ’« = (;px, cdf_weights, rÎ±=Float32[NaN], rs=zeros(Float32, N_cdf), std_rÎ±=Float32[NaN], Î±, use_likelihood_weights, ğ’«...)
                    required_columns=[:logprob, :likelihoodweight, :var_prob, :cvar_prob]
                    agent = PolicyParams(Ï€=Ï€, Ï€_explore=Ï€_explore, Ï€â»=deepcopy(Ï€), pa=px)
                    OffPolicySolver(;agent=agent,
                                     S=S,
                                     log=LoggerParams(;dir="log/cerl_dqn", period=100, fns=[log_episode_averages([:r], 100)], log...),
                                     ğ’«=ğ’«,
                                     N=N,
                                     Î”N=Î”N,
                                     # post_sample_callback=fill_probs,
                                     pre_train_callback=pre_train_callback,
                                     buffer=ExperienceBuffer(S, agent.space, buffer_size, required_columns, prioritized=prioritized),
                                     c_opt = TrainingParams(;loss=c_loss, name="critic_", epochs=Î”N, c_opt...),
                                     target_fn=target_fn,
                                     kwargs...)
end


function ISDRL_Continuous(;Ï€::MixtureNetwork,
                        S,
                        N, 
                        px,
                        N_samples=10,
                        prioritized=true,
                        use_likelihood_weights=true, 
                        Î±,
                        ğ’«=(;),
                        buffer_size=N,
                        Î”N=4,
                        pre_train_callback,
                        Ï€_explore=Ï€, 
                        c_opt::NamedTuple=(;), 
                        a_opt::NamedTuple=(;), 
                        log::NamedTuple=(;),
                        c_loss,
                        a_loss = IS_L_KL_log,
                        kwargs...)
               
                    ğ’« = (;px, rÎ±=[NaN], Î±, use_likelihood_weights, N_samples, ğ’«...)
                    required_columns=[:logprob, :likelihoodweight]
                    agent = PolicyParams(Ï€=Ï€, Ï€_explore=Ï€_explore, Ï€â»=deepcopy(Ï€), pa=px)
                    OffPolicySolver(;agent=agent,
                                     S=S,
                                     log=LoggerParams(;dir="log/cerl_dqn", period=100, fns=[log_episode_averages([:r], 100)], log...),
                                     ğ’«=ğ’«,
                                     N=N,
                                     Î”N=Î”N,
                                     pre_train_callback=pre_train_callback,
                                     buffer=ExperienceBuffer(S, agent.space, buffer_size, required_columns, prioritized=prioritized),
                                     c_opt = TrainingParams(;loss=c_loss, name="critic_", epochs=Î”N, c_opt...),
                                     a_opt = TrainingParams(;loss=a_loss, name="actor_", a_opt...),
                                     target_fn=VaR_CVaR_target,
                                     kwargs...)
end

