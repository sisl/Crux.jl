@with_kw mutable struct ASAFSolver <: Solver
    Ï€ # Policy
    S::AbstractSpace # State space
    A::AbstractSpace = action_space(Ï€) # Action space
    N::Int = 1000 # Number of environment interactions
    Î”N::Int = 2000 # Number of interactions between updates
    max_steps::Int = 100 # Maximum number of steps per episode
    log::Union{Nothing, LoggerParams} = LoggerParams(;dir = "log/ASAF") # The logging parameters
    i::Int = 0 # The current number of environment interactions
    a_opt::TrainingParams # Training parameters for the actor
    required_columns = Symbol[]
    ğ’Ÿ_demo
end

function ASAF(;Ï€, S, A=action_space(Ï€), ğ’Ÿ_demo, normalize_demo::Bool=true, Î”N=50, Î»_orth=1f-4, a_opt::NamedTuple=(;), c_opt::NamedTuple=(;), log::NamedTuple=(;), kwargs...)
    normalize_demo && (ğ’Ÿ_demo = normalize!(deepcopy(ğ’Ÿ_demo), S, A))
    ğ’Ÿ_demo = ğ’Ÿ_demo |> device(Ï€)
    ASAFSolver(;Ï€=Ï€, 
                 S=S, 
                 A=A,
                 ğ’Ÿ_demo=ğ’Ÿ_demo,
                 Î”N=Î”N,
                 log=LoggerParams(;dir="log/ASAF", period=100, log...),
                 a_opt=TrainingParams(;name="actor_", loss=nothing, a_opt...), 
                 kwargs...)
end


function ASAF_actor_loss(Ï€G)
    (Ï€, ğ’Ÿ, ğ’Ÿ_demo; info=Dict()) -> begin
        Ï€sa_G = logpdf(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a])
        Ï€sa_E = logpdf(Ï€, ğ’Ÿ_demo[:s], ğ’Ÿ_demo[:a])
        Ï€Gsa_G = logpdf(Ï€G, ğ’Ÿ[:s], ğ’Ÿ[:a])
        Ï€Gsa_E = logpdf(Ï€G, ğ’Ÿ_demo[:s], ğ’Ÿ_demo[:a])
        e = mean(entropy(Ï€, ğ’Ÿ[:s]))
        
        ignore() do
            info[:entropy] = e
        end 
        
        L = Flux.mean(log.(1 .+ exp.(Ï€Gsa_E - Ï€sa_E))) + Flux.mean(log.(exp.(Ï€sa_G - Ï€Gsa_G)  .+ 1))  - 0.1f0*e
        # if !isnothing(ğ’Ÿ_nda)
        #     Ï€sa_NDA = logpdf(Ï€, ğ’Ÿ_nda[:s], ğ’Ÿ_nda[:a])
        #     Ï€Gsa_NDA = logpdf(Ï€G, ğ’Ÿ_nda[:s], ğ’Ÿ_nda[:a])
        #     L += Flux.mean(log.(1 .+ exp.(Ï€sa_NDA - Ï€Gsa_NDA)))
        # end
        L
    end
end

function POMDPs.solve(ğ’®::ASAFSolver, mdp)
    # Construct the training buffer, constants, and sampler
    ğ’Ÿ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.Î”N, ğ’®.required_columns, device=device(ğ’®.Ï€))
    s = Sampler(mdp, ğ’®.Ï€, S=ğ’®.S, A=ğ’®.A, max_steps=ğ’®.max_steps, Ï€_explore=ğ’®.Ï€, required_columns=ğ’®.required_columns)
    isnothing(ğ’®.log.sampler) && (ğ’®.log.sampler = s)

    # Log the pre-train performance
    log(ğ’®.log, ğ’®.i)

    # Loop over the desired number of environment interactions
    for ğ’®.i = range(ğ’®.i, stop=ğ’®.i + ğ’®.N - ğ’®.Î”N, step=ğ’®.Î”N)
        # Sample transitions into the batch buffer
        push!(ğ’Ÿ, steps!(s, Nsteps=ğ’®.Î”N, reset=true, explore=true, i=ğ’®.i))
        
        # Train the actor
        ğ’®.a_opt.loss = ASAF_actor_loss(deepcopy(ğ’®.Ï€))
        info = batch_train!(actor(ğ’®.Ï€), ğ’®.a_opt, ğ’Ÿ, ğ’®.ğ’Ÿ_demo)
        
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, info)
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.Ï€
end

