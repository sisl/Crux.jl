mse_action_loss() = (, ; kwargs...) -> Flux.mse(action(, [:s]), [:a])
function mse_value_loss(位e::Float32)
    (, ; kwargs...) -> begin
        eloss = -mean(entropy(, [:s]))
        mseloss = Flux.mse(value(, [:s]), [:value])
        位e*eloss + mseloss
    end
end
function logpdf_bc_loss(位e::Float32)
    (, ; info=Dict())->begin
        eloss = -mean(entropy(, [:s]))
        lloss = -mean(logpdf(, [:s], [:a]).*[:r])
        ignore() do
            info[:entropy] = -eloss
            info[:logpdf] = lloss
        end
        位e*eloss + lloss
    end
end

function BC(;, S, A=action_space(), _demo, normalize_demo::Bool=true, loss=nothing, validation_fraction=0.3, window=100, 位e::Float32=1f-3, opt::NamedTuple=(;), log::NamedTuple=(;), kwargs...)
    if isnothing(loss)
        loss =  isa ContinuousNetwork ? mse_action_loss() : logpdf_bc_loss(位e)
    end
    normalize_demo && (_demo = normalize!(deepcopy(_demo), S, A))
    _demo = _demo |> device()
    
    # Splite between train and validation sets
    shuffle!(_demo)
    _train, _validate = split(_demo, [1-validation_fraction, validation_fraction])
    
    BatchSolver(;=, 
              S=S,
              A=A,
              _train=_train, 
              a_opt=TrainingParams(;early_stopping=stop_on_validation_increase(, _validate, loss, window=window), loss=loss, opt...), 
              log=LoggerParams(;dir="log/bc", period=1, log...),
              kwargs...)
end

