@with_kw mutable struct BCSolver <: Solver
     # The policy to train
    S::AbstractSpace # State space
    A::AbstractSpace = action_space() # Action space
    max_steps::Int = 100 # Maximum number of steps per episode
    _expert # training data
    opt::TrainingParams # Training parameters
    log::Union{Nothing, LoggerParams} = nothing # The logging parameters
    i = 0 # Number of epochs of training
end

mse_bc_loss(, ; kwargs...) = Flux.mse(action(, [:s]), [:a])
function logpdf_bc_loss(位e::Float32)
    (, ; kwargs...)->begin
        eloss = -mean(entropy(, [:s]))
        lloss = -mean(logpdf(, [:s], [:a]))
        位e*eloss + lloss
    end
end

function BC(;, _expert, loss=nothing, validation_fraction=0.3, 位e::Float32=1f-3, opt::NamedTuple=(;), log::NamedTuple=(;), kwargs...)
    if isnothing(loss)
        loss =  isa ContinuousNetwork ? mse_bc_loss : logpdf_bc_loss(位e)
    end
    shuffle!(_expert)
    _train, _validate = split(_expert, [1-validation_fraction, validation_fraction])    
    BCSolver(;=, 
              _expert=_train, 
              opt=TrainingParams(;early_stopping=stop_on_validation_increase(, _validate, loss), loss=loss, opt...), 
              log=LoggerParams(;dir="log/bc", period=1, log...),
              kwargs...)
end

function POMDPs.solve(::BCSolver, mdp)
    # Minibatch buffer
     = ExperienceBuffer(.S, .A, .opt.batch_size, device=device(.))
    
    # Sampler for logging performance
    s = Sampler(mdp, ., .S, .A, max_steps=.max_steps)
    
    # Loop over the number of epochs
    infos = []
    for .i=.i:.i + .opt.epochs
        rand!(, ._expert) # fill minibatch buffer
        push!(infos, train!(., (;kwargs...)->.opt.loss(., ; kwargs...), .opt)) # take training step
        .opt.early_stopping(infos) && break # Stop early
        log(.log, .i, infos[end], s=s) # Log the results
    end
    
    .
end

