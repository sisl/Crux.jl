function GAIL_D_loss(gan_loss)
    (D, ğ’«, ğ’Ÿ_ex, ğ’Ÿ_Ï€; info = Dict()) ->begin
        Lá´°(gan_loss, D, ğ’Ÿ_ex[:a], ğ’Ÿ_Ï€[:a], yD = (ğ’Ÿ_ex[:s],), yG = (ğ’Ÿ_Ï€[:s],))
    end
end
    

function OnPolicyGAIL(;Ï€, S, A=action_space(Ï€), ğ’Ÿ_demo, normalize_demo::Bool=true, D::ContinuousNetwork, solver=PPO, gan_loss::GANLoss=GAN_BCELoss(), d_opt::NamedTuple=(;), log::NamedTuple=(;),  kwargs...)
    d_opt = TrainingParams(;loss = GAIL_D_loss(gan_loss), name="discriminator_", d_opt...)
    normalize_demo && (ğ’Ÿ_demo = normalize!(deepcopy(ğ’Ÿ_demo), S, A))
    ğ’Ÿ_demo = ğ’Ÿ_demo |> device(Ï€)
    
    function GAIL_callback(ğ’Ÿ; info=Dict())
        batch_train!(D, d_opt, (;), ğ’Ÿ_demo, ğ’Ÿ, info=info)
        
        discriminator_signal = haskey(ğ’Ÿ, :advantage) ? :advantage : :return
        D_out = value(D, ğ’Ÿ[:a], ğ’Ÿ[:s])
        r = Base.log.(sigmoid.(D_out) .+ 1f-5) .- Base.log.(1f0 .- sigmoid.(D_out) .+ 1f-5)
        ğ’Ÿ[discriminator_signal] .= r # This is swapped because a->x and s->y and the convention for GANs is D(x,y)
    end
    ğ’® = solver(;Ï€=Ï€, S=S, A=A, post_batch_callback=GAIL_callback, log=(dir="log/onpolicygail", period=500, log...), kwargs...)
    ğ’®.c_opt = nothing # disable the critic 
    ğ’®
end

function OffPolicyGAIL(;Ï€, S, A=action_space(Ï€), ğ’Ÿ_demo, ğ’Ÿ_ndas::Array{ExperienceBuffer} = ExperienceBuffer[], normalize_demo::Bool=true, D::ContinuousNetwork, solver=SAC, d_opt::NamedTuple=(epochs=5,), log::NamedTuple=(;), kwargs...)
    d_opt = TrainingParams(;name="discriminator_", loss=()->nothing, d_opt...)
    
    dev = device(Ï€)
    normalize_demo && (ğ’Ÿ_demo = normalize!(deepcopy(ğ’Ÿ_demo), S, A))
    ğ’Ÿ_demo = ğ’Ÿ_demo |> dev
    N_nda = length(ğ’Ÿ_ndas)
    Î»_nda = Float32(-1 / N_nda)
    N_datasets = 2 + N_nda
    for i in 1:N_nda
        ğ’Ÿ_ndas[i] = normalize_demo && normalize!(deepcopy(ğ’Ÿ_ndas[i]), S, A)
        ğ’Ÿ_ndas[i] = ğ’Ÿ_ndas[i] |> dev
    end

    
    ğ’® = solver(;Ï€=Ï€, S=S, A=A, 
            post_experience_callback=(ğ’Ÿ; kwargs...) -> ğ’Ÿ[:r] .= 0, 
            log=(dir="log/offpolicygail", period=500, log...),
            kwargs...)
            
    
    
    B = d_opt.batch_size
    ğ’Ÿ_batch = buffer_like(ğ’®.buffer, capacity=B, device=dev)
    
    ğ’Ÿ_demo_batch = deepcopy(ğ’Ÿ_batch)
    ğ’Ÿ_demo_Ï€_batch = deepcopy(ğ’Ÿ_batch)
    ğ’Ÿ_ndas_batch = [deepcopy(ğ’Ÿ_batch) for ğ’Ÿ_nda in ğ’Ÿ_ndas]
    ğ’Ÿ_ndas_Ï€_batch = [deepcopy(ğ’Ÿ_batch) for ğ’Ÿ_nda in ğ’Ÿ_ndas]
    
    function GAIL_callback(ğ’Ÿ; info=Dict())
        for i=1:d_opt.epochs
            # Sample minibatchs
            rand!(ğ’Ÿ_demo_batch, ğ’Ÿ_demo)
            # rand!(ğ’Ÿ_demo_Ï€_batch, ğ’Ÿ_demo)
            # ğ’Ÿ_demo_Ï€_batch.data[:a] = action(Ï€, ğ’Ÿ_demo_Ï€_batch[:s])
            rand!(ğ’Ÿ_batch, ğ’®.buffer)
            for i in 1:N_nda
                rand!(ğ’Ÿ_ndas_batch[i], ğ’Ÿ_ndas[i])
                # rand!(ğ’Ÿ_ndas_Ï€_batch[i], ğ’Ÿ_ndas[i])
                # ğ’Ÿ_ndas_Ï€_batch[i].data[:a] = action(Ï€, ğ’Ÿ_ndas_Ï€_batch[i][:s])
            end
            
            ğ’Ÿ_full = hcat(ğ’Ÿ_demo_batch, ğ’Ÿ_batch, ğ’Ÿ_ndas_batch...)
            
            # concat inputs
            x = cat(ğ’Ÿ_full[:s], ğ’Ÿ_full[:a], dims=1)
            
            # Add some noise
            x .+= Float32.(rand(Normal(0, 0.2f0), size(x))) |> dev
            
            # Create labels
            y_demo = Flux.onehotbatch(ones(Int, B), 1:N_datasets)
            # y_demo_Ï€ = Flux.onehotbatch(2*ones(Int, B), 1:N_datasets)
            y_Ï€ = Flux.onehotbatch(2*ones(Int, B), 1:N_datasets)
            y_ndas = [Flux.onehotbatch((i+2)*ones(Int, B), 1:N_datasets) for i=1:N_nda]
            # y_ndas_Ï€ = [Flux.onehotbatch(2*ones(Int, B), 1:N_datasets) for i=1:N_nda]
            
            y = cat(y_demo, y_Ï€, y_ndas..., dims=2) |> dev
            
            
            # println(gradient_penalty(D, x_demo, x_Ï€))
            # + 10f0 * gradient_penalty(D, x_demo, x_Ï€)
            train!(D, (;kwargs...) -> Flux.Losses.logitcrossentropy(D(x), y), d_opt, info=info)
        end
        
        # ## replace the bufffer
        # rand!(ğ’Ÿ_demo_batch, ğ’Ÿ_demo)
        # # rand!(ğ’Ÿ_demo_Ï€_batch, ğ’Ÿ_demo)
        # # ğ’Ÿ_demo_Ï€_batch.data[:a] = action(Ï€, ğ’Ÿ_demo_Ï€_batch[:s])
        # rand!(ğ’Ÿ_batch, ğ’®.buffer)
        # for i in 1:N_nda
        #     rand!(ğ’Ÿ_ndas_batch[i], ğ’Ÿ_ndas[i])
        #     # rand!(ğ’Ÿ_ndas_Ï€_batch[i], ğ’Ÿ_ndas[i])
        #     # ğ’Ÿ_ndas_Ï€_batch[i].data[:a] = action(Ï€, ğ’Ÿ_ndas_Ï€_batch[i][:s])
        # end
        # 
        # ğ’Ÿ_full = hcat(ğ’Ÿ_demo_batch, ğ’Ÿ_batch, ğ’Ÿ_ndas_batch...)
        # 
        # for k in keys(ğ’Ÿ)
        #     ğ’Ÿ.data[k] = ğ’Ÿ_full.data[k]
        # end
        # ğ’Ÿ.elements = ğ’Ÿ_full.elements
        # ğ’Ÿ.next_ind = ğ’Ÿ_full.next_ind
        
        ## Compute the rewards
        D_out = Flux.softmax(value(D, ğ’Ÿ[:s], ğ’Ÿ[:a]))
        # we = max.(D_out[1, :], 1f0 .- D_out[1, :])
        # w_ndas = [-D_out[i, :] .* (1f0 .- we) ./ sum(D_out[3:N_datasets, :], dims=1)[:] for i=3:N_datasets]
        # w = cat(we, zeros(Float32, size(we)), w_ndas..., dims=2)'
        w = [1f0, 0f0, Î»_nda*ones(Float32, N_nda)...] |> dev
        
        ğ’Ÿ[:r] .= sum((Base.log.(D_out .+ 1f-5) .- Base.log.(1f0 .- D_out .+ 1f-5)) .* w, dims=1)
    end
    
    ğ’®.post_batch_callback = GAIL_callback
    ğ’®
end

