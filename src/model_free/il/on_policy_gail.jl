function GAIL_D_loss(gan_loss)
    (D, ğ’«, ğ’Ÿ_ex, ğ’Ÿ_Ï€; info = Dict()) ->begin
        Lá´°(gan_loss, D, ğ’Ÿ_ex[:a], ğ’Ÿ_Ï€[:a], yD = (ğ’Ÿ_ex[:s],), yG = (ğ’Ÿ_Ï€[:s],))
    end
end

function OnPolicyGAIL(;Ï€, 
                       S,
                       Î³,
                       Î»_gae::Float32 = 0.95f0,
                       ğ’Ÿ_demo, 
                       normalize_demo::Bool=true, 
                       D::ContinuousNetwork, 
                       solver=PPO, 
                       gan_loss::GANLoss=GAN_BCELoss(), 
                       d_opt::NamedTuple=(;), 
                       log::NamedTuple=(;), 
                       discriminator_transform=sigmoid,
                       kwargs...)
                       
    d_opt = TrainingParams(;loss = GAIL_D_loss(gan_loss), name="discriminator_", d_opt...)
    normalize_demo && (ğ’Ÿ_demo = normalize!(deepcopy(ğ’Ÿ_demo), S, action_space(Ï€)))
    ğ’Ÿ_demo = ğ’Ÿ_demo |> device(Ï€)
    
    function GAIL_callback(ğ’Ÿ; info=Dict(), ğ’®)
        batch_train!(D, d_opt, (;), ğ’Ÿ_demo, deepcopy(ğ’Ÿ), info=info)
        
        discriminator_signal = haskey(ğ’Ÿ, :advantage) ? :advantage : :return
        D_out = value(D, ğ’Ÿ[:a], ğ’Ÿ[:s]) # This is swapped because a->x and s->y and the convention for GANs is D(x,y)
        # r = Base.log.(discriminator_transform.(D_out) .+ 1f-5) .- Base.log.(1f0 .- discriminator_transform.(D_out) .+ 1f-5)
        r =  -Base.log.(1f0 .- discriminator_transform.(D_out) .+ 1f-5)
        ignore() do
            info["disc_reward"] = mean(r)
        end
        
        ğ’Ÿ[:r] .= r
        
        eps = episodes(ğ’Ÿ)
        for ep in eps
            eprange = ep[1]:ep[2]
            fill_gae!(ğ’Ÿ, eprange, ğ’®.agent.Ï€, Î»_gae, Î³)
            fill_returns!(ğ’Ÿ, eprange, Î³)
            ğ’Ÿ[:advantage] .= whiten(ğ’Ÿ[:advantage])
        end
        
    end
    solver(;Ï€=Ï€, S=S, post_batch_callback=GAIL_callback, log=(dir="log/onpolicygail", period=500, log...), Î»_gae=Î»_gae, kwargs...)
end

