function GAIL_D_loss(gan_loss)
    (D, ğ’«, ğ’Ÿ_ex, ğ’Ÿ_Ï€; info = Dict()) ->begin
        Lá´°(gan_loss, D, ğ’Ÿ_ex[:a], ğ’Ÿ_Ï€[:a], yD = (ğ’Ÿ_ex[:s],), yG = (ğ’Ÿ_Ï€[:s],))
    end
end

function OnPolicyGAIL(;Ï€, 
                       S, 
                       ğ’Ÿ_demo, 
                       normalize_demo::Bool=true, 
                       D::ContinuousNetwork, 
                       solver=PPO, 
                       gan_loss::GANLoss=GAN_BCELoss(), 
                       d_opt::NamedTuple=(;), 
                       log::NamedTuple=(;), 
                       discriminator_transform=sigmoid,
                       kwargs...)
                       
    d_opt = TrainingParams(;loss = GAIL_D_loss(gan_loss), name="discriminator_", d_opt...)
    normalize_demo && (ğ’Ÿ_demo = normalize!(deepcopy(ğ’Ÿ_demo), S, action_space(Ï€)))
    ğ’Ÿ_demo = ğ’Ÿ_demo |> device(Ï€)
    
    function GAIL_callback(ğ’Ÿ; info=Dict())
        batch_train!(D, d_opt, (;), ğ’Ÿ_demo, ğ’Ÿ, info=info)
        
        discriminator_signal = haskey(ğ’Ÿ, :advantage) ? :advantage : :return
        D_out = value(D, ğ’Ÿ[:a], ğ’Ÿ[:s]) # This is swapped because a->x and s->y and the convention for GANs is D(x,y)
        r = Base.log.(discriminator_transform.(D_out) .+ 1f-5) .- Base.log.(1f0 .- discriminator_transform.(D_out) .+ 1f-5)
        ignore() do
            info["disc_reward"] = mean(r)
        end
        ğ’Ÿ[discriminator_signal] .= r 
    end
    ğ’® = solver(;Ï€=Ï€, S=S, post_batch_callback=GAIL_callback, log=(dir="log/onpolicygail", period=500, log...), kwargs...)
    ğ’®.c_opt = nothing # disable the critic 
    ğ’®
end

