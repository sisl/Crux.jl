function GAIL_D_loss(gan_loss)
    (D, ğ’«, ğ’Ÿ_ex, ğ’Ÿ_Ï€; info = Dict()) ->begin
        Lá´°(gan_loss, D, ğ’Ÿ_ex[:a], ğ’Ÿ_Ï€[:a], yD = (ğ’Ÿ_ex[:s],), yG = (ğ’Ÿ_Ï€[:s],))
    end
end

function OnPolicyGAIL(;Ï€,
                       S,
                       Î³,
                       Î»_gae::Float32 = 0.95f0,
                       ğ’Ÿ_demo,
                       Î±r::Float32 = 0.5f0,
                       normalize_demo::Bool=true,
                       D::ContinuousNetwork,
                       solver=PPO,
                       gan_loss::GANLoss=GAN_BCELoss(),
                       d_opt::NamedTuple=(;),
                       log::NamedTuple=(;),
                       Rscale=1f0,
                       kwargs...)

    d_opt = TrainingParams(;loss = GAIL_D_loss(gan_loss), name="discriminator_", d_opt...)
    normalize_demo && (ğ’Ÿ_demo = normalize!(deepcopy(ğ’Ÿ_demo), S, action_space(Ï€)))
    ğ’Ÿ_demo = ğ’Ÿ_demo |> device(Ï€)

    function GAIL_callback(ğ’Ÿ; info=Dict(), ğ’®)
        batch_train!(D, d_opt, (;), ğ’Ÿ_demo, deepcopy(ğ’Ÿ), info=info)

        D_out = value(D, ğ’Ÿ[:a], ğ’Ÿ[:s]) # This is swapped because a->x and s->y and the convention for GANs is D(x,y)
        r = Î±r * logÏƒ.(D_out) .- (1f0 - Î±r) * logcompÏƒ.(D_out)
        ignore_derivatives() do
            info["disc_reward"] = mean(r)
        end

        ğ’Ÿ[:r] .= r.*Rscale

        eps = episodes(ğ’Ÿ)
        for ep in eps
            eprange = ep[1]:ep[2]
            fill_gae!(ğ’Ÿ, eprange, ğ’®.agent.Ï€, Î»_gae, Î³)
            fill_returns!(ğ’Ÿ, eprange, Î³)
        end
        ğ’Ÿ[:advantage] .= whiten(ğ’Ÿ[:advantage])

    end
    solver(;Ï€=Ï€, S=S, post_batch_callback=GAIL_callback, log=(dir="log/onpolicygail", period=500, log...), Î»_gae=Î»_gae, kwargs...)
end
