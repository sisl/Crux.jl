@with_kw mutable struct ValueDICESolver <: Solver
    Ï€ # Policy
    S::AbstractSpace # State space
    A::AbstractSpace = action_space(Ï€) # Action space
    N::Int = 1000 # Number of environment interactions
    Î”N::Int = 4 # Number of interactions between updates
    max_steps::Int = 100 # Maximum number of steps per episode
    log::Union{Nothing, LoggerParams} = LoggerParams(;dir = "log/valueDICE") # The logging parameters
    i::Int = 0 # The current number of environment interactions
    a_opt::TrainingParams # Training parameters for the actor
    c_opt::TrainingParams # Training parameters for the critic
    
    ğ’Ÿ_expert # expert buffer
    Î±::Float32 = 0.1 # mixing parameter
    Ï€_explore=Ï€
    buffer_size = 1000 # Size of the buffer
    buffer::ExperienceBuffer=ExperienceBuffer(S, A, buffer_size,[:t]) # The replay buffer
    buffer_init::Int=max(c_opt.batch_size, 200) # Number of observations to initialize the buffer with
end

ValueDICE(;Î”N=50, a_opt::NamedTuple=(;), c_opt::NamedTuple=(;), kwargs...) = ValueDICESolver(;Î”N=Î”N, a_opt=TrainingParams(;loss=valueDICE_loss, a_opt...), c_opt=TrainingParams(;loss=valueDICE_loss, epochs=Î”N, c_opt...), kwargs...)

# orthogonal initialization
# GP on critic
# orthogonal regularization on the policy

function valueDICE_loss(Ï€, ğ’Ÿ, ğ’Ÿ_exp, Î±, Î³; info=Dict())
    a0, _= exploration(Ï€.A, ğ’Ÿ_exp[:s]) #:s0
    a, _ = exploration(Ï€.A, ğ’Ÿ[:sp])
    ae, _  = exploration(Ï€.A, ğ’Ÿ_exp[:sp])
    
    Î”Î½E = value(Ï€, ğ’Ÿ_exp[:s], ğ’Ÿ_exp[:a]) - Î³*value(Ï€, ğ’Ÿ_exp[:sp], ae)
    Î”Î½ = value(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a]) - Î³*value(Ï€, ğ’Ÿ[:sp], a)
    
    
    Jlog = log(mean((1-Î±)*exp.(Î”Î½E) .+ Î±*exp.(Î”Î½)))
    Jlin = mean((1-Î±)*(1-Î³)*value(Ï€, ğ’Ÿ_exp[:s], a0) + Î±.*Î”Î½)
    
    Jlog - Jlin
end

function POMDPs.solve(ğ’®::ValueDICESolver, mdp, logmdp)
    # Construct the training buffer, constants, and sampler
    ğ’Ÿ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.c_opt.batch_size, [:t], device=device(ğ’®.Ï€))
    ğ’Ÿ_exp = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.c_opt.batch_size, [:t, :s0], device=device(ğ’®.Ï€))
    ğ’Ÿ_exp.data[:expert_val] = ones(Float32, 1, ğ’®.c_opt.batch_size)
    
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.S, ğ’®.A, max_steps=ğ’®.max_steps, Ï€_explore=ğ’®.Ï€_explore, required_columns=[:t])
    slog = Sampler(logmdp, ğ’®.Ï€, ğ’®.S, ğ’®.A, max_steps=ğ’®.max_steps, Ï€_explore=ğ’®.Ï€_explore, required_columns=[:t])

    # Log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, s=slog)

    # Fill the buffer with initial observations before training
    ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i=ğ’®.i, explore=true)
    
    # Loop over the desired number of environment interactions
    for ğ’®.i in range(ğ’®.i, stop=ğ’®.i + ğ’®.N - ğ’®.Î”N, step=ğ’®.Î”N)
        # Sample transitions into the replay buffer
        push!(ğ’®.buffer, steps!(s, Nsteps=ğ’®.Î”N, explore=true, i=ğ’®.i))

        infos = []
        # Loop over the desired number of training steps
        for epoch in 1:ğ’®.c_opt.epochs
            # geometric_sample!(ğ’Ÿ, ğ’®.buffer, Î³)
            # geometric_sample!(ğ’Ÿ_exp, ğ’®.ğ’Ÿ_expert, Î³)
            # 
            rand!(ğ’Ÿ, ğ’®.buffer)
            rand!(ğ’Ÿ_exp, ğ’®.ğ’Ÿ_expert)
            
            # Update the critic and actor
            info_c = train!(ğ’®.Ï€.C, (;kwargs...) -> ğ’®.c_opt.loss(ğ’®.Ï€, ğ’Ÿ, ğ’Ÿ_exp, ğ’®.Î±, Î³; kwargs...), ğ’®.c_opt)
            info_a = train!(ğ’®.Ï€.A, (;kwargs...) -> -ğ’®.a_opt.loss(ğ’®.Ï€, ğ’Ÿ, ğ’Ÿ_exp, ğ’®.Î±, Î³; kwargs...), ğ’®.a_opt)
            
            push!(infos, merge(info_c, info_a))            
        end
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, aggregate_info(infos), s=slog)
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.Ï€
end

