@with_kw mutable struct ValueDICESolver <: Solver
    Ï€ # Policy
    S::AbstractSpace # State space
    A::AbstractSpace = action_space(Ï€) # Action space
    N::Int = 1000 # Number of environment interactions
    Î”N::Int = 4 # Number of interactions between updates
    max_steps::Int = 100 # Maximum number of steps per episode
    log::Union{Nothing, LoggerParams} = LoggerParams(;dir = "log/valueDICE") # The logging parameters
    i::Int = 0 # The current number of environment interactions
    a_opt::TrainingParams # Training parameters for the actor
    c_opt::TrainingParams # Training parameters for the critic
    
    ğ’Ÿ_expert # expert buffer
    Î±::Float32 = 0.1 # mixing parameter
    Ï€_explore=Ï€
    buffer_size = 1000 # Size of the buffer
    buffer::ExperienceBuffer=ExperienceBuffer(S, A, buffer_size,[:t]) # The replay buffer
    buffer_init::Int=max(c_opt.batch_size, 200) # Number of observations to initialize the buffer with
end

ValueDICE(;Î”N=50, Î»_orth=1f-4, a_opt::NamedTuple=(;), c_opt::NamedTuple=(;), log::NamedTuple=(;), kwargs...) = 
    ValueDICESolver(;Î”N=Î”N,
                     log=LoggerParams(;dir="log/valueDICE", period=100, log...),
                     a_opt=TrainingParams(;name="actor_", loss=valueDICE_Ï€_loss, regularizer=OrthogonalRegularizer(Î»_orth), a_opt...), 
                     c_opt=TrainingParams(;name="critic_", loss=valueDICE_C_loss, epochs=Î”N, c_opt...), 
                     kwargs...)

function weighted_softmax(x, weights; dims=1)
    x = x .- maximum(x, dims=dims)
    weights .* exp.(x) ./ sum(weights .* exp.(x), dims=dims)
end

function valueDICE_loss(Ï€, ğ’Ÿ, ğ’Ÿ_exp, Î±, Î³; info=Dict())
    ae, _  = exploration(Ï€.A, ğ’Ÿ_exp[:sp]) # Policy next actions
    a, _ = exploration(Ï€.A, ğ’Ÿ[:sp]) # rb next actions
    a0, _= exploration(Ï€.A, ğ’Ÿ_exp[:s]) #:s0 # Policy initial actions
    
    Î½E_0 = value(Ï€, ğ’Ÿ_exp[:s], a0) # expert_nu_0
    Î½E = value(Ï€, ğ’Ÿ_exp[:s], ğ’Ÿ_exp[:a]) # expert_nu
    Î½E_next = value(Ï€, ğ’Ÿ_exp[:sp], ae) # expert_nu
    
    Î½RB = value(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a]) # rb_nu
    Î½RB_next = value(Ï€, ğ’Ÿ[:sp], a)
    
    Î”Î½E = Î½E - Î³*Î½E_next
    Î”Î½RB = Î½RB - Î³*Î½RB_next
    
    Jlin_E = mean(Î½E_0*(1f0-Î³))
    Jlin_RB = mean(Î”Î½RB)
    Jlin = Jlin_E*(1f0-Î±) + Jlin_RB*Î±
    
    RB_E_diff = vcat(Î”Î½E, Î”Î½RB)
    RB_E_weights = [1-Î±, Î±]
    Jlog = sum(Zygote.dropgrad(weighted_softmax(RB_E_diff, RB_E_weights, dims=1)).*RB_E_diff)
    
    Jlog - Jlin, ae, a
end

valueDICE_Ï€_loss(Ï€, ğ’Ÿ, ğ’Ÿ_exp, Î±, Î³; info=Dict()) = -valueDICE_loss(Ï€, ğ’Ÿ, ğ’Ÿ_exp, Î±, Î³, info=info)[1]

function valueDICE_C_loss(Ï€, ğ’Ÿ, ğ’Ÿ_exp, Î±, Î³; info=Dict())
    l, ae, a = valueDICE_loss(Ï€, ğ’Ÿ, ğ’Ÿ_exp, Î±, Î³, info=info)
    real = hcat(vcat(ğ’Ÿ_exp[:s], ğ’Ÿ_exp[:a]), vcat(ğ’Ÿ_exp[:sp], ae))
    fake = hcat(vcat(ğ’Ÿ[:s], ğ’Ÿ[:a]), vcat(ğ’Ÿ[:sp], a))
    
     l + 10f0*gradient_penalty(Ï€.C, real, fake)
end

function POMDPs.solve(ğ’®::ValueDICESolver, mdp)
    # Construct the training buffer, constants, and sampler
    ğ’Ÿ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.c_opt.batch_size, [:t], device=device(ğ’®.Ï€))
    ğ’Ÿ_exp = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.c_opt.batch_size, [:t], device=device(ğ’®.Ï€))
    ğ’Ÿ_exp.data[:expert_val] = ones(Float32, 1, ğ’®.c_opt.batch_size)
    
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€, max_steps=ğ’®.max_steps, Ï€_explore=ğ’®.Ï€_explore, required_columns=[:t])

    # Log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, s=s)

    # Fill the buffer with initial observations before training
    ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i=ğ’®.i, explore=true)
    
    # Loop over the desired number of environment interactions
    for ğ’®.i in range(ğ’®.i, stop=ğ’®.i + ğ’®.N - ğ’®.Î”N, step=ğ’®.Î”N)
        # Sample transitions into the replay buffer
        push!(ğ’®.buffer, steps!(s, Nsteps=ğ’®.Î”N, explore=true, i=ğ’®.i))

        infos = []
        # Loop over the desired number of training steps
        for epoch in 1:ğ’®.c_opt.epochs
            # geometric_sample!(ğ’Ÿ, ğ’®.buffer, Î³)
            # geometric_sample!(ğ’Ÿ_exp, ğ’®.ğ’Ÿ_expert, Î³)
            # 
            rand!(ğ’Ÿ, ğ’®.buffer)
            rand!(ğ’Ÿ_exp, ğ’®.ğ’Ÿ_expert)
            
            # Update the critic and actor
            info_c = train!(ğ’®.Ï€.C, (;kwargs...) -> ğ’®.c_opt.loss(ğ’®.Ï€, ğ’Ÿ, ğ’Ÿ_exp, ğ’®.Î±, Î³; kwargs...), ğ’®.c_opt)
            info_a = train!(ğ’®.Ï€.A, (;kwargs...) -> ğ’®.a_opt.loss(ğ’®.Ï€, ğ’Ÿ, ğ’Ÿ_exp, ğ’®.Î±, Î³; kwargs...), ğ’®.a_opt)
            
            push!(infos, merge(info_c, info_a))            
        end
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, aggregate_info(infos), s=s)
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.Ï€
end

