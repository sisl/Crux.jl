"""
Off policy solver type.

Fields
======

- `agent::PolicyParams` # Policy parameters ([`PolicyParams`](@ref))
- `S::AbstractSpace` # State space
- `N::Int = 1000` # Number of environment interactions
- `Î”N::Int = 4` # Number of interactions between updates
- `max_steps::Int = 100` # Maximum number of steps per episode
- `log::Union{Nothing, LoggerParams} = nothing` # The logging parameters
- `i::Int = 0` # The current number of environment interactions
- `param_optimizers::Dict{Any, TrainingParams} = Dict()` # Training parameters for the parameters
- `a_opt::Union{Nothing, TrainingParams} = nothing` # Training parameters for the actor
- `c_opt::TrainingParams` # Training parameters for the critic
- `ğ’«::NamedTuple = (;)` # Parameters of the algorithm
- `interaction_storage = nothing` # If this is initialized to an array then it will store all interactions
- `post_sample_callback = (ğ’Ÿ; kwargs...) -> nothing` # Callback that that happens after sampling experience


Off-policy-specific parameters
======

- `post_batch_callback = (ğ’Ÿ; kwargs...) -> nothing` Callback that that happens after sampling a batch
- `pre_train_callback = (ğ’®; kwargs...) -> nothing` callback that gets called once prior to training
- `target_update = (Ï€â», Ï€; kwargs...) -> polyak_average!(Ï€â», Ï€, 0.005f0)` Function for updating the target network
- `target_fn` Target for critic regression with input signature `(Ï€â», ğ’Ÿ, Î³; i)`
- `buffer_size = 1000` Size of the buffer
- `required_columns = Symbol[]`
- `buffer = ExperienceBuffer(S, agent.space, buffer_size, required_columns)` The replay buffer
- `priority_fn = td_error` function for prioritized replay
- `buffer_init::Int = max(c_opt.batch_size, 200)` Number of observations to initialize the buffer with
- `extra_buffers = []` extra buffers (i.e. for experience replay in continual learning)
- `buffer_fractions = [1.0]` Fraction of the minibatch devoted to each buffer
"""
@with_kw mutable struct OffPolicySolver <: Solver
    agent::PolicyParams # Policy
    S::AbstractSpace # State space
    N::Int = 1000 # Number of environment interactions
    Î”N::Int = 4 # Number of interactions between updates
    max_steps::Int = 100 # Maximum number of steps per episode
    log::Union{Nothing, LoggerParams} = nothing # The logging parameters
    i::Int = 0 # The current number of environment interactions
    param_optimizers::Dict{Any, TrainingParams} = Dict() # Training parameters for the parameters
    a_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the actor
    c_opt::TrainingParams # Training parameters for the critic
    ğ’«::NamedTuple = (;) # Parameters of the algorithm
	interaction_storage = nothing # If this is initialized to an array then it will store all interactions
	post_sample_callback = (ğ’Ÿ; kwargs...) -> nothing # Callback that that happens after sampling experience
    
    # Off-policy-specific parameters
	post_batch_callback = (ğ’Ÿ; kwargs...) -> nothing # Callback that that happens after sampling a batch
	pre_train_callback = (ğ’®; kwargs...) -> nothing # callback that gets called once prior to training
    target_update = (Ï€â», Ï€; kwargs...) -> polyak_average!(Ï€â», Ï€, 0.005f0) # Function for updating the target network
    target_fn # Target for critic regression with input signature (Ï€â», ğ’Ÿ, Î³; i)
    buffer_size = 1000 # Size of the buffer
    required_columns = Symbol[]
    buffer = ExperienceBuffer(S, agent.space, buffer_size, required_columns) # The replay buffer
	priority_fn = td_error  # function for prioritized replay
    buffer_init::Int = max(c_opt.batch_size, 200) # Number of observations to initialize the buffer with
    extra_buffers = [] # extra buffers (i.e. for experience replay in continual learning)
    buffer_fractions = [1.0] # Fraction of the minibatch devoted to each buffer
end

function value_training(ğ’®::OffPolicySolver, ğ’Ÿ, Î³)
    infos = []
    # Loop over the desired number of training steps
    for epoch in 1:ğ’®.c_opt.epochs
        # Sample a random minibatch of ğ‘ transitions (sáµ¢, aáµ¢, ráµ¢, sáµ¢â‚Šâ‚) from ğ’Ÿ
        rand!(ğ’Ÿ, ğ’®.buffer, ğ’®.extra_buffers..., fracs=ğ’®.buffer_fractions, i=ğ’®.i)
        
        # Dictionary to store info from the various optimization processes
        info = Dict()
        
        # Callack for potentially updating the buffer
        ğ’®.post_batch_callback(ğ’Ÿ, ğ’®=ğ’®, info=info)
        
        # Compute target
        y = ğ’®.target_fn(ğ’®.agent.Ï€â», ğ’®.ğ’«, ğ’Ÿ, Î³, i=ğ’®.i)
        
        # Update priorities (for prioritized replay)
        isprioritized(ğ’®.buffer) && update_priorities!(ğ’®.buffer, ğ’Ÿ.indices, cpu(ğ’®.priority_fn(ğ’®.agent.Ï€, ğ’®.ğ’«, ğ’Ÿ, y)))
        
        # Train parameters
        for (Î¸s, p_opt) in ğ’®.param_optimizers
            train!(Î¸s, (;kwargs...) -> p_opt.loss(ğ’®.agent.Ï€, ğ’®.ğ’«, ğ’Ÿ; kwargs...), p_opt, info=info)
        end
        
        # Train the critic
        if ((epoch-1) % ğ’®.c_opt.update_every) == 0
            train!(critic(ğ’®.agent.Ï€), (;kwargs...) -> ğ’®.c_opt.loss(ğ’®.agent.Ï€, ğ’®.ğ’«, ğ’Ÿ, y; kwargs...), ğ’®.c_opt, info=info)
        end
        
        # Train the actor 
        if !isnothing(ğ’®.a_opt) && ((epoch-1) % ğ’®.a_opt.update_every) == 0
            train!(actor(ğ’®.agent.Ï€), (;kwargs...) -> ğ’®.a_opt.loss(ğ’®.agent.Ï€, ğ’®.ğ’«, ğ’Ÿ; kwargs...), ğ’®.a_opt, info=info)
        
            # Update the target network
            ğ’®.target_update(ğ’®.agent.Ï€â», ğ’®.agent.Ï€)
        end
        
        # Store the training information
        push!(infos, info)
        
    end
    # If not using a separate actor, update target networks after critic training
    isnothing(ğ’®.a_opt) && ğ’®.target_update(ğ’®.agent.Ï€â», ğ’®.agent.Ï€, i=ğ’®.i + 1:ğ’®.i + ğ’®.Î”N)
    
    aggregate_info(infos)
end

function POMDPs.solve(ğ’®::OffPolicySolver, mdp)
    # Construct the training buffer, constants, and sampler
    ğ’Ÿ = buffer_like(ğ’®.buffer, capacity=ğ’®.c_opt.batch_size, device=device(ğ’®.agent.Ï€))
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.agent, S=ğ’®.S, max_steps=ğ’®.max_steps, required_columns=extra_columns(ğ’®.buffer))
    isnothing(ğ’®.log.sampler) && (ğ’®.log.sampler = s)

    # Fill the buffer with initial observations before training
	info = Dict()
	Nfill = max(0, ğ’®.buffer_init - length(ğ’®.buffer))
	istart = ğ’®.i
	if Nfill > 0
		ğ’®.i += Nfill
		steps!(s, ğ’®.buffer, Nsteps=Nfill, explore=true, i=ğ’®.i, store=ğ’®.interaction_storage, cb=(D)->ğ’®.post_sample_callback(D, ğ’®=ğ’®, info=info))
	end 
	
	# Log the pre-train performance
	log(ğ’®.log, ğ’®.i, info, ğ’®=ğ’®)
    
    # Loop over the desired number of environment interactions
    for ğ’®.i in range(ğ’®.i, stop=istart + ğ’®.N - ğ’®.Î”N, step=ğ’®.Î”N)
		# Store info here
		info = Dict()
        
        # Sample transitions into the replay buffer
		steps!(s, ğ’®.buffer, Nsteps=ğ’®.Î”N, explore=true, i=ğ’®.i, store=ğ’®.interaction_storage, cb=(D)->ğ’®.post_sample_callback(D, ğ’®=ğ’®, info=info))
		
		ğ’®.pre_train_callback(ğ’®, info=info)
        
        # Train the networks
        training_info = value_training(ğ’®, ğ’Ÿ, Î³)
        
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, training_info, info, ğ’®=ğ’®)
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.agent.Ï€
end

