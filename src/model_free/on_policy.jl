@with_kw mutable struct OnPolicySolver <: Solver
    agent::PolicyParams # Policy
    S::AbstractSpace # State space
    N::Int = 1000 # Number of environment interactions
    N::Int = 200 # Number of interactions between updates
    max_steps::Int = 100 # Maximum number of steps per episode
    log::Union{Nothing, LoggerParams} = nothing # The logging parameters
    i::Int = 0 # The current number of environment interactions
    param_optimizers::Dict{Any, TrainingParams} = Dict() # Training parameters for the parameters
    a_opt::TrainingParams # Training parameters for the actor
    c_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the critic
    ::NamedTuple = (;) # Parameters of the algorithm
    interaction_storage = nothing # If this is initialized to an array then it will store all interactions
    post_sample_callback = (; kwargs...) -> nothing # Callback that that happens after sampling experience
    post_batch_callback = (; kwargs...) -> nothing # Callback that that happens after sampling a batch
    
    # On-policy-specific parameters
    位_gae::Float32 = 0.95 # Generalized advantage estimation parameter
    required_columns = Symbol[]# Extra data columns to store
    
    # Parameters specific to cost constraints (a separete value network)
    Vc::Union{ContinuousNetwork, Nothing} = nothing # Cost value approximator
    cost_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the cost value
end

function POMDPs.solve(::OnPolicySolver, mdp)
    # Construct the training buffer, constants, and sampler
     = ExperienceBuffer(.S, .agent.space, .N, .required_columns, device=device(.agent.))
    纬, 位 = Float32(discount(mdp)), .位_gae
    s = Sampler(mdp, .agent, S=.S, required_columns=.required_columns, 位=位, max_steps=.max_steps, Vc=.Vc)
    isnothing(.log.sampler) && (.log.sampler = s)

    # Log the pre-train performance
    log(.log, .i, =)

    # Loop over the desired number of environment interactions
    for .i = range(.i, stop=.i + .N - .N, step=.N)
        # Info to collect during training
        info = Dict()
        
        # Sample transitions into the batch buffer
        steps!(s, , Nsteps=.N, explore=true, i=.i, store=.interaction_storage, cb=(D) -> .post_sample_callback(D, info=info, =))
        
        # Post-batch callback, often used for additional training
        .post_batch_callback(, info=info, =)
        
        # Train parameters
        for (胃s, p_opt) in .param_optimizers
            batch_train!(胃s, p_opt, ., , info=info, _loss=.agent.)
        end
        
        # Train the actor
        batch_train!(actor(.agent.), .a_opt, ., , info=info)
        
        # Train the critic (if applicable)
        if !isnothing(.c_opt)
            batch_train!(critic(.agent.), .c_opt, ., , info=info)
        end
        
        if !isnothing(.cost_opt)
            batch_train!(.Vc, .cost_opt, ., , info=info)
        end
        
        # Log the results
        log(.log, .i + 1:.i + .N, info, =)
    end
    .i += .N
    .agent.
end

