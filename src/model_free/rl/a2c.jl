function a2c_loss(, , ; info = Dict())
    new_probs = logpdf(, [:s], [:a])
    p_loss = -mean(new_probs .* [:advantage])
    e_loss = -mean(entropy(, [:s]))
    
    # Log useful information
    ignore() do
        info[:entropy] = -e_loss
        info[:kl] = mean([:logprob] .- new_probs)
    end 
    
    [:位p]*p_loss + [:位e]*e_loss
end

function A2C(;::ActorCritic, 
              a_opt::NamedTuple=(;), 
              c_opt::NamedTuple=(;), 
              log::NamedTuple=(;), 
              位p::Float32=1f0, 
              位e::Float32=0.1f0, kwargs...)
              
    OnPolicySolver(;agent=PolicyParams(),
                    =(位p=位p, 位e=位e),
                    log=LoggerParams(;dir = "log/a2c", log...),
                    a_opt=TrainingParams(;loss=a2c_loss, early_stopping = (infos) -> (infos[end][:kl] > 0.015), name = "actor_", a_opt...),
                    c_opt=TrainingParams(;loss=(, , D; kwargs...) -> Flux.mse(value(, D[:s]), D[:return]), name = "critic_", c_opt...),
                    post_batch_callback=(; kwargs...) -> ([:advantage] .= whiten([:advantage])),
                    kwargs...)
end
        
    



