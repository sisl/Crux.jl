# Set yáµ¢ = ráµ¢ + Î³Qâ€²(sáµ¢â‚Šâ‚, Î¼â€²(sáµ¢â‚Šâ‚ | Î¸áµ˜â€²) | Î¸á¶œâ€²)
DDPG_target(Ï€, ğ’Ÿ, Î³::Float32; kwargs...) = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* value(Ï€, ğ’Ÿ[:sp], action(Ï€, ğ’Ÿ[:sp]))

# âˆ‡_Î¸áµ˜ ğ½ â‰ˆ 1/ğ‘ Î£áµ¢ âˆ‡â‚Q(s, a | Î¸á¶œ)|â‚›â‚Œâ‚›áµ¢, â‚â‚Œáµ¤â‚â‚›áµ¢â‚ âˆ‡_Î¸áµ˜ Î¼(s | Î¸áµ˜)|â‚›áµ¢
DDPG_actor_loss(Ï€, ğ’Ÿ; info=Dict()) = -mean(value(Ï€, ğ’Ÿ[:s], action(Ï€, ğ’Ÿ[:s])))

# T. P. Lillicrap, et al., "Continuous control with deep reinforcement learning", ICLR 2016.
DDPG(;Ï€::ActorCritic{ContinuousNetwork, ContinuousNetwork}, Î”N=50, Ï€_explore=GaussianNoiseExplorationPolicy(0.1f0),  a_opt::NamedTuple=(;), c_opt::NamedTuple=(;), log::NamedTuple=(;), kwargs...) = 
    OffPolicySolver(;
        Ï€=Ï€, 
        Î”N=Î”N,
        log=LoggerParams(;dir = "log/ddpg", log...),
        a_opt=TrainingParams(;loss=DDPG_actor_loss, name="actor_", a_opt...),
        c_opt=TrainingParams(;loss=td_loss, name="critic_", epochs=Î”N, c_opt...),
        Ï€_explore=Ï€_explore,
        target_fn=DDPG_target,
        kwargs...)

