# Set yáµ¢ = ráµ¢ + Î³Qâ€²(sáµ¢â‚Šâ‚, Î¼â€²(sáµ¢â‚Šâ‚ | Î¸áµ˜â€²) | Î¸á¶œâ€²)
function DDPG_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
    ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* value(Ï€, ğ’Ÿ[:sp], action(Ï€, ğ’Ÿ[:sp]))
end

function smoothed_DDPG_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; i)
    ap, _ = exploration(ğ’«[:Ï€_smooth], ğ’Ÿ[:sp], Ï€_on=Ï€, i=i)
    y = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* value(Ï€, ğ’Ÿ[:sp], ap)
end

# âˆ‡_Î¸áµ˜ ğ½ â‰ˆ 1/ğ‘ Î£áµ¢ âˆ‡â‚Q(s, a | Î¸á¶œ)|â‚›â‚Œâ‚›áµ¢, â‚â‚Œáµ¤â‚â‚›áµ¢â‚ âˆ‡_Î¸áµ˜ Î¼(s | Î¸áµ˜)|â‚›áµ¢
DDPG_actor_loss(Ï€, ğ’«, ğ’Ÿ; info=Dict()) = -mean(value(Ï€, ğ’Ÿ[:s], action(Ï€, ğ’Ÿ[:s])))

# T. P. Lillicrap, et al., "Continuous control with deep reinforcement learning", ICLR 2016.
function DDPG(;Ï€::ActorCritic, 
               Î”N=50, 
               Ï€_explore=GaussianNoiseExplorationPolicy(0.1f0),  
               a_opt::NamedTuple=(;), 
               c_opt::NamedTuple=(;),
               a_loss=DDPG_actor_loss,
               c_loss=td_loss(),
               target_fn=DDPG_target,
               prefix="",
               log::NamedTuple=(;), 
               Ï€_smooth=GaussianNoiseExplorationPolicy(0.1f0, Ïµ_min=-0.5f0, Ïµ_max=0.5f0), kwargs...)
               
    OffPolicySolver(;agent=PolicyParams(Ï€=Ï€, Ï€_explore=Ï€_explore, Ï€â»=deepcopy(Ï€)), 
                     Î”N=Î”N,
                     ğ’«=(Ï€_smooth=Ï€_smooth,),
                     log=LoggerParams(;dir = "log/ddpg", log...),
                     a_opt=TrainingParams(;loss=a_loss, name=string(prefix, "actor_"), a_opt...),
                     c_opt=TrainingParams(;loss=c_loss, name=string(prefix, "critic_"), epochs=Î”N, c_opt...),
                     target_fn=target_fn,
                     kwargs...)
end 
        

