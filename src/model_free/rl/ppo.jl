# PPO loss
function ppo_loss(, , ; info = Dict())
    new_probs = logpdf(, [:s], [:a]) 
    r = exp.(new_probs .- [:logprob])
    
    A = [:advantage]
    p_loss = -mean(min.(r .* A, clamp.(r, (1f0 - [:系]), (1f0 + [:系])) .* A))
    e_loss = -mean(entropy(, [:s]))
    
    # Log useful information
    ignore() do
        info[:entropy] = -e_loss
        info[:kl] = mean([:logprob] .- new_probs)
        info[:clip_fraction] = sum((r .> 1 + [:系]) .| (r .< 1 - [:系])) / length(r)
    end 
    [:位p]*p_loss + [:位e]*e_loss
end

function PPO(;::ActorCritic, 
     系::Float32 = 0.2f0, 
     位p::Float32 = 1f0, 
     位e::Float32 = 0.1f0, 
     target_kl = 0.012f0,
     a_opt::NamedTuple=(;), 
     c_opt::NamedTuple=(;), 
     log::NamedTuple=(;), 
     kwargs...)
     
     OnPolicySolver(;agent=PolicyParams(),
                    =(系=系, 位p=位p, 位e=位e),
                    log = LoggerParams(;dir = "log/ppo", log...),
                    a_opt = TrainingParams(;loss = ppo_loss, early_stopping = (infos) -> (infos[end][:kl] > target_kl), name = "actor_", a_opt...),
                    c_opt = TrainingParams(;loss = (, , D; kwargs...) -> Flux.mse(value(, D[:s]), D[:return]), name = "critic_", c_opt...),
                    post_batch_callback = (; kwargs...) -> ([:advantage] .= whiten([:advantage])),
                    kwargs...)
end

# PPO loss with a penalty
function lagrange_ppo_loss(, , ; info = Dict())
    new_probs = logpdf(, [:s], [:a]) 
    r = exp.(new_probs .- [:logprob])
    
    A = [:advantage]
    p_loss = -mean(min.(r .* A, clamp.(r, (1f0 - [:系]), (1f0 + [:系])) .* A))
    e_loss = -mean(entropy(, [:s]))
    
    penalty = ignore(() -> Flux.softplus([:penalty_param][1])) 
    cost_loss = (penalty /  (1+penalty)) * mean(r .* [:cost_advantage])
    
    # Log useful information
    ignore() do
        info[:entropy] = -e_loss
        info[:kl] = mean([:logprob] .- new_probs)
        info[:clip_fraction] = sum((r .> 1 + [:系]) .| (r .< 1 - [:系])) / length(r)
    end 
    [:位p]*p_loss + [:位e]*e_loss + 0.5f0*cost_loss
end

function lagrange_ppo_penalty_loss(, , ; info = Dict())
    penalty = Flux.softplus([:penalty_param][1])
    cur_cost = mean([:cost])
    
    ignore() do
        info["penalty"] = penalty
        info["cur_cost"] = cur_cost
    end
    
    -penalty * (cur_cost - [:target_cost])
end

function LagrangePPO(;::ActorCritic,
     Vc::ContinuousNetwork, # value network for estimating cost
     系::Float32 = 0.2f0, 
     位p::Float32 = 1f0, 
     位e::Float32 = 0.1f0,
     位_gae = 0.95f0,
     target_kl = 0.012f0,
     penalty_init = 1f0,
     target_cost = 0.025f0,
     a_opt::NamedTuple=(;), 
     c_opt::NamedTuple=(;), 
     penalty_opt::NamedTuple=(;),
     cost_opt::NamedTuple=(;),
     log::NamedTuple=(;), 
     kwargs...)
     
     =(系=系, 位p=位p, 位e=位e, penalty_param=Float32[Base.log(exp(penalty_init)-1)], target_cost=target_cost)
     
     OnPolicySolver(;agent=PolicyParams(),
                    =,
                    Vc=Vc,
                    log = LoggerParams(;dir = "log/lagrange_ppo", log...),
                    param_optimizers = Dict(Flux.params([:penalty_param]) => TrainingParams(;loss=lagrange_ppo_penalty_loss, name="penalty_", penalty_opt...)),
                    a_opt = TrainingParams(;loss = lagrange_ppo_loss, early_stopping = (infos) -> (infos[end][:kl] > target_kl), name = "actor_", a_opt...),
                    c_opt = TrainingParams(;loss = (, , D; kwargs...) -> Flux.mse(value(, D[:s]), D[:return]), name = "critic_", c_opt...),
                    cost_opt = TrainingParams(;loss = (, , D; kwargs...) -> Flux.mse(value(, D[:s]), D[:cost_return]), name = "cost_critic_", cost_opt...),
                    required_columns = [:return, :advantage, :logprob, :cost_advantage, :cost, :cost_return],
                    post_batch_callback = (; kwargs...) -> ([:advantage] .= whiten([:advantage])),
                    kwargs...)
end




        
    



