# PPO loss
function ppo_loss(Ï€, ğ’«, ğ’Ÿ; info = Dict())
    new_probs = logpdf(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a]) 
    r = exp.(new_probs .- ğ’Ÿ[:logprob])
    
    A = ğ’Ÿ[:advantage]
    p_loss = -mean(min.(r .* A, clamp.(r, (1f0 - ğ’«[:Ïµ]), (1f0 + ğ’«[:Ïµ])) .* A))
    e_loss = -mean(entropy(Ï€, ğ’Ÿ[:s]))
    
    # Log useful information
    ignore() do
        info[:entropy] = -e_loss
        info[:kl] = mean(ğ’Ÿ[:logprob] .- new_probs)
        info[:clip_fraction] = sum((r .> 1 + ğ’«[:Ïµ]) .| (r .< 1 - ğ’«[:Ïµ])) / length(r)
    end 
    ğ’«[:Î»p]*p_loss + ğ’«[:Î»e]*e_loss
end

function PPO(;Ï€::ActorCritic, 
     Ïµ::Float32 = 0.2f0, 
     Î»p::Float32 = 1f0, 
     Î»e::Float32 = 0.1f0, 
     target_kl = 0.012f0,
     a_opt::NamedTuple=(;), 
     c_opt::NamedTuple=(;), 
     log::NamedTuple=(;), 
     kwargs...)
     
     OnPolicySolver(;agent=PolicyParams(Ï€),
                    ğ’«=(Ïµ=Ïµ, Î»p=Î»p, Î»e=Î»e),
                    log = LoggerParams(;dir = "log/ppo", log...),
                    a_opt = TrainingParams(;loss = ppo_loss, early_stopping = (infos) -> (infos[end][:kl] > target_kl), name = "actor_", a_opt...),
                    c_opt = TrainingParams(;loss = (Ï€, ğ’«, D; kwargs...) -> Flux.mse(value(Ï€, D[:s]), D[:return]), name = "critic_", c_opt...),
                    post_batch_callback = (ğ’Ÿ; kwargs...) -> (ğ’Ÿ[:advantage] .= whiten(ğ’Ÿ[:advantage])),
                    kwargs...)
end

# PPO loss with a penalty
function lagrange_ppo_loss(Ï€, ğ’«, ğ’Ÿ; info = Dict())
    new_probs = logpdf(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a]) 
    r = exp.(new_probs .- ğ’Ÿ[:logprob])
    
    A = ğ’Ÿ[:advantage]
    p_loss = -mean(min.(r .* A, clamp.(r, (1f0 - ğ’«[:Ïµ]), (1f0 + ğ’«[:Ïµ])) .* A))
    e_loss = -mean(entropy(Ï€, ğ’Ÿ[:s]))
    
    #update the cost penalty
    penalty = ignore() do
        # ğ’«[:penalty_param][1] = clamp(ğ’«[:penalty_param][1], -7, 10)
        # Flux.softplus(ğ’«[:penalty_param][1])
        
        # Average cost
        Jc = sum(ğ’Ÿ[:cost]) / sum(ğ’Ÿ[:episode_end])
        # Jc = maximum(ğ’Ÿ[:cost])
        
        
        # Compute the error
        Î” = Jc - ğ’«[:target_cost]
        
        # Update integral term
        ğ’«[:I][1] = max(0, ğ’«[:I][1] + ğ’«[:Ki]*Î”)
        
        # Smooth out the values
        Î± = ğ’«[:ema_Î±]
        ğ’«[:smooth_Î”][1] = Î± * ğ’«[:smooth_Î”][1] + (1 - Î±)*Î”
        ğ’«[:smooth_Jc][1] = Î± * ğ’«[:smooth_Jc][1] + (1 - Î±)*Jc
        
        # Compute the derivative term
        âˆ‚ = max(0, ğ’«[:smooth_Jc][1] - ğ’«[:Jc_prev][1])
        
        # Update the previous cost
        ğ’«[:Jc_prev][1] = ğ’«[:smooth_Jc][1]
        
        # PID update
        penalty = max(0, ğ’«[:Kp] * ğ’«[:smooth_Î”][1] + ğ’«[:I][1] + ğ’«[:Kd]*âˆ‚)
        
        info["penalty"] = penalty
        info["cur_cost"] = Jc
        info["smooth_delta"] = ğ’«[:smooth_Î”][1]
        info["deriv_term"] = âˆ‚
        info["Kd"] = ğ’«[:Kd]
        info["Kp"] = ğ’«[:Kp]
        info["integral term"] = ğ’«[:I][1]
        
        
        penalty
    end

    # cost_loss = ğ’«[:penalty_scale] * penalty * mean(r .* ğ’Ÿ[:cost_advantage])
    cost_loss = penalty * mean(max.(r .* ğ’Ÿ[:cost_advantage], clamp.(r, (1f0 - ğ’«[:Ïµ]), (1f0 + ğ’«[:Ïµ])) .* ğ’Ÿ[:cost_advantage]))
    
    # Log useful information
    ignore() do
        info[:entropy] = -e_loss
        info[:kl] = mean(ğ’Ÿ[:logprob] .- new_probs)
        info[:clip_fraction] = sum((r .> 1 + ğ’«[:Ïµ]) .| (r .< 1 - ğ’«[:Ïµ])) / length(r)
    end 
    (ğ’«[:Î»p]*p_loss + ğ’«[:Î»e]*e_loss + cost_loss) / (1 + penalty)
end

function lagrange_ppo_penalty_loss(Ï€, ğ’«, ğ’Ÿ; info = Dict())
    penalty = Flux.softplus(ğ’«[:penalty_param][1])
    cur_cost = mean(ğ’Ÿ[:cost])
    
    ignore() do
        info["penalty"] = penalty
        info["cur_cost"] = cur_cost
    end
    
    -penalty * ğ’«[:penalty_scale] * (cur_cost - ğ’«[:target_cost])
end

function LagrangePPO(;Ï€::ActorCritic,
     Vc::ContinuousNetwork, # value network for estimating cost
     Ïµ::Float32 = 0.2f0, 
     Î»p::Float32 = 1f0, 
     Î»e::Float32 = 0f0,
     Î»_gae = 0.95f0,
     target_kl = 0.012f0,
     penalty_init = 1f0,
     target_cost = 0.025f0,
     penalty_scale = 1f0,
     Ki = 1f-3,
     Kp = 1,
     Kd = 0, 
     ema_Î± = 0.95,    
     a_opt::NamedTuple=(;), 
     c_opt::NamedTuple=(;), 
     penalty_opt::NamedTuple=(;),
     cost_opt::NamedTuple=(;),
     log::NamedTuple=(;), 
     kwargs...)
     
     ğ’«=(Ïµ=Ïµ, Î»p=Î»p, Î»e=Î»e, 
        penalty_param=Float32[Base.log(exp(penalty_init)-1)], 
        target_cost=target_cost, 
        penalty_scale=penalty_scale,
        I = [0f0],
        Jc_prev = [0f0],
        Ki=Ki,
        Kp=Kp,
        Kd=Kd,
        ema_Î±=ema_Î±,
        smooth_Î” = [0f0],
        smooth_Jc = [0f0]
        )
     
     OnPolicySolver(;agent=PolicyParams(Ï€),
                    ğ’«=ğ’«,
                    Vc=Vc,
                    log = LoggerParams(;dir = "log/lagrange_ppo", log...),
                    # param_optimizers = Dict(Flux.params(ğ’«[:penalty_param]) => TrainingParams(;loss=lagrange_ppo_penalty_loss, name="penalty_", penalty_opt...)),
                    a_opt = TrainingParams(;loss = lagrange_ppo_loss, early_stopping = (infos) -> (infos[end][:kl] > target_kl), name = "actor_", a_opt...),
                    c_opt = TrainingParams(;loss = (Ï€, ğ’«, D; kwargs...) -> Flux.mse(value(Ï€, D[:s]), D[:return]), name = "critic_", c_opt...),
                    cost_opt = TrainingParams(;loss = (Ï€, ğ’«, D; kwargs...) -> Flux.mse(value(Ï€, D[:s]), D[:cost_return]), name = "cost_critic_", cost_opt...),
                    required_columns = [:return, :advantage, :logprob, :cost_advantage, :cost, :cost_return],
                    post_batch_callback = (ğ’Ÿ; kwargs...) -> (ğ’Ÿ[:advantage] .= whiten(ğ’Ÿ[:advantage])),
                    kwargs...)
end




        
    



