# PPO loss
ppo_loss(;Ïµ::Float32 = 0.2f0, Î»â‚š::Float32 = 1f0, Î»â‚‘::Float32 = 0.1f0) = (Ï€, ğ’Ÿ; info = Dict()) -> ppo_loss(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a], ğ’Ÿ[:advantage], ğ’Ÿ[:logprob], Ïµ, Î»â‚š, Î»â‚‘, info)

function ppo_loss(Ï€, s, a, A, old_probs, Ïµ, Î»â‚š, Î»â‚‘, info = Dict())
    new_probs = logpdf(Ï€, s, a) 
    r = exp.(new_probs .- old_probs)

    p_loss = -mean(min.(r .* A, clamp.(r, (1f0 - Ïµ), (1f0 + Ïµ)) .* A))
    e_loss = -mean(entropy(Ï€, s))
    
    # Log useful information
    ignore() do
        info[:entropy] = -e_loss
        info[:kl] = mean(old_probs .- new_probs)
        info[:clip_fraction] = sum((r .> 1 + Ïµ) .| (r .< 1 - Ïµ)) / length(r)
    end 
    Î»â‚š*p_loss + Î»â‚‘*e_loss
end
# Build an A2C solver
PPO(;Ï€::ActorCritic, Ïµ::Float32 = 0.2f0, Î»â‚š::Float32 = 1f0, Î»â‚‘::Float32 = 0.1f0, a_opt::NamedTuple=(;), c_opt::NamedTuple=(;), log::NamedTuple=(;), kwargs...) = 
    OnPolicySolver(;
        Ï€ = Ï€,
        log = LoggerParams(;dir = "log/ppo", log...),
        a_opt = TrainingParams(;loss = ppo_loss(Ïµ=Ïµ, Î»â‚š=Î»â‚š, Î»â‚‘=Î»â‚‘), early_stopping = (info) -> (info[:kl] > 0.015), name = "actor_", a_opt...),
        c_opt = TrainingParams(;loss = (Ï€, D; kwargs...) -> Flux.mse(value(Ï€, D[:s]), D[:return]), name = "critic_", c_opt...),
        post_batch_callback = (ğ’Ÿ; kwargs...) -> (ğ’Ÿ[:advantage] .= whiten(ğ’Ÿ[:advantage])),
        kwargs...)
    



