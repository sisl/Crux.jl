# PPO loss
function ppo_loss(, , ; info = Dict())
    new_probs = logpdf(, [:s], [:a]) 
    r = exp.(new_probs .- [:logprob])
    
    A = [:advantage]
    p_loss = -mean(min.(r .* A, clamp.(r, (1f0 - [:系]), (1f0 + [:系])) .* A))
    e_loss = -mean(entropy(, [:s]))
    
    # Log useful information
    ignore() do
        info[:entropy] = -e_loss
        info[:kl] = mean([:logprob] .- new_probs)
        info[:clip_fraction] = sum((r .> 1 + [:系]) .| (r .< 1 - [:系])) / length(r)
    end 
    [:位p]*p_loss + [:位e]*e_loss
end

PPO(;::ActorCritic, 系::Float32 = 0.2f0, 位p::Float32 = 1f0, 位e::Float32 = 0.1f0, a_opt::NamedTuple=(;), c_opt::NamedTuple=(;), log::NamedTuple=(;), kwargs...) = 
    OnPolicySolver(;
         = ,
        =(系=系, 位p=位p, 位e=位e),
        log = LoggerParams(;dir = "log/ppo", log...),
        a_opt = TrainingParams(;loss = ppo_loss, early_stopping = (infos) -> (infos[end][:kl] > 0.015), name = "actor_", a_opt...),
        c_opt = TrainingParams(;loss = (, , D; kwargs...) -> Flux.mse(value(, D[:s]), D[:return]), name = "critic_", c_opt...),
        post_batch_callback = (; kwargs...) -> ([:advantage] .= whiten([:advantage])),
        kwargs...)
    



