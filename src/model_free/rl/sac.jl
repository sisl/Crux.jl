"""
SAC target function.
"""
function sac_target(Ï€)
    (Ï€â», ð’«, ð’Ÿ, Î³; kwargs...) -> begin
        ap, logprob = exploration(actor(Ï€), ð’Ÿ[:sp])
        y = ð’Ÿ[:r] .+ Î³ .* (1.f0 .- ð’Ÿ[:done]) .* (min.(value(Ï€â», ð’Ÿ[:sp], ap)...) .- exp(ð’«[:SAC_log_Î±][1]).*logprob)
    end
end

"""
Deterministic SAC target function.
"""
function sac_deterministic_target(Ï€)
    (Ï€â», ð’«, ð’Ÿ, Î³; kwargs...) -> begin
        y = ð’Ÿ[:r] .+ Î³ .* (1.f0 .- ð’Ÿ[:done]) .* min.(value(Ï€â», ð’Ÿ[:sp], action(actor(Ï€), ð’Ÿ[:sp]))...)
    end
end

"""
Max-Q SAC target function.
"""
function sac_max_q_target(Ï€)
    (Ï€â», ð’«, ð’Ÿ, Î³; kwargs...) -> begin
        error("not implemented")
        #TODO: Sample some number of actions and then choose the max
    end
end


"""
SAC actor loss function.
"""
function sac_actor_loss(Ï€, ð’«, ð’Ÿ; info = Dict())
    a, logprob = exploration(Ï€.A, ð’Ÿ[:s])
    ignore_derivatives() do
        info["entropy"] = -mean(logprob)
    end
    mean(exp(ð’«[:SAC_log_Î±][1]).*logprob .- min.(value(Ï€, ð’Ÿ[:s], a)...))
end

"""
SAC temp-based loss function.
"""
function sac_temp_loss(Ï€, ð’«, ð’Ÿ; info = Dict())
    ignore_derivatives() do
        info["SAC alpha"] = exp(ð’«[:SAC_log_Î±][1])
    end
    _, logprob = exploration(Ï€.A, ð’Ÿ[:s])
    target_Î± = logprob .+ ð’«[:SAC_H_target]
    -mean(exp(ð’«[:SAC_log_Î±][1]) .* target_Î±)
end


"""
Soft Actor Critic (SAC) solver.

```julia
SAC(;
    Ï€::ActorCritic{T, DoubleNetwork{ContinuousNetwork, ContinuousNetwork}},
    Î”N=50,
    SAC_Î±::Float32=1f0,
    SAC_H_target::Float32 = Float32(-prod(dim(action_space(Ï€)))),
    Ï€_explore=GaussianNoiseExplorationPolicy(0.1f0),
    SAC_Î±_opt::NamedTuple=(;),
    a_opt::NamedTuple=(;),
    c_opt::NamedTuple=(;),
    a_loss=sac_actor_loss,
    c_loss=double_Q_loss(),
    target_fn=sac_target(Ï€),
    prefix="",
    log::NamedTuple=(;),
    ð’«::NamedTuple=(;),
    param_optimizers=Dict(),
    kwargs...)
```
"""
function SAC(;
        Ï€::ActorCritic{T, DoubleNetwork{ContinuousNetwork, ContinuousNetwork}},
        Î”N=50,
        SAC_Î±::Float32=1f0,
        SAC_H_target::Float32 = Float32(-prod(dim(action_space(Ï€)))),
        Ï€_explore=GaussianNoiseExplorationPolicy(0.1f0),
        SAC_Î±_opt::NamedTuple=(;),
        a_opt::NamedTuple=(;),
        c_opt::NamedTuple=(;),
        a_loss=sac_actor_loss,
        c_loss=double_Q_loss(),
        target_fn=sac_target(Ï€),
        prefix="",
        log::NamedTuple=(;),
        ð’«::NamedTuple=(;),
        param_optimizers=Dict(),
        kwargs...) where T

    ð’« = (SAC_log_Î±=[Base.log(SAC_Î±)], SAC_H_target=SAC_H_target, ð’«...)
    OffPolicySolver(;agent=PolicyParams(Ï€=Ï€, Ï€_explore=Ï€_explore, Ï€â»=deepcopy(Ï€)),
                     Î”N=Î”N,
                     ð’«=ð’«,
                     log=LoggerParams(;dir = "log/sac", log...),
                     param_optimizers=Dict(Flux.params(ð’«[:SAC_log_Î±]) => TrainingParams(;loss=sac_temp_loss, name="temp_", SAC_Î±_opt...), param_optimizers...),
                     a_opt=TrainingParams(;loss=a_loss, name=string(prefix, "actor_"), a_opt...),
                     c_opt=TrainingParams(;loss=c_loss, name=string(prefix, "critic_"), epochs=Î”N, c_opt...),
                     target_fn=target_fn,
                     kwargs...)
end
