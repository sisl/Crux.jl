# Soft-Q is technically on-policy, as the learned Q maps to the exploration policy. As such we have two options:
# 1) use a DiscreteNetwork for the main policiy, and define our own wrapped exploration function for the exploration policy
# 2) define a SoftDiscreteNetwork and use this for POMDPs.action() and exploration() [Preferred]
# Will have to do something similar in the case of ContinuousNetwork for SoftActorCritic


## Network for representing a discrete set of outputs (value or policy)
# NOTE: Incoming actions (i.e. arguments) are all assumed to be one hot encoding. Outputs are discrete actions taken form outputs
mutable struct SoftDiscreteNetwork <: NetworkPolicy
    network
    outputs
    logit_conversion
    alpha
    always_stochastic
    device
    SoftDiscreteNetwork(network, outputs; logit_conversion=(Ï€, s) -> softmax(value(Ï€, s)), always_stochastic=false, dev=nothing) = new(network, cpu(outputs), logit_conversion, always_stochastic, device(network))
    SoftDiscreteNetwork(network, outputs, logit_conversion, always_stochastic, dev) = new(network, cpu(outputs), logit_conversion, always_stochastic, device(network))
end

Flux.@functor SoftDiscreteNetwork

Flux.trainable(Ï€::SoftDiscreteNetwork) = Flux.trainable(Ï€.network)

layers(Ï€::SoftDiscreteNetwork) = Ï€.network.layers

POMDPs.value(Ï€::SoftDiscreteNetwork, s) = mdcall(Ï€.network, s, Ï€.device)

POMDPs.value(Ï€::SoftDiscreteNetwork, s, a_oh) = sum(value(Ï€, s) .* a_oh, dims=1)

POMDPs.action(Ï€::SoftDiscreteNetwork, s) = Ï€.always_stochastic ? exploration(Ï€, s)[1] : Ï€.outputs[mapslices(argmax, value(Ï€, s), dims=1)]

function Flux.onehotbatch(Ï€::SoftDiscreteNetwork, a)
    ignore_derivatives() do
        a_oh = Flux.onehotbatch(a[:] |> cpu, Ï€.outputs) |> device(a)
        length(a) == 1 ? dropdims(a_oh, dims=2) : a_oh
    end
end

logits(Ï€::SoftDiscreteNetwork, s) = Ï€.logit_conversion(Ï€, s)

categorical_logpdf(probs, a_oh) = log.(sum(probs .* a_oh, dims=1))

function exploration(Ï€::SoftDiscreteNetwork, s; kwargs...)
    ps = logits(Ï€, s)
    ai = mapslices((v) -> rand(Categorical(v)), ps, dims=1)
    a = Ï€.outputs[ai]
    a, categorical_logpdf(ps, Flux.onehotbatch(Ï€, a))
end

function Distributions.logpdf(Ï€::SoftDiscreteNetwork, s, a)
    # If a does not seem to be a one-hot encoding then we encode it
    ignore_derivatives() do
        size(a, 1) == 1 && (a = Flux.onehotbatch(Ï€, a))
    end
    return categorical_logpdf(logits(Ï€, s), a)
end

function Distributions.entropy(Ï€::SoftDiscreteNetwork, s)
    ps = logits(Ï€, s)
    -sum(ps .* log.(ps .+ eps(Float32)), dims=1)
end

action_space(Ï€::SoftDiscreteNetwork) = DiscreteSpace(length(Ï€.outputs), Ï€.outputs)




########## 




# since explore is on (offpolicysolver), can just define our own function 
# a, log_probs = exploration(sampler.agent.Ï€_explore, sampler.svec, Ï€_on=sampler.agent.Ï€, i=i)
# exploration: action(s) propto softmax(q(s)/alpha) 


soft_value(Ï€, s; alpha::Float32=NaN32) = alpha .* logsumexp((value(Ï€, s) ./ alpha), dims=1)

# target = reward + (1-done)*gamma*v_target(sp)
# v_target(sp) = alpha*logsumexp(q_target(sp)/alpha) 
# update q(s, a) to target
# v(s) = alpha*logsumexp(q(s)/alpha)
function SoftQ_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; alpha::Float32=NaN32, kwargs...)
    ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* soft_value(Ï€, ğ’Ÿ[:sp], alpha=alpha)
end

function SoftQ(;Ï€::SoftDiscreteNetwork, 
          N::Int, 
          alpha::Real=0.5,
          Î”N=4, 
          c_opt::NamedTuple=(;), 
          log::NamedTuple=(;),
          c_loss=td_loss(),
          target_fn=SoftQ_target,
          prefix="",
          kwargs...)

          Ï€_explore = ... 
OffPolicySolver(;agent=PolicyParams(Ï€=Ï€, Ï€_explore=Ï€_explore, Ï€â»=deepcopy(Ï€)), 
                  log=LoggerParams(;dir="log/dqn", log...),
                  N=N,
                  Î”N=Î”N,
                  c_opt = TrainingParams(;loss=c_loss, name=string(prefix, "critic_"), epochs=Î”N, c_opt...),
                  target_fn=(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...) -> target_fn(Ï€, ğ’«, ğ’Ÿ, Î³;alpha=alpha, kwargs...),
                  kwargs...)
end 
    




