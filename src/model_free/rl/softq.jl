# Soft-Q is technically on-policy, as the learned Q maps to the exploration policy. As such we have two options:
# 1) use a DiscreteNetwork for the main policiy, and define our own wrapped exploration function for the exploration policy
# 2) define a SoftDiscreteNetwork and use this for POMDPs.action() and exploration() [Preferred]
# Will have to do something similar in the case of ContinuousNetwork for SoftActorCritic


## Network for representing a discrete set of outputs (value or policy)
# NOTE: Incoming actions (i.e. arguments) are all assumed to be one hot encoding. Outputs are discrete actions taken form outputs
mutable struct SoftDiscreteNetwork <: NetworkPolicy
    network
    outputs
    Î±
    device
    SoftDiscreteNetwork(network, outputs; Î±=1., dev=nothing) = new(network, cpu(outputs), Î±, device(network))
    SoftDiscreteNetwork(network, outputs, Î±, dev) = new(network, cpu(outputs), Î±, device(network))
end

#logit_conversion=(Ï€, s) -> softmax(value(Ï€, s)),

Flux.@functor SoftDiscreteNetwork

Flux.trainable(Ï€::SoftDiscreteNetwork) = Flux.trainable(Ï€.network)

layers(Ï€::SoftDiscreteNetwork) = Ï€.network.layers

function Flux.onehotbatch(Ï€::SoftDiscreteNetwork, a)
    ignore_derivatives() do
        a_oh = Flux.onehotbatch(a[:] |> cpu, Ï€.outputs) |> device(a)
        length(a) == 1 ? dropdims(a_oh, dims=2) : a_oh
    end
end

# Return Q(s,â‹…)
POMDPs.value(Ï€::SoftDiscreteNetwork, s) = mdcall(Ï€.network, s, Ï€.device)

# Return V(s)
soft_value(Ï€::SoftDiscreteNetwork, s) = Ï€.Î± .* logsumexp((value(Ï€, s) ./ Ï€.Î±), dims=1)

# Return Q(s,a)
POMDPs.value(Ï€::SoftDiscreteNetwork, s, a_oh) = sum(value(Ï€, s) .* a_oh, dims=1)

POMDPs.action(Ï€::SoftDiscreteNetwork, s) = exploration(Ï€, s)[1]

logits(Ï€::SoftDiscreteNetwork, s) = value(Ï€, s) ./ Ï€.Î±
probs(Ï€::SoftDiscreteNetwork, s) = softmax(logits(Ï€, s))

function exploration(Ï€::SoftDiscreteNetwork, s; kwargs...)
    ps = probs(Ï€, s)
    ai = mapslices((v) -> rand(Categorical(v)), ps, dims=1)
    a = Ï€.outputs[ai]
    a, categorical_logpdf(ps, Flux.onehotbatch(Ï€, a))
end

function Distributions.logpdf(Ï€::SoftDiscreteNetwork, s, a)
    # If a does not seem to be a one-hot encoding then we encode it
    ignore_derivatives() do
        size(a, 1) == 1 && (a = Flux.onehotbatch(Ï€, a))
    end
    return categorical_logpdf(probs(Ï€, s), a)
end

function Distributions.entropy(Ï€::SoftDiscreteNetwork, s)
    ps = probs(Ï€, s)
    -sum(ps .* log.(ps .+ eps(Float32)), dims=1)
end

action_space(Ï€::SoftDiscreteNetwork) = DiscreteSpace(length(Ï€.outputs), Ï€.outputs)




########## 

# since explore is on (offpolicysolver), can just define our own function 
# a, log_probs = exploration(sampler.agent.Ï€_explore, sampler.svec, Ï€_on=sampler.agent.Ï€, i=i)

# exploration: action(s) propto softmax(q(s)/alpha) 
# target = reward + (1-done)*gamma*v_target(sp)
# v(s) = alpha*logsumexp(q(s)/alpha)
# v_target(sp) = alpha*logsumexp(q_target(sp)/alpha) 
# update q(s, a) to target

function SoftQ_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
    ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* soft_value(Ï€, ğ’Ÿ[:sp])
end

function SoftQ(;Ï€::SoftDiscreteNetwork, 
          N::Int, 
          Î”N=4, 
          c_opt::NamedTuple=(;epochs=4), 
          log::NamedTuple=(;),
          c_loss=td_loss(),
          target_fn=SoftQ_target,
          prefix="",
          kwargs...)
OffPolicySolver(;agent=PolicyParams(Ï€=Ï€, Ï€â»=deepcopy(Ï€)), 
                  log=LoggerParams(;dir="log/dqn", log...),
                  N=N,
                  Î”N=Î”N,
                  c_opt = TrainingParams(;loss=c_loss, name=string(prefix, "critic_"), c_opt...),
                  target_fn=target_fn,
                  kwargs...)
end 
    




