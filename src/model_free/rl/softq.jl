# exploration: action(s) propto softmax(q(s)/alpha) 

# update target = reward + (1-done)*gamma*soft_v(sp)
# soft_v(s) = alpha*logsumexp(q(s)/alpha)
# update q(s, a) to target

soft_value(Ï€::DiscreteNetwork, s;Î±=Float32(1.)) = Î± .* logsumexp((value(Ï€, s) ./ Î±), dims=1)

function softq_target(Î±)
    (Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...) -> begin
        ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* soft_value(Ï€, ğ’Ÿ[:sp];Î±=Î±)
    end
end

function SoftQ(;Ï€::DiscreteNetwork, 
          N::Int, 
          Î”N=4, 
          c_opt::NamedTuple=(;epochs=4), 
          log::NamedTuple=(;),
          c_loss=td_loss(),
          Î±=Float32(1.),
          prefix="",
          kwargs...)

Ï€.always_stochastic = true
Ï€.logit_conversion = (Ï€, s) -> softmax(value(Ï€, s) ./ Î±)

OffPolicySolver(;agent=PolicyParams(Ï€=Ï€, Ï€â»=deepcopy(Ï€)), 
                  log=LoggerParams(;dir="log/softq", log...),
                  N=N,
                  Î”N=Î”N,
                  c_opt = TrainingParams(;loss=c_loss, name=string(prefix, "critic_"), c_opt...),
                  target_fn=softq_target(Î±),
                  kwargs...)
end 
    




