function TD3_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; i) 
    ap, _ = exploration(ğ’«[:Ï€_smooth], ğ’Ÿ[:sp], Ï€_on=Ï€, i=i)
    y = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* min.(value(Ï€, ğ’Ÿ[:sp], ap)...)
end

TD3_actor_loss(Ï€, ğ’«, ğ’Ÿ; info = Dict()) = -mean(value(Ï€.C.N1, ğ’Ÿ[:s], action(Ï€, ğ’Ÿ[:s])))

function TD3(;Ï€::ActorCritic{A, C}, 
              Î”N=50, 
              Ï€_smooth::Policy=GaussianNoiseExplorationPolicy(0.1f0, Ïµ_min=-0.5f0, Ïµ_max=0.5f0), 
              Ï€_explore=GaussianNoiseExplorationPolicy(0.1f0), 
              a_opt::NamedTuple=(;), 
              c_opt::NamedTuple=(;), 
              a_loss=TD3_actor_loss,
              c_loss=double_Q_loss(),
              target_fn=TD3_target,
              prefix="",
              log::NamedTuple=(;), 
              ğ’«::NamedTuple=(;), 
              kwargs...) where {A, C<:DoubleNetwork}
     
    OffPolicySolver(;agent=PolicyParams(Ï€=Ï€, Ï€_explore=Ï€_explore, Ï€â»=deepcopy(Ï€)),
                     Î”N=Î”N,
                     ğ’«=(Ï€_smooth=Ï€_smooth, ğ’«...),
                     log=LoggerParams(;dir = "log/td3", log...),
                     a_opt=TrainingParams(;loss=a_loss, name=string(prefix, "actor_"), a_opt...),
                     c_opt=TrainingParams(;loss=c_loss, name=string(prefix, "critic_"), epochs=Î”N, c_opt...),
                     target_fn=target_fn,
                     kwargs...)
end
        

