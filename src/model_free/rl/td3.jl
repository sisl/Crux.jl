function TD3_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; i) 
    ap, _ = exploration(ğ’«[:Ï€_smooth], ğ’Ÿ[:sp], Ï€_on=Ï€, i=i)
    y = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* min.(value(Ï€, ğ’Ÿ[:sp], ap)...)
end

TD3_actor_loss(Ï€, ğ’«, ğ’Ÿ; info = Dict()) = -mean(value(Ï€.C.N1, ğ’Ÿ[:s], action(Ï€, ğ’Ÿ[:s])))

TD3(;Ï€::ActorCritic{A, C}, Î”N=50, 
     Ï€_smooth::Policy=GaussianNoiseExplorationPolicy(0.1f0, Ïµ_min=-0.5f0, Ïµ_max=0.5f0), Ï€_explore=GaussianNoiseExplorationPolicy(0.1f0), a_opt::NamedTuple=(;), c_opt::NamedTuple=(;), log::NamedTuple=(;), kwargs...) where {A, C<:DoubleNetwork} = 
    OffPolicySolver(;
        Ï€ = Ï€,
        Î”N=Î”N,
        ğ’«=(Ï€_smooth=Ï€_smooth,),
        log = LoggerParams(;dir = "log/td3", log...),
        a_opt = TrainingParams(;loss=TD3_actor_loss, name="actor_", update_every=2, a_opt...),
        c_opt = TrainingParams(;loss=double_Q_loss, name="critic_", epochs=Î”N, c_opt...),
        Ï€_explore = Ï€_explore,
        target_fn = TD3_target,
        kwargs...)

