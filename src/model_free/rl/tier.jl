# ## Network for representing continous functions (value or policy)
# mutable struct LatentConditionedNetwork <: NetworkPolicy
#     policy
#     z
# end
# 
# device(Ï€::LatentConditionedNetwork) = device(Ï€.policy)
# 
# Flux.@functor LatentConditionedNetwork 
# 
# Flux.trainable(Ï€::LatentConditionedNetwork) = Flux.trainable(Ï€.policy)
# 
# layers(Ï€::LatentConditionedNetwork) = layers(Ï€.policy)
# 
# POMDPs.action(Ï€::LatentConditionedNetwork, s; z=Ï€.z) = value(Ï€, s; z=z)
# 
# function POMDPs.value(Ï€::LatentConditionedNetwork, s; z=Ï€.z) 
#     if size(z, 2) != size(s)[end]
#         z = repeat(z, 1, ndims(s) == 1 ? 1 : size(s)[end])
#     end
#     value(Ï€.policy, vcat(z,s))
# end
# 
# function POMDPs.value(Ï€::LatentConditionedNetwork, s, a; z=Ï€.z) 
#     if size(z, 2) != size(s)[end]
#         z = repeat(z, 1, ndims(s) == 1 ? 1 : size(s)[end])
#     end
#     value(Ï€.policy, vcat(z,s), a)
# end
# 
# 
# action_space(Ï€::LatentConditionedNetwork) = action_space(Ï€.policy)
# 
# 
# @with_kw mutable struct OffPolicyLatentSolver <: Solver
#     Ï€ # Policy
#     S::AbstractSpace # State space
#     A::AbstractSpace = action_space(Ï€) # Action space
#     N::Int = 1000 # Number of environment interactions
#     Î”N::Int = 4 # Number of interactions between updates
#     max_steps::Int = 100 # Maximum number of steps per episode
#     log::Union{Nothing, LoggerParams} = nothing # The logging parameters
#     i::Int = 0 # The current number of environment interactions
#     a_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the actor
#     c_opt::TrainingParams # Training parameters for the critic
#     post_batch_callback = (ğ’Ÿ) -> nothing
# 
#     # Off-policy-specific parameters
#     Ï€â» = deepcopy(Ï€)
#     Ï€_explore::Policy # exploration noise
#     target_update = (Ï€â», Ï€; kwargs...) -> polyak_average!(Ï€â», Ï€, 0.005f0) # Function for updating the target network
#     target_fn # Target for critic regression with input signature (Ï€â», ğ’Ÿ, Î³; i)
#     buffer_size = 1000 # Size of the buffer
#     required_columns = Symbol[]
#     buffer::ExperienceBuffer = ExperienceBuffer(S, A, buffer_size, required_columns) # The replay buffer
#     buffer_init::Int = max(c_opt.batch_size, 200) # Number of observations to initialize the buffer with
#     extra_buffers = [] # extra buffers (i.e. for experience replay in continual learning)
#     buffer_fractions = [1.0] # Fraction of the minibatch devoted to each buffer
#     z_dists = MvNormal[]
# end
# 
# TIER(;Ï€::ActorCritic, Î”N=50, Ï€_explore=GaussianNoiseExplorationPolicy(0.1f0),  a_opt::NamedTuple=(;), c_opt::NamedTuple=(;), log::NamedTuple=(;), Ï€_smooth::Policy=GaussianNoiseExplorationPolicy(0.1f0, Ïµ_min=-0.5f0, Ïµ_max=0.5f0), kwargs...) = 
#     OffPolicyLatentSolver(;
#         Ï€=Ï€, 
#         Î”N=Î”N,
#         log=LoggerParams(;dir = "log/ddpg", log...),
#         a_opt=TrainingParams(;loss=TD3_actor_loss_w_latent, name="actor_", a_opt...),
#         c_opt=TrainingParams(;loss=double_Q_loss_w_latent, name="critic_", epochs=Î”N, c_opt...),
#         Ï€_explore=Ï€_explore,
#         target_fn=TD3_target_w_latent(Ï€_smooth),
#         kwargs...)
# 
# # function optimize_latent(loss, latent_dim)
# #     rng = MersenneTwister(0)
# #     z_prospective = [Float32.(rand(rng, Uniform(-1,1), latent_dim)) for i=1:100]
# #     vals = [loss(z) for z in z_prospective]
# #     z_prospective[argmin(vals)]
# # end
# 
# function cross_entropy_optimization(f, P, latent_dim)
#     m = max(floor(Int, 1000 * norm(P.Î£)), 5*(latent_dim+3))
#     m_elite = floor(Int, 0.2*m)
#     # P = MvNormal(zeros(latent_dim), 0.5*I)
#     for i=1:1
#     # mi = max(floor(Int, m / length(Ps)), 1)
#     # samples = clamp.(hcat([rand(P,mi) for P in Ps]...), -1f0, 1f0)
#         # P = MvNormal(P.Î¼, P.Î£ + 1f-5*I)
#         samples = clamp.(rand(P, m), -1f0, 1f0)
#         samples .+= rand(Uniform(-1f-5, 1f-5), size(samples)...)
#         order = sortperm([f(samples[:,i]) for i=1:m])
#         P = fit(MvNormal, Float64.(samples[:, order[1:m_elite]]))
#     end
#     P
# end
# 
# function find_latent!(loss, P, latent_dim)
#     # ğ’Ÿ[:Î¼_z] .= optimize_latent(loss, latent_dim)
#     # Ps = [MvNormal(ğ’Ÿ[:Î¼_z][:,i], ğ’Ÿ[:Î£_z][:,:,i] + 1f-5 * I ) for i=1:length(ğ’Ÿ)]
#     P = cross_entropy_optimization(loss, P, latent_dim)
#     # ğ’Ÿ[:Î¼_z] .= P.Î¼
#     # P
#     # ğ’Ÿ[:Î£_z] .= P.Î£    
# end
# 
# function latent_loss(Ï€â», Ï€, Î³, ğ’Ÿ)
#     (z) -> begin
#         y = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* value(critic(Ï€â»), ğ’Ÿ[:sp], action(actor(Ï€â»), ğ’Ÿ[:sp], z=z), z=z)
#         Q = value(critic(Ï€), ğ’Ÿ[:s], ğ’Ÿ[:a], z=z)
#         Flux.mae(Q, y)
#     end
# end
# 
# function TD3_latent_loss(Ï€â», Ï€, Î³, ğ’Ÿ)
#     (z) -> begin
#         y = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* value(critic(Ï€).N1, ğ’Ÿ[:sp], action(actor(Ï€), ğ’Ÿ[:sp], z=z), z=z)
#         Q = value(critic(Ï€).N1, ğ’Ÿ[:s], ğ’Ÿ[:a], z=z)
#         Flux.mae(Q, y)
#     end
# end
# 
# function latent_target(Ï€, ğ’Ÿ, Î³; z=ğ’Ÿ[:Î¼_z], kwargs...)
#     ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* value(critic(Ï€), ğ’Ÿ[:sp], action(actor(Ï€), ğ’Ÿ[:sp], z=z), z=z)
# end 
# 
# function critic_loss_w_latent(Ï€, ğ’Ÿ, y; loss=Flux.mae, weighted=false, name=:Qavg, info=Dict())
#     Q = value(critic(Ï€), ğ’Ÿ[:s], ğ’Ÿ[:a]; z=ğ’Ÿ[:Î¼_z]) 
# 
# 
#     # Store useful information
#     ignore() do
#         info[name] = mean(Q)
#     end
# 
#     loss(Q, y, agg = weighted ? weighted_mean(ğ’Ÿ[:weight]) : mean)
# end
# 
# function double_Q_loss_w_latent(Ï€, ğ’Ÿ, y; info=Dict(), weighted=false)
#     q1loss = critic_loss_w_latent(Ï€.C.N1, ğ’Ÿ, y, info=info, name=:Q1avg, weighted=weighted)
#     q2loss = critic_loss_w_latent(Ï€.C.N2, ğ’Ÿ, y, info=info, name=:Q2avg, weighted=weighted)
#     q1loss + q2loss
# end
# 
# function actor_loss_w_latent(Ï€, ğ’Ÿ; info=Dict()) 
#     -mean(value(critic(Ï€), ğ’Ÿ[:s], action(actor(Ï€), ğ’Ÿ[:s], z=ğ’Ÿ[:Î¼_z]), z=ğ’Ÿ[:Î¼_z]))
# end
# 
# TD3_actor_loss_w_latent(Ï€, ğ’Ÿ; info = Dict()) = -mean(value(critic(Ï€).N1, ğ’Ÿ[:s], action(actor(Ï€), ğ’Ÿ[:s],z=ğ’Ÿ[:Î¼_z]), z=ğ’Ÿ[:Î¼_z]))
# 
# function TD3_target_w_latent(Ï€_smooth)
#     (Ï€, ğ’Ÿ, Î³::Float32; i,  z=ğ’Ÿ[:Î¼_z]) -> begin
#         ap, _ = exploration(Ï€_smooth, ğ’Ÿ[:sp], Ï€_on=Ï€, i=i)
#         y = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* min.(value(critic(Ï€).N1, ğ’Ÿ[:sp], ap, z=z), value(critic(Ï€).N2, ğ’Ÿ[:sp], ap, z=z))
#     end
# end
# 
# action_regularization_tier(Ï€, ğ’Ÿs) = length(ğ’Ÿs) == 0 ? 0 : mean([Flux.mse(action(actor(Ï€), ğ’Ÿ[:s], z=ğ’Ÿ[:Î¼_z]), ğ’Ÿ[:a]) for ğ’Ÿ in ğ’Ÿs])
# action_value_regularization_tier(Ï€, ğ’Ÿs) = length(ğ’Ÿs) == 0 ? 0 : mean([Flux.mse(value(critic(Ï€).N1, ğ’Ÿ[:s], ğ’Ÿ[:a], z=ğ’Ÿ[:Î¼_z]), ğ’Ÿ[:value]) for ğ’Ÿ in ğ’Ÿs]) +  mean([Flux.mse(value(critic(Ï€).N2, ğ’Ÿ[:s], ğ’Ÿ[:a], z=ğ’Ÿ[:Î¼_z]), ğ’Ÿ[:value]) for ğ’Ÿ in ğ’Ÿs])
# 
# 
# 
# function POMDPs.solve(ğ’®::OffPolicyLatentSolver, mdp)
#     # Compute the latent dimension
#     latent_dim = length(actor(ğ’®.Ï€).z)
# 
#     # Add data for normal distributions
#     # C = capacity(ğ’®.buffer)
#     # ğ’®.buffer.data[:Î¼_z] = zeros(Float32, latent_dim, C)
#     # ğ’®.buffer.data[:Î£_z] = 0.5f0*repeat(Array{Float32, 2}(I, latent_dim, latent_dim), outer=[1,1,C])
# 
#     # Create minibatch buffers for each buffer
#     allbuffs = [ğ’®.extra_buffers..., ğ’®.buffer]
#     push!(ğ’®.z_dists, MvNormal(zeros(latent_dim), 0.5*I))
#     # 
#     # for b in allbuffs
#     #     @assert haskey(b, :Î¼_z) && haskey(b, :Î£_z)
#     # end
# 
# 
# 
#     batches = split_batches(ğ’®.c_opt.batch_size, ğ’®.buffer_fractions)
#     ğ’Ÿs = [buffer_like(b, capacity=batchsize, device=device(ğ’®.Ï€)) for (b, batchsize) in zip(allbuffs, batches)]
#     # Add latent dimension
#     # last_z = [zeros(Float32, latent_dim) for _ in ğ’Ÿs]
# 
#     for ğ’Ÿ in ğ’Ÿs
#         ğ’Ÿ.data[:Î¼_z] = zeros(Float32, latent_dim, capacity(ğ’Ÿ))
#         # ğ’Ÿ.data[:Î£_z] = 0.5f0*repeat(Array{Float32, 2}(I, latent_dim, latent_dim), outer=[1,1,capacity(ğ’Ÿ)])
#         ğ’Ÿ.data[:value] = zeros(Float32, 1, capacity(ğ’Ÿ))
#     end
# 
#     Î³ = Float32(discount(mdp))
#     s = Sampler(mdp, ğ’®.Ï€, S=ğ’®.S, A=ğ’®.A, max_steps=ğ’®.max_steps, Ï€_explore=ğ’®.Ï€_explore, required_columns=extra_columns(ğ’®.buffer))
#     isnothing(ğ’®.log.sampler) && (ğ’®.log.sampler = s)
# 
#     # Log the pre-train performance
#     log(ğ’®.log, ğ’®.i)
# 
#     # Fill the buffer with initial observations before training
#     ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i=ğ’®.i, explore=true)
# 
#     # Loop over the desired number of environment interactions
#     for ğ’®.i in range(ğ’®.i, stop=ğ’®.i + ğ’®.N - ğ’®.Î”N, step=ğ’®.Î”N)
#         # Sample transitions into the replay buffer
#         data = steps!(s, Nsteps=ğ’®.Î”N, explore=true, i=ğ’®.i)
#         # data[:Î¼_z] = repeat(actor(ğ’®.Ï€).z, outer=[1,ğ’®.Î”N])
#         # data[:Î£_z] = repeat(Array{Float32, 2}(I, latent_dim, latent_dim), outer=[1,1,ğ’®.Î”N])
#         push!(ğ’®.buffer, data)
# 
#         # callback for potentially updating the buffer
#         ğ’®.post_batch_callback(ğ’®.buffer) 
# 
#         # Determine the latent variables
#         info_z = Dict()
#         for (b, index, Dz) in zip(allbuffs, 1:length(ğ’Ÿs), ğ’®.z_dists)
#             D = minibatch(b, rand(1:length(b), 1000))
#             z_before = ğ’®.z_dists[index].Î¼
#             ğ’®.z_dists[index] = find_latent!(TD3_latent_loss(ğ’®.Ï€â», ğ’®.Ï€, Î³, D), Dz, latent_dim)
#             # b[:Î¼_z][:,ğ’Ÿ.indices] .= ğ’Ÿ[:Î¼_z]
#             # b[:Î£_z][:,:,ğ’Ÿ.indices] .= ğ’Ÿ[:Î£_z]
#             z_after = ğ’®.z_dists[index].Î¼
#             info_z["Î”z$index"] = norm(z_after .- z_before) # Store the change in task identifier
#             info_z["z$index"] = norm(z_after) # Store the change in task identifier
#             info_z["Î£$index"] = norm(ğ’®.z_dists[index].Î£) # Store the norm of the covariance
#             info_z["td_error$index"] = mean(abs.(value(critic(ğ’®.Ï€).N1, D[:s], D[:a], z=z_after)  .- ğ’®.target_fn(ğ’®.Ï€â», D, Î³, i=ğ’®.i, z=z_after))) # store td error on the ith task
#         end
# 
# 
#         infos = []
#         # Loop over the desired number of training steps
#         for epoch in 1:ğ’®.c_opt.epochs
#             # Sample a random minibatch of ğ‘ transitions (sáµ¢, aáµ¢, ráµ¢, sáµ¢â‚Šâ‚) from ğ’Ÿ
#             for (ğ’Ÿ, b, index) in zip(ğ’Ÿs, allbuffs, 1:length(ğ’Ÿs))
#                 rand!(ğ’Ÿ, b, i=ğ’®.i) # sample a batch
#                 ğ’Ÿ[:Î¼_z] .= ğ’®.z_dists[index].Î¼ # set the parameters
#             end
# 
#             # Set the latent variable of the current actor and critic
#             z = ğ’®.z_dists[end].Î¼
#             actor(ğ’®.Ï€).z .= z
#             critic(ğ’®.Ï€).N1.z .= z
#             critic(ğ’®.Ï€).N2.z .= z
# 
#             # concatenate the minibatch buffers
#             ğ’Ÿ = hcat(ğ’Ÿs...)
# 
#             # Compute target
#             y = ğ’®.target_fn(ğ’®.Ï€â», ğ’Ÿ, Î³, i=ğ’®.i)
# 
#             # Train the critic
#             info = train!(critic(ğ’®.Ï€), (;kwargs...) -> ğ’®.c_opt.loss(ğ’®.Ï€, ğ’Ÿ, y; kwargs...) + action_value_regularization_tier(ğ’®.Ï€, ğ’Ÿs[1:end-1]), ğ’®.c_opt)
# 
#             # Train the actor 
#             if !isnothing(ğ’®.a_opt) && ((epoch-1) % ğ’®.a_opt.update_every) == 0
#                 info_a = train!(actor(ğ’®.Ï€), (;kwargs...) -> ğ’®.a_opt.loss(ğ’®.Ï€, ğ’Ÿ; kwargs...) + action_regularization_tier(ğ’®.Ï€, ğ’Ÿs[1:end-1]), ğ’®.a_opt)
#                 info = merge(info, info_a)
# 
#                 # Update the target network
#                 ğ’®.target_update(ğ’®.Ï€â», ğ’®.Ï€)
#             end
# 
#             # Store the training information
#             push!(infos, info)
# 
#         end
#         # If not using a separate actor, update target networks after critic training
#         isnothing(ğ’®.a_opt) && ğ’®.target_update(ğ’®.Ï€â», ğ’®.Ï€, i=ğ’®.i + 1:ğ’®.i + ğ’®.Î”N)
# 
#         # Log the results
#         log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, aggregate_info(infos), info_z)
#     end
#     ğ’®.i += ğ’®.Î”N
#     ğ’®.Ï€
# end
# 
# 
# # function optimize_latent(Ï€, loss, i, kmax=10)
# #     z, y = Ï€.z, loss()
# #     z_best, y_best = Ï€.z, y
# #     scale = Float32((1/5000)*i + 1)
# #     for k in 1:kmax
# #         Ï€.z = min.(max.(z .+ Float32.(randn(size(z)...)) ./ scale, -1f0), 1f0)
# #         yâ€² = loss()
# #         Î”y = yâ€² - y
# #         if Î”y â‰¤ 0 || rand() < exp(-Î”y*i)
# #             z, y = Ï€.z, yâ€²
# #         end
# #         if yâ€² < y_best
# #             z_best, y_best = Ï€.z, yâ€²
# #         end
# #     end
# #     Ï€.z = z_best 
# # end
# 
