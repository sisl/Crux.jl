## General GPU support for policies

function Flux.Optimise.train!(Ï€::Policy, loss::Function, opt, device; regularizer = (Î¸) -> 0)
    Î¸ = Flux.params(Ï€)
    l, back = Flux.pullback(() -> loss() + regularizer(Î¸), Î¸)
    grad = back(1f0)
    gnorm = norm(grad, p=Inf)
    Flux.update!(opt, Î¸, grad)
    l, gnorm
end

# Train with minibatches
function Flux.Optimise.train!(Ï€::Policy, loss::Function, ð’Ÿ::ExperienceBuffer, B, opt, device; rng::AbstractRNG = Random.GLOBAL_RNG)
    losses, grads = [], []
    for i in partition(shuffle(rng, 1:length(ð’Ÿ)), B)
        mb = minibatch(ð’Ÿ, i)
        l, g = train!(Ï€, ()->loss(mb), opt, device)
        push!(losses, l)
        push!(grads, g)
    end
    losses, grads
end


## Baseline
@with_kw mutable struct Baseline <: Policy
    V
    L = Flux.Losses.mse
    opt = ADAM(1f-3)
    steps::Int = 40
    Î»::Float32 = 0.95f0
    device = device(V)
end

Flux.params(b::Baseline) = Flux.params(b.V)

POMDPs.value(b::Baseline, s) = mdcall(b.V, s, b.device)

function Flux.Optimise.train!(b::Baseline, ð’Ÿ::ExperienceBuffer)
    Î¸ = Flux.params(b)
    data = Flux.Data.DataLoader((ð’Ÿ[:s], ð’Ÿ[:return]), batchsize = length(ð’Ÿ))
    for i=1:b.steps
        train!((x,y) -> b.L(value(b, x), y), Î¸, data, b.opt)
    end
end
    



## Deep Q-network Policy
@with_kw mutable struct DQNPolicy <: Policy
    Q
    actions::Vector
    device = device(Q)
    Qâ» = deepcopy(Q)
end

Flux.params(Ï€::DQNPolicy) = Flux.params(Ï€.Q)

POMDPs.action(Ï€::DQNPolicy, s::S) where S <: AbstractArray = Ï€.actions[argmax(value(Ï€, s))]

POMDPs.value(Ï€::DQNPolicy, s::S) where S <: AbstractArray = mdcall(Ï€.Q, s, Ï€.device)

action_space(Ï€::DQNPolicy) = DiscreteSpace(length(Ï€.actions))


## Categorical Policy
@with_kw mutable struct CategoricalPolicy <: Policy
    A
    actions
    device = device(A)
    rng::AbstractRNG = Random.GLOBAL_RNG
end

Flux.params(Ï€::CategoricalPolicy) = Flux.params(Ï€.A)

POMDPs.action(Ï€::CategoricalPolicy, s::AbstractArray) = Ï€.actions[rand(Ï€.rng, Categorical(logits(Ï€, s)[:]))]

logits(Ï€::CategoricalPolicy, s::AbstractArray) = mdcall(Ï€.A, s, Ï€.device)
    
function logpdf(Ï€::CategoricalPolicy, s::AbstractArray, a::AbstractArray)
    log.(sum(logits(Ï€, s) .* a, dims = 1) .+ eps(Float32))
end

action_space(Ï€::CategoricalPolicy) = DiscreteSpace(length(Ï€.actions))


## Gaussian Policy
@with_kw mutable struct GaussianPolicy <: Policy
    Î¼
    logÎ£
    device = device(Î¼)
    rng::AbstractRNG = Random.GLOBAL_RNG
end

Flux.params(Ï€::GaussianPolicy) = Flux.params(Ï€.Î¼, Ï€.logÎ£)

function POMDPs.action(Ï€::GaussianPolicy, s::AbstractArray)
    Î¼, logÎ£ = mdcall(Ï€.Î¼, s, Ï€.device), device(s)(Ï€.logÎ£)
    d = MvNormal(Î¼, diagm(0=>exp.(logÎ£).^2))
    a = rand(Ï€.rng, d)
    @assert length(a) == 1
    a[1]
end

function logpdf(Ï€::GaussianPolicy, s::AbstractArray, a::AbstractArray)
    Î¼ = mdcall(Ï€.Î¼, s, device)
    Ïƒ = exp.(Ï€.logÎ£)
    ÏƒÂ² = Ïƒ.^2
    broadcast(-, ((a .- Î¼).^2f0)./(2f0 .* ÏƒÂ²)) .-  0.4594692666f0 .- log.(Ïƒ)
end

action_space(Ï€::GaussianPolicy) = ContinuousSpace((length(Ï€.logÎ£),), typeof(Ï€.logÎ£[1]))

