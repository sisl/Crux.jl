## General GPU support for policies
Flux.params(Ï€::Policy, device) = Flux.params(network(Ï€, device)...)

device(Ï€::Policy) = isnothing(network(Ï€, gpu)[1]) ? cpu : gpu

function sync!(Ï€::Policy, device)
    device == cpu && return 
    cpu_nets, gpu_nets = network(Ï€, cpu),  network(Ï€, gpu)
    for i=1:length(cpu_nets)
        copyto!(cpu_nets[i], gpu_nets[i])
    end
end

function Flux.Optimise.train!(Ï€::Policy, loss::Function, opt, device)
    Î¸ = Flux.params(Ï€, device)
    l, back = Flux.pullback(loss, Î¸)
    grad = back(1f0)
    gnorm = norm(grad, p=Inf)
    Flux.update!(opt, Î¸, grad)
    sync!(Ï€, device)
    l, gnorm
end

function Flux.Optimise.train!(Ï€::Policy, loss::Function, ğ’Ÿ::ExperienceBuffer, B, opt, device; rng::AbstractRNG = Random.GLOBAL_RNG)
    losses, grads = [], []
    for i in partition(shuffle(rng, 1:length(ğ’Ÿ)), B)
        mb = minibatch(ğ’Ÿ, i)
        l, g = train!(Ï€, ()->loss(mb), opt, device)
        push!(losses, l)
        push!(grads, g)
    end
    losses, grads
end


## Baseline
mutable struct Baseline <: Policy
    V
    L
    opt
    steps::Int
    Î»::Float32
    V_GPU
end

Baseline(V; L = Flux.Losses.mse, opt = ADAM(1f-3), steps::Int = 40, Î»::Float32 = 0.95f0, device = cpu) = Baseline(V, L, opt, steps, Î», todevice(V, device))

network(b::Baseline, device) = (device == gpu) ? [b.V_GPU] : [b.V]

POMDPs.value(b::Baseline, s) = network(b, device(s))[1](s)

function Flux.Optimise.train!(b::Baseline, ğ’Ÿ::ExperienceBuffer)
    Î¸ = Flux.params(b, device(ğ’Ÿ))
    data = Flux.Data.DataLoader((ğ’Ÿ[:s], ğ’Ÿ[:return]), batchsize = length(ğ’Ÿ))
    for i=1:b.steps
        train!((x,y) -> b.L(value(b, x), y), Î¸, data, b.opt)
    end
    sync!(b,  device(ğ’Ÿ))
end
    



## Deep Q-network Policy
mutable struct DQNPolicy <: Policy
    Q
    actions
    Q_GPU
    Qâ»
end

DQNPolicy(Q, actions; device = cpu) = DQNPolicy(Q, actions, todevice(Q, device), deepcopy(Q) |> device)

network(Ï€::DQNPolicy, device) = (device == gpu) ? [Ï€.Q_GPU] : [Ï€.Q]

POMDPs.action(Ï€::DQNPolicy, s::S) where S <: AbstractArray = Ï€.actions[argmax(value(Ï€, s))]

POMDPs.value(Ï€::DQNPolicy, s::S) where S <: AbstractArray = network(Ï€, device(s))[1](s)


## Categorical Policy
mutable struct CategoricalPolicy <: Policy
    A
    actions
    rng::AbstractRNG
    A_GPU
end

CategoricalPolicy(A, actions; device = cpu, rng::AbstractRNG = Random.GLOBAL_RNG) = CategoricalPolicy(A, actions, rng, todevice(A, device))

network(Ï€::CategoricalPolicy, device) = (device == gpu) ? [Ï€.A_GPU] : [Ï€.A]

POMDPs.action(Ï€::CategoricalPolicy, s::AbstractArray) = Ï€.actions[rand(Ï€.rng, Categorical(Ï€.A(s)))]

logits(Ï€::CategoricalPolicy, s::AbstractArray) = network(Ï€, device(s))[1](s)
    
function Distributions.logpdf(Ï€::CategoricalPolicy, s::AbstractArray, a::AbstractArray)
    log.(sum(logits(Ï€, s) .* a, dims = 1) .+ eps(Float32))
end


## Gaussian Policy
@with_kw mutable struct GaussianPolicy <: Policy
    Î¼
    logÎ£
    rng::AbstractRNG = Random.GLOBAL_RNG
    Î¼_GPU = nothing
    logÎ£_GPU = nothing
end

GaussianPolicy(Î¼, logÎ£; rng::AbstractRNG = Random.GLOBAL_RNG) = GaussianPolicy(Î¼, logÎ£, rng, todevice(Î¼, device), todevice(logÎ£, device))

network(Ï€::GaussianPolicy, device) = (device == gpu) ? [Ï€.Î¼, Ï€.logÎ£] : [Ï€.Î¼_GPU, Ï€.logÎ£_GPU]

function POMDPs.action(Ï€::GaussianPolicy, s::AbstractArray)
    d = MvNormal(Ï€.Î¼(s), diagm(0=>exp.(Ï€.logÎ£).^2))
    rand(rng, d)
end

function Distributions.logpdf(Ï€::GaussianPolicy, s::AbstractArray, a::AbstractArray)
    Î¼_net, logÎ£_net = network(p, device(s))
    Î¼ = Î¼_net(s)
    Ïƒ2 = exp.(logÎ£_net).^2
    sum((a .- (Î¼ ./ Ïƒ2)).^2 .- 0.5 * log.(6.2831853071794*Ïƒ2), dims=2)
end

