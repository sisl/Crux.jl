## General GPU support for policies
Flux.params(Ï€::Policy, device) = Flux.params(network(Ï€, device)...)

function sync!(Ï€::Policy, device)
    device == cpu && return 
    cpu_nets, gpu_nets = network(Ï€, cpu),  network(Ï€, gpu)
    for i=1:length(cpu_nets)
        copyto!(cpu_nets[i], gpu_nets[i])
    end
end

function Flux.Optimise.train!(Ï€::Policy, loss::Function, opt, device)
    Î¸ = Flux.params(Ï€, device)
    loss, back = Flux.pullback(loss, Î¸)
    grad = back(1f0)
    Flux.update!(opt, Î¸, grad)
    sync!(Ï€, device)
    loss, grad
end

## Baseline
mutable struct Baseline <: Policy
    V
    L
    opt
    steps::Int
    Î»::Float32
    V_GPU
end

Baseline(V; L = Flux.Losses.mse, opt = ADAM(1f-3), steps::Int = 40, Î»::Float32 = 0.95f0, device = cpu) = Baseline(V, L, opt, steps, Î», todevice(V, device))

network(b::Baseline, device) = (device == gpu) ? [b.V_GPU] : [b.V]

POMDPs.value(b::Baseline, s) = network(b, device(s))[1](s)

function Flux.Optimise.train!(b::Baseline, ğ’Ÿ::ExperienceBuffer)
    Î¸ = Flux.params(b, device(ğ’Ÿ))
    data = Flux.Data.DataLoader((ğ’Ÿ[:s], ğ’Ÿ[:return]), batchsize = length(ğ’Ÿ))
    for i=1:b.steps
        train!((x,y) -> b.L(value(b, x), y), Î¸, data, b.opt)
    end
    sync!(b,  device(ğ’Ÿ))
end

function fill_gae!(b::ExperienceBuffer, start::Int, Nsteps::Int, V, Î»::Float32, Î³::Float32)
    A, c = 0f0, Î»*Î³
    for i in reverse(get_indices(b, start, Nsteps))
        Vsp = V(b[:sp][:,i])
        Vs = V(b[:s][:,i])
        @assert length(Vs) == 1
        A = c*A + b[:r][1,i] + (1.f0 - b[:done][1,i])*Î³*Vsp[1] - Vs[1]
        b[:advantage][:, i] .= A
    end
end

function fill_returns!(b::ExperienceBuffer, start::Int, Nsteps::Int, Î³::Float32)
    r = 0f0
    for i in reverse(get_indices(b, start, Nsteps))
        r = b[:r][i] + Î³*r
        b[:return][:, i] .= r
    end
end

## Categorical Policy
mutable struct DQNPolicy <: Policy
    Q
    mdp
    Q_GPU
end

DQNPolicy(Q, mdp; device = cpu) = DQNPolicy(Q, mdp, todevice(Q, device))

network(Ï€::DQNPolicy, device) = (device == gpu) ? [Ï€.Q_GPU] : [Ï€.Q]

POMDPs.action(Ï€::DQNPolicy, s) = actions(Ï€.mdp)[argmax(Ï€.Q(convert_s(AbstractVector, s, Ï€.mdp)))]

POMDPs.value(Ï€::DQNPolicy, s::AbstractArray) = network(Ï€, device(s))[1](s)

## Categorical Policy
mutable struct CategoricalPolicy <: Policy
    A
    mdp
    rng::AbstractRNG
    A_GPU
end

CategoricalPolicy(A, mdp; device = cpu, rng::AbstractRNG = Random.GLOBAL_RNG) = CategoricalPolicy(A, mdp, rng, todevice(A, device))

network(Ï€::CategoricalPolicy, device) = (device == gpu) ? [Ï€.A_GPU] : [Ï€.A]

POMDPs.action(Ï€::CategoricalPolicy, s) = actions(Ï€.mdp)[rand(Ï€.rng, Categorical(Ï€.A(convert_s(AbstractVector, s, Ï€.mdp))))]

logits(Ï€::CategoricalPolicy, s::AbstractArray) = network(Ï€, device(s))[1](s)
    
function Distributions.logpdf(Ï€::CategoricalPolicy, s::AbstractArray, a::AbstractArray)
    log.(sum(logits(Ï€, s) .* a, dims = 1) .+ eps(Float32))
end


## Gaussian Policy
@with_kw mutable struct GaussianPolicy <: Policy
    Î¼
    logÎ£
    mdp
    rng::AbstractRNG = Random.GLOBAL_RNG
    Î¼_GPU = nothing
    logÎ£_GPU = nothing
end

GaussianPolicy(Î¼, logÎ£, mdp; rng::AbstractRNG = Random.GLOBAL_RNG) = GaussianPolicy(Î¼, logÎ£, mdp, rng, todevice(Î¼, device), todevice(logÎ£, device))

network(Ï€::GaussianPolicy, device) = (device == gpu) ? [Ï€.Î¼, Ï€.logÎ£] : [Ï€.Î¼_GPU, Ï€.logÎ£_GPU]

function POMDPs.action(Ï€::GaussianPolicy, s)
    svec = convert_s(AbstractVector, s, Ï€.mdp)
    d = MvNormal(Ï€.Î¼(svec), diagm(0=>exp.(Ï€.logÎ£).^2))
    rand(rng, d)
end

function Distributions.logpdf(Ï€::GaussianPolicy, s::AbstractArray, a::AbstractArray)
    Î¼_net, logÎ£_net = network(p, device(s))
    Î¼ = Î¼_net(s)
    Ïƒ2 = exp.(logÎ£_net).^2
    sum((a .- (Î¼ ./ Ïƒ2)).^2 .- 0.5 * log.(6.2831853071794*Ïƒ2), dims=2)
end

