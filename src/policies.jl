## Deep Q-network Policy
@with_kw mutable struct DQNPolicy <: Policy
    Q
    actions::Vector
    device = device(Q)
    Qâ» = deepcopy(Q)
end

Flux.trainable(Ï€::DQNPolicy) = Flux.trainable(Ï€.Q)

POMDPs.action(Ï€::DQNPolicy, s::S) where S <: AbstractArray = Ï€.actions[argmax(value(Ï€, s))]

POMDPs.value(Ï€::DQNPolicy, s::S) where S <: AbstractArray = mdcall(Ï€.Q, s, Ï€.device)

action_space(Ï€::DQNPolicy) = DiscreteSpace(length(Ï€.actions))

## Actor Critic Architecture
@with_kw mutable struct ActorCritic <: Policy
    A # actor 
    C # critic
end

Flux.trainable(Ï€::ActorCritic) = (Flux.trainable(Ï€.A)..., Flux.trainable(Ï€.C)...)

POMDPs.value(Ï€::ActorCritic, s; kwargs...) = value(Ï€.C, s; kwargs...)

POMDPs.action(Ï€::ActorCritic, s::AbstractArray) = action(Ï€.A, s)
    
logpdf(Ï€::ActorCritic, s::AbstractArray, a::AbstractArray) = logpdf(Ï€.A, s, a)

action_space(Ï€::ActorCritic) = action_space(Ï€.A)

entropy(Ï€::ActorCritic, s::AbstractArray) = entropy(Ï€.A, s)


## Categorical Policy
@with_kw mutable struct CategoricalPolicy <: Policy
    A
    actions
    device = device(A)
    rng::AbstractRNG = Random.GLOBAL_RNG
end

Flux.trainable(Ï€::CategoricalPolicy) = Flux.trainable(Ï€.A)

POMDPs.action(Ï€::CategoricalPolicy, s::AbstractArray) = Ï€.actions[rand(Ï€.rng, Categorical(logits(Ï€, s)[:]))]

logits(Ï€::CategoricalPolicy, s::AbstractArray) = mdcall(Ï€.A, s, Ï€.device)
    
function logpdf(Ï€::CategoricalPolicy, s::AbstractArray, a::AbstractArray)
    log.(sum(logits(Ï€, s) .* a, dims = 1) .+ eps(Float32))
end

function entropy(Ï€::CategoricalPolicy, s::AbstractArray)
    aprob = logits(Ï€, s)
    sum(aprob .* log.(aprob .+ eps(Float32)), dims=1)
end

action_space(Ï€::CategoricalPolicy) = DiscreteSpace(length(Ï€.actions))


## Gaussian Policy
@with_kw mutable struct GaussianPolicy <: Policy
    Î¼
    logÎ£
    device = device(Î¼)
    rng::AbstractRNG = Random.GLOBAL_RNG
end

Flux.trainable(Ï€::GaussianPolicy) = (Flux.trainable(Ï€.Î¼)..., Ï€.logÎ£)

function POMDPs.action(Ï€::GaussianPolicy, s::AbstractArray)
    Î¼, logÎ£ = mdcall(Ï€.Î¼, s, Ï€.device), device(s)(Ï€.logÎ£)
    d = MvNormal(Î¼, exp.(logÎ£))
    a = rand(Ï€.rng, d)
end

function logpdf(Ï€::GaussianPolicy, s::AbstractArray, a::AbstractArray)
    Î¼ = mdcall(Ï€.Î¼, s, Ï€.device)
    logÎ£ = device(s)(Ï€.logÎ£)
    ÏƒÂ² = exp.(logÎ£).^2
    sum(-((a .- Î¼).^2) ./ (2 .* ÏƒÂ²) .-  0.9189385332046727f0 .- logÎ£, dims = 1) # 0.9189385332046727f0 = log.(sqrt(2Ï€))
end

entropy(Ï€::GaussianPolicy, s::AbstractArray) = 1.4189385332046727f0 .+ Ï€.logÎ£ # 1.4189385332046727 = 0.5 + 0.5 * log(2Ï€)

action_space(Ï€::GaussianPolicy) = ContinuousSpace((length(Ï€.logÎ£),), typeof(cpu(Ï€.logÎ£)[1]))

## Linear Policy - Archived for now
# @with_kw mutable struct LinearBaseline <: Baseline
#     Î¸ = nothing
#     featurize::Function = control_features
#     c::Float32 = eps(Float32) # regularization_ceoff
#     Î»::Float32 = 0.95f0 # gae
#     device = cpu
# end
# 
# function control_features(s::AbstractArray; t::AbstractArray)
#     vcat(s, s.^2, t, t.^2, t.^3, ones(Float32, 1, size(s,2)))
# end
# 
# function Flux.Optimise.train!(b::LinearBaseline, ð’Ÿ::ExperienceBuffer)
#     X = b.featurize(ð’Ÿ[:s], t = ð’Ÿ[:t])
#     y = ð’Ÿ[:return]
#     d, n = size(X)
#     A = X * X' ./ n + b.c*b.device(Matrix{typeof(X[1])}(I,d,d))
#     B = X * y' ./ n
#     b.Î¸ = dropdims(pinv(A) * B, dims = 2)
# end
# 
# POMDPs.value(b::LinearBaseline, s; kwargs...) = b.Î¸' * b.featurize(s; kwargs...) 

