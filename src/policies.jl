abstract type NetworkPolicy <: Policy end
device(Ï€::NetworkPolicy) = Ï€.device

function polyak_average!(to, from, Ï„=1f0)
    to_data = Flux.params(to).order.data
    from_data, from_device = Flux.params(from).order.data, device(from)
    device_match = from_device == device(to)
    for i = 1:length(to_data)
        if device_match
            copyto!(to_data[i], Ï„.*from_data[i] .+ (1f0-Ï„).*to_data[i])
        else
            copyto!(to_data[i], Ï„.*from_data[i] .+ (1f0-Ï„).*from_device(to_data[i]))
        end            
    end
end

function Base.copyto!(to, from)
    for i = 1:length(Flux.params(to).order.data)
        copyto!(Flux.params(to).order.data[i], Flux.params(from).order.data[i])
    end
end

## Network for representing continous functions (value or policy)
@with_kw mutable struct ContinuousNetwork <: NetworkPolicy
    network
    output_dim = size(last(network.layers).b)
    device = device(network)
end

ContinuousNetwork(network, output_dim = size(last(network.layers).b)) = ContinuousNetwork(network = network, output_dim = output_dim)

Flux.trainable(Ï€::ContinuousNetwork) = Flux.trainable(Ï€.network)

POMDPs.action(Ï€::ContinuousNetwork, s) = value(Ï€, s)

POMDPs.value(Ï€::ContinuousNetwork, s::AbstractArray) = mdcall(Ï€.network, s, Ï€.device)
POMDPs.value(Ï€::ContinuousNetwork, s::AbstractArray, a::AbstractArray) = mdcall(Ï€.network, vcat(s,a), Ï€.device)

action_space(Ï€::ContinuousNetwork) = ContinuousSpace(Ï€.output_dim)


## Network for representing a discrete set of outputs (value or policy)
@with_kw mutable struct DiscreteNetwork <: NetworkPolicy
    network
    outputs::Vector
    device = device(network)
    rng::AbstractRNG = Random.GLOBAL_RNG
end

DiscreteNetwork(network, outputs::Vector; kwargs...) = DiscreteNetwork(network = network, outputs = outputs; kwargs...)
Flux.trainable(Ï€::DiscreteNetwork) = Flux.trainable(Ï€.network)

POMDPs.action(Ï€::DiscreteNetwork, s::S) where S <: AbstractArray = Ï€.outputs[argmax(value(Ï€, s))] # Deterministic
POMDPs.action(Ï€::DiscreteNetwork, on_policy::Policy, k, s::AbstractArray) = Ï€.outputs[rand(Ï€.rng, Categorical(value(Ï€, s)[:]))] # Stochastic

POMDPs.value(Ï€::DiscreteNetwork, s::S) where S <: AbstractArray = mdcall(Ï€.network, s, Ï€.device)
POMDPs.value(Ï€::DiscreteNetwork, s::AbstractArray, a::AbstractArray) = sum(value(Ï€, s) .* a, dims = 1)


action_space(Ï€::DiscreteNetwork) = DiscreteSpace(length(Ï€.outputs))

function logpdf(Ï€::DiscreteNetwork, s::AbstractArray, a::AbstractArray)
    log.(sum(value(Ï€, s) .* a, dims = 1) .+ eps(Float32))
end

function entropy(Ï€::DiscreteNetwork, s::AbstractArray)
    aprob = value(Ï€, s)
    sum(aprob .* log.(aprob .+ eps(Float32)), dims=1)
end



## Actor Critic Architecture
@with_kw mutable struct ActorCritic{TA, TC} <: NetworkPolicy
    A::TA # actor 
    C::TC # critic
end

device(Ï€::ActorCritic) = device(Ï€.A)

Flux.trainable(Ï€::ActorCritic) = (Flux.trainable(Ï€.A)..., Flux.trainable(Ï€.C)...)

POMDPs.value(Ï€::ActorCritic, s) = value(Ï€.C, s)
POMDPs.value(Ï€::ActorCritic, s, a) = value(Ï€.C, s, a)

POMDPs.action(Ï€::ActorCritic, s::AbstractArray) = action(Ï€.A, s)
POMDPs.action(Ï€::ActorCritic, on_policy::Policy, k, s::AbstractArray) = action(Ï€.A, on_policy, k, s)
    
logpdf(Ï€::ActorCritic, s::AbstractArray, a::AbstractArray) = logpdf(Ï€.A, s, a)

action_space(Ï€::ActorCritic) = action_space(Ï€.A)

entropy(Ï€::ActorCritic, s::AbstractArray) = entropy(Ï€.A, s)


## Gaussian Policy
@with_kw mutable struct GaussianPolicy <: NetworkPolicy
    Î¼::ContinuousNetwork
    logÎ£::AbstractArray
    device = device(Î¼)
    rng::AbstractRNG = Random.GLOBAL_RNG
end

GaussianPolicy(Î¼, logÎ£; kwargs...) = GaussianPolicy(Î¼ = Î¼, logÎ£ = logÎ£; kwargs...)

Flux.trainable(Ï€::GaussianPolicy) = (Flux.trainable(Ï€.Î¼)..., Ï€.logÎ£)

POMDPs.action(Ï€::GaussianPolicy, s::AbstractArray) = action(Ï€.Î¼, s)

function POMDPs.action(Ï€::GaussianPolicy, on_policy::Policy, k, s::AbstractArray) 
    Î¼, logÎ£ = action(Ï€, s), device(s)(Ï€.logÎ£)
    d = MvNormal(Î¼, exp.(logÎ£))
    a = rand(Ï€.rng, d)
end

function logpdf(Ï€::GaussianPolicy, s::AbstractArray, a::AbstractArray)
    Î¼, logÎ£ = action(Ï€, s), device(s)(Ï€.logÎ£)
    ÏƒÂ² = exp.(logÎ£).^2
    sum(-((a .- Î¼).^2) ./ (2 .* ÏƒÂ²) .-  0.9189385332046727f0 .- logÎ£, dims = 1) # 0.9189385332046727f0 = log.(sqrt(2Ï€))
end

entropy(Ï€::GaussianPolicy, s::AbstractArray) = 1.4189385332046727f0 .+ Ï€.logÎ£ # 1.4189385332046727 = 0.5 + 0.5 * log(2Ï€)

action_space(Ï€::GaussianPolicy) = action_space(Ï€.Î¼)


## Exploration policy with Gaussian noise
@with_kw mutable struct GaussianNoiseExplorationPolicy <: ExplorationPolicy
    Ïƒ::Function = (i) -> 0.01f0
    clip_min::Vector{Float32} = [-Inf32]
    clip_max::Vector{Float32} = [Inf32]
    rng::AbstractRNG = Random.GLOBAL_RNG
end

GaussianNoiseExplorationPolicy(Ïƒ::Real; kwargs...) = GaussianNoiseExplorationPolicy(Ïƒ = (i) -> Ïƒ; kwargs...)
GaussianNoiseExplorationPolicy(Ïƒ::Function; kwargs...) = GaussianNoiseExplorationPolicy(Ïƒ = (i) -> Ïƒ; kwargs...)

function POMDPs.action(Ï€::GaussianNoiseExplorationPolicy, on_policy::Policy, k, s::AbstractArray)
    a = action(on_policy, s)
    Ïµ = randn(Ï€.rng, length(a))*Ï€.Ïƒ(k)
    return clamp.(a + Ïµ, Ï€.clip_min, Ï€.clip_max)
end


## use exploration policy for first N timesteps, then revert to base policy
@with_kw mutable struct FirstExplorePolicy <: ExplorationPolicy
    N::Int64 # Number of steps to explore for
    initial_policy::Policy # the policy to use for the first N steps
    after_policy::Union{Nothing, ExplorationPolicy} = nothing # the policy to use after the first N steps. Nothing means you will use on-policy
end

FirstExplorePolicy(N::Int64, initial_policy::Policy) = FirstExplorePolicy(N, initial_policy, after_policy)

function POMDPs.action(Ï€::FirstExplorePolicy, on_policy::Policy, k, s::AbstractArray)
    if k < Ï€.N
        return action(Ï€.initial_policy, s)
    elseif isnothing(Ï€.after_policy)
        return action(on_policy, s)
    else
        return action(Ï€.after_policy, on_policy, k, s)
    end
end

## Linear Policy - Archived for now
# @with_kw mutable struct LinearBaseline <: Baseline
#     Î¸ = nothing
#     featurize::Function = control_features
#     c::Float32 = eps(Float32) # regularization_ceoff
#     Î»::Float32 = 0.95f0 # gae
#     device = cpu
# end
# 
# function control_features(s::AbstractArray; t::AbstractArray)
#     vcat(s, s.^2, t, t.^2, t.^3, ones(Float32, 1, size(s,2)))
# end
# 
# function Flux.Optimise.train!(b::LinearBaseline, ð’Ÿ::ExperienceBuffer)
#     X = b.featurize(ð’Ÿ[:s], t = ð’Ÿ[:t])
#     y = ð’Ÿ[:return]
#     d, n = size(X)
#     A = X * X' ./ n + b.c*b.device(Matrix{typeof(X[1])}(I,d,d))
#     B = X * y' ./ n
#     b.Î¸ = dropdims(pinv(A) * B, dims = 2)
# end
# 
# POMDPs.value(b::LinearBaseline, s; kwargs...) = b.Î¸' * b.featurize(s; kwargs...) 

