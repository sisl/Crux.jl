@with_kw mutable struct PGSolver <: Solver 
    Ï€::Policy
    S::AbstractSpace
    A::AbstractSpace = action_space(Ï€)
    loss
    N::Int64 = 1000
    Î”N::Int = 200
    Î»_gae::Float32 = 0.95
    batch_size::Int = 64
    batch_size_v::Int = batch_size
    epochs::Int = 10
    epochs_v::Int = epochs
    max_steps::Int64 = 100
    eval_eps::Int = 10
    opt = ADAM(3e-4)
    opt_v = deepcopy(opt)
    loss_v = (Ï€, D; kwargs...) -> Flux.mse(value(Ï€, D[:s]), D[:return])
    regularizer = (Î¸) -> 0
    regularizer_v = regularizer
    early_stopping = (info) -> false
    required_columns = Ï€ isa ActorCritic ? [:return, :advantage, :logprob] : [:return, :logprob]
    normalize_advantage = (:advantage in required_columns) ? true : false
    rng::AbstractRNG = Random.GLOBAL_RNG
    log = LoggerParams(dir = "log/actor_critic", period = 500)
    device = device(Ï€)
    i::Int64 = 0
end

# REINFORCE loss
reinforce() = (Ï€, ð’Ÿ; info = Dict()) -> reinforce(Ï€, ð’Ÿ[:s], ð’Ÿ[:a], ð’Ÿ[:return], ð’Ÿ[:logprob], info)
function reinforce(Ï€, s, a, G, old_probs, info = Dict())
    new_probs = logpdf(Ï€, s, a)
    
    ignore() do
        info[:entropy] = mean(entropy(Ï€, s))
        info[:kl] = mean(old_probs .- new_probs)
    end 
    
    -mean(new_probs .* G)
end

# A2C Loss
a2c(;Î»â‚š::Float32 = 1f0, Î»áµ¥::Float32 = 1f0, Î»â‚‘::Float32 = 0.1f0) = (Ï€, ð’Ÿ; info = Dict()) -> a2c(Ï€, ð’Ÿ[:s], ð’Ÿ[:a], ð’Ÿ[:advantage], ð’Ÿ[:return], ð’Ÿ[:logprob], Î»â‚š, Î»áµ¥, Î»â‚‘, info)

function a2c(Ï€, s, a, A, G, old_probs, Î»â‚š, Î»áµ¥, Î»â‚‘, info = Dict())
    new_probs = logpdf(Ï€, s, a)
    p_loss = -mean(new_probs .* A)
    # v_loss = mean((value(Ï€, s) .- G).^2)
    e_loss = -mean(entropy(Ï€, s))
    
    # Log useful information
    ignore() do
        info[:entropy] = -e_loss
        info[:kl] = mean(old_probs .- new_probs)
    end 
    
    Î»â‚š*p_loss + Î»â‚‘*e_loss #+ Î»áµ¥*v_loss
end

# PPO Loss
ppo(;Ïµ::Float32 = 0.2f0, Î»â‚š::Float32 = 1f0, Î»áµ¥::Float32 = 1f0, Î»â‚‘::Float32 = 0.1f0) = (Ï€, ð’Ÿ; info = Dict()) -> ppo(Ï€, ð’Ÿ[:s], ð’Ÿ[:a], ð’Ÿ[:advantage], ð’Ÿ[:return], ð’Ÿ[:logprob], Ïµ, Î»â‚š, Î»áµ¥, Î»â‚‘, info)

function ppo(Ï€, s, a, A, G, old_probs, Ïµ, Î»â‚š, Î»áµ¥, Î»â‚‘, info = Dict())
    new_probs = logpdf(Ï€, s, a) 
    r = exp.(new_probs .- old_probs)

    p_loss = -mean(min.(r .* A, clamp.(r, (1f0 - Ïµ), (1f0 + Ïµ)) .* A))
    # v_loss = mean((value(Ï€, s) .- G).^2)
    e_loss = -mean(entropy(Ï€, s))
    
    # Log useful information
    ignore() do
        info[:entropy] = -e_loss
        info[:kl] = mean(old_probs .- new_probs)
        info[:clip_fraction] = sum((r .> 1 + Ïµ) .| (r .< 1 - Ïµ)) / length(r)
    end 
    Î»â‚š*p_loss + Î»â‚‘*e_loss # + Î»áµ¥*v_loss
end

function POMDPs.solve(ð’®::PGSolver, mdp)
    # Construct the experience buffer and sampler
    ð’Ÿ = ExperienceBuffer(ð’®.S, ð’®.A, ð’®.Î”N, ð’®.required_columns, device = ð’®.device)
    Î³, Î» = Float32(discount(mdp)), ð’®.Î»_gae
    s = Sampler(mdp, ð’®.Ï€, ð’®.S, ð’®.A, required_columns = ð’®.required_columns, Î» = ð’®.Î»_gae, max_steps = ð’®.max_steps, rng = ð’®.rng)
    
    # Log the pre-train performance
    ð’®.i == 0 && log(ð’®.log, ð’®.i, log_undiscounted_return(s, Neps = ð’®.eval_eps))
    
    for ð’®.i = range(ð’®.i, stop = ð’®.i + ð’®.N - ð’®.Î”N, step = ð’®.Î”N)
        # Sample transitions
        push!(ð’Ÿ, steps!(s, Nsteps = ð’®.Î”N, reset = true))
        
        # Normalize the advantage
        ð’®.normalize_advantage && (ð’Ÿ[:advantage] .= whiten(ð’Ÿ[:advantage]))
        
        # Train the policy (using batches)
        info = train!(ð’®.Ï€, ð’®.loss, ð’®.batch_size, ð’®.opt, ð’Ÿ, 
                        epochs = ð’®.epochs, 
                        rng = ð’®.rng, 
                        regularizer = ð’®.regularizer, 
                        early_stopping = ð’®.early_stopping,
                        loss_sym = :policy_loss,
                        grad_sym = :policy_grad_norm)
        
        # Train the value function (if actor critic)
        if ð’®.Ï€ isa ActorCritic
            info_v = train!(ð’®.Ï€, ð’®.loss_v, ð’®.batch_size_v, ð’®.opt, ð’Ÿ, 
                            epochs = ð’®.epochs_v,
                            rng = ð’®.rng, 
                            regularizer = ð’®.regularizer,
                            loss_sym = :value_loss, 
                            grad_sym = :value_grad_norm)
            merge!(info, info_v)
        end
        
        # Log the results
        log(ð’®.log, ð’®.i + 1:ð’®.i + ð’®.Î”N, log_undiscounted_return(s, Neps = ð’®.eval_eps), info)
    end
    ð’®.i += ð’®.Î”N
    ð’®.Ï€
end

