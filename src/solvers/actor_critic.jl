@with_kw mutable struct PGSolver <: Solver 
    Ï€::Policy
    S::AbstractSpace
    A::AbstractSpace = action_space(Ï€)
    loss
    N::Int64 = 1000
    Î”N::Int = 200
    Î»_gae::Float32 = 0.95
    batch_size::Int = 64
    epochs::Int = 10
    max_steps::Int64 = 100
    eval_eps::Int = 10
    opt = ADAM(3e-4)
    required_columns = Ï€ isa ActorCritic ? [:return, :advantage, :logprob] : [:return]
    rng::AbstractRNG = Random.GLOBAL_RNG
    log = LoggerParams(dir = "log/actor_critic", period = 500)
    device = device(Ï€)
    i::Int64 = 0
end

# REINFORCE loss
reinforce() = (Ï€, ğ’Ÿ) -> reinforce(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a], ğ’Ÿ[:return])
reinforce(Ï€, s, a, G) = -mean(logpdf(Ï€, s, a) .* G)

# A2C Loss
a2c(;Î»â‚š::Float32 = 1f0, Î»áµ¥::Float32 = 1f0, Î»â‚‘::Float32 = 0.1f0) = (Ï€, ğ’Ÿ) -> a2c(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a], ğ’Ÿ[:advantage], ğ’Ÿ[:return], Î»â‚š, Î»áµ¥, Î»â‚‘)

function a2c(Ï€, s, a, A, G, Î»â‚š, Î»áµ¥, Î»â‚‘)
        p_loss = -mean(logpdf(Ï€, s, a) .* A)
        v_loss = mean((value(Ï€, s) .- G).^2)
        e_loss = -mean(entropy(Ï€, s))
        
        Î»â‚š*p_loss + Î»áµ¥*v_loss + Î»â‚‘*e_loss
end

# PPO Loss
ppo(;Ïµ::Float32 = 0.2f0, Î»â‚š::Float32 = 1f0, Î»áµ¥::Float32 = 1f0, Î»â‚‘::Float32 = 0.1f0) = (Ï€, ğ’Ÿ) -> ppo(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a], ğ’Ÿ[:advantage], ğ’Ÿ[:return], ğ’Ÿ[:logprob], Ïµ, Î»â‚š, Î»áµ¥, Î»â‚‘)

function ppo(Ï€, s, a, A, G, old_probs, Ïµ, Î»â‚š, Î»áµ¥, Î»â‚‘)
        r = exp.(logpdf(Ï€, s, a) .- old_probs)

        p_loss = -mean(min.(r .* A, clamp.(r, (1f0 - Ïµ), (1f0 + Ïµ)) .* A))
        v_loss = mean((value(Ï€, s) .- G).^2)
        e_loss = -mean(entropy(Ï€, s))

        Î»â‚š*p_loss + Î»áµ¥*v_loss + Î»â‚‘*e_loss
end

function POMDPs.solve(ğ’®::PGSolver, mdp)
    # Construct the experience buffer and sampler
    ğ’Ÿ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.Î”N, ğ’®.required_columns, device = ğ’®.device)
    Î³, Î» = Float32(discount(mdp)), ğ’®.Î»_gae
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.S, ğ’®.A, required_columns = ğ’®.required_columns, Î» = ğ’®.Î»_gae, max_steps = ğ’®.max_steps, rng = ğ’®.rng)
    
    # Log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, log_undiscounted_return(s, Neps = ğ’®.eval_eps))
    
    for ğ’®.i = range(ğ’®.i, stop = ğ’®.i + ğ’®.N - ğ’®.Î”N, step = ğ’®.Î”N)
        # Sample transitions
        push!(ğ’Ÿ, steps!(s, Nsteps = ğ’®.Î”N, reset = true))
        
        # Train the policy (using batches)
        losses, grads = train!(ğ’®.Ï€, (D) -> ğ’®.loss(ğ’®.Ï€, D), ğ’®.batch_size, ğ’®.opt, ğ’Ÿ, epochs = ğ’®.epochs, rng = ğ’®.rng)
        
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, log_undiscounted_return(s, Neps = ğ’®.eval_eps), 
                                        log_loss(losses),
                                        log_gradient(grads))
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.Ï€
end

