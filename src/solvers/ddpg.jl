@with_kw mutable struct DDPGSolver <: Solver
    Ï€::ActorCritic{ContinuousNetwork, ContinuousNetwork}
    S::AbstractSpace # state space
    A::AbstractSpace = action_space(Ï€) # action space
    N::Int = 1000 # number of training iterations
    rng::AbstractRNG = Random.GLOBAL_RNG
    exploration_policy::ExplorationPolicy = GaussianNoiseExplorationPolicy(0.1f0, rng = rng)
    critic_loss::Function = Flux.Losses.mse # critic loss function
    regularizer_actor = (Î¸) -> 0
    regularizer_critic = (Î¸) -> 0
    opt_actor = Flux.Optimiser(ClipNorm(1f0), ADAM(1f-4)) # optimizer for the actor
    opt_critic =Flux.Optimiser(ClipNorm(1f0), ADAM(1f-3)) # optimizer for the critic
    Ï„::Float32 = 0.005f0 # polyak averaging parameters used when updating target networks
    batch_size::Int = 100
    max_steps::Int = 100
    eval_eps::Int = 10
    Î”N::Int = 4
    epochs::Int = Î”N
    buffer_size = 1000
    buffer::ExperienceBuffer = ExperienceBuffer(S, A, buffer_size)
    buffer_init::Int = max(batch_size, 200)
    log::Union{Nothing, LoggerParams} = LoggerParams(dir="log/ddpg", period=500)
    Ï€â»::ActorCritic{ContinuousNetwork, ContinuousNetwork} = deepcopy(Ï€) # Target network
    device = device(Ï€)
    i::Int = 0
end

DDPG_target(Ï€, ğ’Ÿ, Î³::Float32) = ğ’Ÿ[:r] .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* value(Ï€, ğ’Ÿ[:sp], action(Ï€, ğ’Ÿ[:sp]))

# T. P. Lillicrap, et al., "Continuous control with deep reinforcement learning", ICLR 2016.
function POMDPs.solve(ğ’®::DDPGSolver, mdp)
    # Initialize replay buffer ğ’Ÿ
    ğ’Ÿ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.batch_size, device=ğ’®.device)
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.S, ğ’®.A, max_steps=ğ’®.max_steps, exploration_policy=ğ’®.exploration_policy)

    # Logging: log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, log_undiscounted_return(s, Neps=ğ’®.eval_eps))

    # Fill the buffer as needed
    ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i=ğ’®.i, explore=true)
    
    # for t = 1, T do
    for ğ’®.i in range(ğ’®.i, stop=ğ’®.i + ğ’®.N - ğ’®.Î”N, step=ğ’®.Î”N)
        # Select action aâ‚œ = Î¼(sâ‚œ | Î¸áµ˜) + ğ’©â‚œ according to the current policy and exploration noise
        # Execute action aâ‚œ and observe reward râ‚œ and observe new state sâ‚œâ‚Šâ‚
        # Store transition (sâ‚œ, aâ‚œ, râ‚œ, sâ‚œâ‚Šâ‚) in ğ’Ÿ
        push!(ğ’®.buffer, steps!(s, explore=true, i=ğ’®.i, Nsteps=ğ’®.Î”N))

        infos = []
        for táµ¢ in 1:ğ’®.epochs
            # Sample a random minibatch of ğ‘ transitions (sáµ¢, aáµ¢, ráµ¢, sáµ¢â‚Šâ‚) from ğ’Ÿ
            rand!(ğ’®.rng, ğ’Ÿ, ğ’®.buffer, i=ğ’®.i)

            # Set yáµ¢ = ráµ¢ + Î³Qâ€²(sáµ¢â‚Šâ‚, Î¼â€²(sáµ¢â‚Šâ‚ | Î¸áµ˜â€²) | Î¸á¶œâ€²)
            y = DDPG_target(ğ’®.Ï€â», ğ’Ÿ, Î³)
            

            # Update critic by minimizing the loss: â„’ = 1/ğ‘ Î£áµ¢ (yáµ¢ - Q(sáµ¢, aáµ¢, | Î¸á¶œ))Â²
            info_c = train!(ğ’®.Ï€.C, (;kwargs...) -> td_loss(ğ’®.Ï€, ğ’Ÿ, y, ğ’®.critic_loss; kwargs...), ğ’®.opt_critic, 
                            loss_sym = :critic_loss, 
                            grad_sym = :critic_grad_norm, 
                            regularizer = ğ’®.regularizer_critic)
                            
            # Update the actor policy using the sampled policy gradient (using gradient ascent, note minus sign):
            # âˆ‡_Î¸áµ˜ ğ½ â‰ˆ 1/ğ‘ Î£áµ¢ âˆ‡â‚Q(s, a | Î¸á¶œ)|â‚›â‚Œâ‚›áµ¢, â‚â‚Œáµ¤â‚â‚›áµ¢â‚ âˆ‡_Î¸áµ˜ Î¼(s | Î¸áµ˜)|â‚›áµ¢
            info_a = train!(ğ’®.Ï€.A, (;kwargs...) -> -mean(value(ğ’®.Ï€, ğ’Ÿ[:s], action(ğ’®.Ï€, ğ’Ÿ[:s]))), ğ’®.opt_actor, 
                            loss_sym = :actor_loss, 
                            grad_sym = :actor_grad_norm,
                            regularizer = ğ’®.regularizer_actor)
                            
            # Merge the loss information and store it
            push!(infos, merge(info_a, info_c))

            # Update the target networks:
            # Î¸á¶œâ€² âŸµ Ï„Î¸á¶œ + (1 - Ï„)Î¸á¶œâ€²
            # Î¸áµ˜â€² âŸµ Ï„Î¸áµ˜ + (1 - Ï„)Î¸áµ˜â€²
            polyak_average!(ğ’®.Ï€â», ğ’®.Ï€, ğ’®.Ï„)
        end
        

        # Logging: Log results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, log_undiscounted_return(s, Neps=ğ’®.eval_eps),
                                        aggregate_info(infos),
                                        log_exploration(ğ’®.exploration_policy, ğ’®.i))
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.Ï€
end

