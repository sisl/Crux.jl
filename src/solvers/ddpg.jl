@with_kw mutable struct DDPGSolver <: Solver
    Ï€::Policy # behavior policy
    Ï€â€²::Policy # target policy
    S::AbstractSpace # state space
    A::AbstractSpace = action_space(Ï€) # action space
    N::Int = 1000 # number of training iterations
    rng::AbstractRNG = Random.GLOBAL_RNG
    exploration_policy::ExplorationPolicy = GaussianNoiseExplorationPolicy(0.1)
    critic_loss::Function = Flux.Losses.mse # critic loss function
    regularizer = (Î¸) -> 0
    opt_actor = Flux.Optimiser(ClipValue(1f0), ADAM(1f-3)) # optimizer for the actor
    opt_critic = Flux.Optimiser(ClipValue(1f0), ADAM(1f-3)) # optimizer for the critic
    Ï„ = 0.001 # polyak averaging parameters used when updating target networks
    batch_size::Int = 100
    epochs::Int = 1
    max_steps::Int = 100
    eval_eps::Int = 10
    Î”train::Int = 50
    buffer_size = 1000
    buffer::ExperienceBuffer = ExperienceBuffer(S, A, buffer_size)
    buffer_init::Int = max(batch_size, 200)
    log::Union{Nothing, LoggerParams} = LoggerParams(dir="log/ddpg", period=500)
    device = device(Ï€)
    i::Int = 0
end

# T. P. Lillicrap, et al., "Continuous control with deep reinforcement learning", ICLR 2016.
function POMDPs.solve(ğ’®::DDPGSolver, mdp)
    # Initialize replay buffer ğ’Ÿ
    ğ’Ÿ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.batch_size, device=ğ’®.device)
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€.A, ğ’®.S, ğ’®.A, max_steps=ğ’®.max_steps, exploration_policy=ğ’®.exploration_policy)

    # Logging: log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, log_undiscounted_return(s, Neps=ğ’®.eval_eps))

    # Fill the buffer as needed
    ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i=ğ’®.i)

    # for t = 1, T do
    for ğ’®.i in range(ğ’®.i, stop=ğ’®.i + ğ’®.N - ğ’®.Î”train, step=ğ’®.Î”train)
        # Select action aâ‚œ = Î¼(sâ‚œ | Î¸áµ˜) + ğ’©â‚œ according to the current policy and exploration noise
        # Execute action aâ‚œ and observe reward râ‚œ and observe new state sâ‚œâ‚Šâ‚
        # Store transition (sâ‚œ, aâ‚œ, râ‚œ, sâ‚œâ‚Šâ‚) in ğ’Ÿ
        push!(ğ’®.buffer, steps!(s, explore=true, i=ğ’®.i, Nsteps=ğ’®.Î”train))

        local actor_losses
        local actor_grads

        for táµ¢ in 1:ğ’®.Î”train
            # Sample a random minibatch of ğ‘ transitions (sáµ¢, aáµ¢, ráµ¢, sáµ¢â‚Šâ‚) from ğ’Ÿ
            rand!(ğ’®.rng, ğ’Ÿ, ğ’®.buffer, i=ğ’®.i)

            # Set yáµ¢ = ráµ¢ + Î³Qâ€²(sáµ¢â‚Šâ‚, Î¼â€²(sáµ¢â‚Šâ‚ | Î¸áµ˜â€²) | Î¸á¶œâ€²)
            y = target(ğ’®.Ï€â€².A, ğ’®.Ï€â€².C, ğ’Ÿ, Î³)

            # Update critic by minimizing the loss: â„’ = 1/ğ‘ Î£áµ¢ (yáµ¢ - Q(sáµ¢, aáµ¢, | Î¸á¶œ))Â²
            critic_losses, critic_grads = train!(ğ’®.Ï€.C, () -> ğ’®.critic_loss(value(ğ’®.Ï€.C, ğ’Ÿ[:s], ğ’Ÿ[:a]), y, agg=mean), ğ’®.opt_critic)

            # Update the actor policy using the sampled policy gradient (using gradient ascent, note minus sign):
            # âˆ‡_Î¸áµ˜ ğ½ â‰ˆ 1/ğ‘ Î£áµ¢ âˆ‡â‚Q(s, a | Î¸á¶œ)|â‚›â‚Œâ‚›áµ¢, â‚â‚Œáµ¤â‚â‚›áµ¢â‚ âˆ‡_Î¸áµ˜ Î¼(s | Î¸áµ˜)|â‚›áµ¢
            actor_losses, actor_grads = train!(ğ’®.Ï€.A, () -> -mean(value(ğ’®.Ï€.C, ğ’Ÿ[:s], action(ğ’®.Ï€.A, ğ’Ÿ[:s]))), ğ’®.opt_actor)

            # Update the target networks:
            # Î¸á¶œâ€² âŸµ Ï„Î¸á¶œ + (1 - Ï„)Î¸á¶œâ€²
            # Î¸áµ˜â€² âŸµ Ï„Î¸áµ˜ + (1 - Ï„)Î¸áµ˜â€²
            copyto!(ğ’®.Ï€â€².C, ğ’®.Ï€.C, ğ’®.Ï„)
            copyto!(ğ’®.Ï€â€².A, ğ’®.Ï€.A, ğ’®.Ï„)
        end

        # Logging: Log results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”train, log_undiscounted_return(s, Neps=ğ’®.eval_eps),
                                           log_loss(actor_losses),
                                           log_gradient(actor_grads),
                                           log_exploration(ğ’®.exploration_policy, ğ’®.i))
    end
    ğ’®.i += ğ’®.Î”train
    ğ’®.Ï€
end
