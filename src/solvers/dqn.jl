@with_kw mutable struct DQNSolver <: Solver 
    Ï€::DQNPolicy
    S::AbstractSpace
    A::AbstractSpace = action_space(Ï€)
    N::Int = 1000
    rng::AbstractRNG = Random.GLOBAL_RNG
    exploration_policy::ExplorationPolicy = EpsGreedyPolicy(LinearDecaySchedule(start=1., stop=0.1, steps=N/2), rng, Ï€.actions)
    L::Function = Flux.Losses.huber_loss
    regularizer = (Î¸) -> 0
    opt = ADAM(1e-3)
    batch_size::Int = 32
    max_steps::Int = 100
    eval_eps::Int = 10
    Î”train::Int = 4 
    Î”target_update::Int = 2000
    buffer_size = 1000
    buffer::ExperienceBuffer = ExperienceBuffer(S, A, buffer_size)
    buffer_init::Int = max(batch_size, 200)
    log::Union{Nothing, LoggerParams} = LoggerParams(dir = "log/dqn", period = 500)
    device = device(Ï€)
    i::Int = 0
end

function POMDPs.solve(ğ’®::DQNSolver, mdp, extra_buffers...)
    isprioritized = prioritized(ğ’®.buffer)
    required_columns = isprioritized ? [:weight] : Symbol[]
    
    # Initialize minibatch buffer and sampler
    ğ’Ÿ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.batch_size, required_columns, device = ğ’®.device)
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.S, ğ’®.A, required_columns = required_columns, max_steps = ğ’®.max_steps, exploration_policy = ğ’®.exploration_policy, rng = ğ’®.rng)
    
    # Log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, log_undiscounted_return(s, Neps = ğ’®.eval_eps))
    
    # Fill the buffer as needed
    ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i = ğ’®.i)
    
    for ğ’®.i = range(ğ’®.i, stop = ğ’®.i + ğ’®.N - ğ’®.Î”train, step = ğ’®.Î”train)
        # Take Î”train steps in the environment
        push!(ğ’®.buffer, steps!(s, explore = true, i = ğ’®.i, Nsteps = ğ’®.Î”train))
       
        # Sample a minibatch
        rand!(ğ’®.rng, ğ’Ÿ, ğ’®.buffer, extra_buffers..., i = ğ’®.i)
        
        # Compute target, td_error and td_loss for backprop
        y = target(ğ’®.Ï€.Qâ», ğ’Ÿ, Î³)
        isprioritized && update_priorities!(ğ’®.buffer, ğ’Ÿ.indices, cpu(td_error(ğ’®.Ï€, ğ’Ÿ, y)))
        info = train!(ğ’®.Ï€, (;kwargs...) -> td_loss(ğ’®.Ï€, ğ’Ÿ, y, ğ’®.L, isprioritized; kwargs...), ğ’®.opt, regularizer = ğ’®.regularizer)
        
        # Update target network
        elapsed(ğ’®.i + 1:ğ’®.i + ğ’®.Î”train, ğ’®.Î”target_update) && copyto!(ğ’®.Ï€.Qâ», ğ’®.Ï€.Q)
        
        # Log results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”train, log_undiscounted_return(s, Neps = ğ’®.eval_eps), 
                                            info,
                                            log_exploration(ğ’®.exploration_policy, ğ’®.i))
    end
    ğ’®.i += ğ’®.Î”train
    ğ’®.Ï€
end

