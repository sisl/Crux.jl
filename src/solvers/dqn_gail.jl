@with_kw mutable struct DQNGAILSolver <: Solver 
    Ï€::DQNPolicy
    D::DQNPolicy
    sdim::Int
    adim::Int = length(Ï€.actions)
    N::Int = 1000
    buffer::ExperienceBuffer = ExperienceBuffer(sdim, adim, 1000)
    expert_buffer::ExperienceBuffer
    nda_buffer::Union{Nothing, ExperienceBuffer} = nothing
    Î»_nda::Float32 = 0.5f0
    rng::AbstractRNG = Random.GLOBAL_RNG
    exploration_policy::ExplorationPolicy = EpsGreedyPolicy(LinearDecaySchedule(start=1., stop=0.1, steps=N/2), rng, Ï€.actions)
    L::Function = Flux.Losses.huber_loss
    opt = ADAM(1e-3)
    optD = deepcopy(opt)
    batch_size::Int = 32
    max_steps::Int = 100 
    eval_eps::Int = 100
    buffer_init::Int = max(batch_size, 200)
    Î”target_update::Int = 500
    Î”train::Int = 4 
    log = LoggerParams(dir = "log/gail", period = 10)
    device = device(Ï€)
    i::Int64 = 0
end

const LBCE = Flux.Losses.logitbinarycrossentropy

dqngail_target(Q, D, ğ’Ÿ, Î³::Float32) = tanh.(q_predicted(D, ğ’Ÿ)) .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* maximum(Q(ğ’Ÿ[:sp]), dims=1)

function Lá´°(D, ğ’Ÿ_expert::ExperienceBuffer, ğ’Ÿ_Ï€::ExperienceBuffer)
    LBCE(q_predicted(D, ğ’Ÿ_expert), 1.f0) + LBCE(q_predicted(D, ğ’Ÿ_Ï€), 0.f0)
end

function Lá´°_nda(D, ğ’Ÿ_expert::ExperienceBuffer, ğ’Ÿ_Ï€::ExperienceBuffer, ğ’Ÿ_nda::ExperienceBuffer, Î»_nda::Float32)
    LBCE(q_predicted(D, ğ’Ÿ_expert), 1.f0) +  LBCE(q_predicted(D, ğ’Ÿ_Ï€), 0.f0) + Î»_nda*LBCE(q_predicted(D, ğ’Ÿ_nda), 0.f0)
end

function POMDPs.solve(ğ’®::DQNGAILSolver, mdp)
    # Initialize minibatch buffers and sampler
    ğ’Ÿ_Ï€ = ExperienceBuffer(ğ’®.sdim, ğ’®.adim, ğ’®.batch_size, device = ğ’®.device)
    ğ’Ÿ_expert = deepcopy(ğ’Ÿ_Ï€)
    ğ’Ÿ_nda = isnothing(ğ’®.nda_buffer) ? nothing : deepcopy(ğ’Ÿ_Ï€)
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.sdim, ğ’®.adim, max_steps = ğ’®.max_steps, exploration_policy = ğ’®.exploration_policy, rng = ğ’®.rng)
    
    # Log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, log_undiscounted_return(s, Neps = ğ’®.eval_eps))
    
    # Fill the buffer as needed
    ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i = ğ’®.i)
    
    for ğ’®.i = range(ğ’®.i, stop = ğ’®.i + ğ’®.N - ğ’®.Î”train, step = ğ’®.Î”train)
        # Take Î”train steps in the environment
        push!(ğ’®.buffer, steps!(s, i = ğ’®.i, Nsteps = ğ’®.Î”train))
        
        # Sample a minibatch
        rand!(ğ’®.rng, ğ’Ÿ_Ï€, ğ’®.buffer, i = ğ’®.i)
        rand!(ğ’®.rng, ğ’Ÿ_expert, ğ’®.expert_buffer, i = ğ’®.i)
        !isnothing(ğ’®.nda_buffer) && rand!(ğ’®.rng, ğ’Ÿ_nda, ğ’®.nda_buffer, i = ğ’®.i)
        
        # train the discrimnator
        if isnothing(ğ’®.nda_buffer)
            lossD, gradD = train!(ğ’®.D, () -> Lá´°(ğ’®.D, ğ’Ÿ_expert, ğ’Ÿ_Ï€), ğ’®.optD, ğ’®.device)
        else
            lossD, gradD = train!(ğ’®.D, () -> Lá´°_nda(ğ’®.D, ğ’Ÿ_expert, ğ’Ÿ_Ï€, ğ’Ÿ_nda, ğ’®.Î»_nda), ğ’®.optD, ğ’®.device)
        end
        
        # Compute target, update priorities, and train the generator.
        y = dqngail_target(ğ’®.Ï€.Qâ», ğ’®.D.Qâ», ğ’Ÿ_Ï€, Î³)
        prioritized(ğ’®.buffer) && update_priorities!(ğ’®.buffer, ğ’Ÿ_Ï€.indices, td_error(ğ’®.Ï€, ğ’Ÿ_Ï€, y))
        lossG, gradG = train!(ğ’®.Ï€, () -> td_loss(ğ’®.Ï€, ğ’Ÿ_Ï€, y, ğ’®.L), ğ’®.opt, ğ’®.device)
            
        # Update target network
        elapsed(ğ’®.i + 1:ğ’®.i + ğ’®.Î”train, ğ’®.Î”target_update) && begin copyto!(ğ’®.Ï€.Qâ», ğ’®.Ï€.Q); copyto!(ğ’®.D.Qâ», ğ’®.D.Q) end
        
        # Log results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”train, log_undiscounted_return(s, Neps = ğ’®.eval_eps), 
                                            log_loss(lossG, suffix = "G"),
                                            log_loss(lossD, suffix = "D"),
                                            log_gradient(gradG, suffix = "G"),
                                            log_gradient(gradD, suffix = "D"),
                                            log_exploration(ğ’®.exploration_policy, ğ’®.i))
    end
    ğ’®.i += ğ’®.Î”train
    ğ’®.Ï€
end

