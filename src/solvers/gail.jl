@with_kw mutable struct GAILSolver{T} <: Solver
    D
    G::T
    optD = deepcopy(G.opt)
    expert_buffer::ExperienceBuffer
    nda_buffer::Union{Nothing, ExperienceBuffer} = nothing
    Î»_nda::Float32 = 0.5f0
end

## Discriminator stuff
const LBCE = Flux.Losses.logitbinarycrossentropy

function dqn_Lá´°(D, ğ’Ÿ_expert, ğ’Ÿ_Ï€, no_nda::Nothing, Î»_nda::Float32; info = Dict())
    LBCE(value(D, ğ’Ÿ_expert[:s], ğ’Ÿ_expert[:a]), 1.f0) + LBCE(value(D, ğ’Ÿ_Ï€[:s], ğ’Ÿ_Ï€[:a]), 0.f0)
end

function dqn_Lá´°(D, ğ’Ÿ_expert, ğ’Ÿ_Ï€, ğ’Ÿ_nda, Î»_nda::Float32; info = Dict())
    LBCE(value(D, ğ’Ÿ_expert[:s], ğ’Ÿ_expert[:a]), 1.f0) + 
    LBCE(value(D, ğ’Ÿ_Ï€[:s], ğ’Ÿ_Ï€[:a]), 0.f0) + 
    Î»_nda*LBCE(value(D, ğ’Ÿ_nda[:s], ğ’Ÿ_nda[:a]), 0.f0)
end

## DQN-GAIL stuff
dqngail_target(Ï€, D, ğ’Ÿ, Î³::Float32) = tanh.(value(D, ğ’Ÿ[:s], ğ’Ÿ[:a])) .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* maximum(value(Ï€, ğ’Ÿ[:sp]), dims=1)

function POMDPs.solve(ğ’®GAIL::GAILSolver{DQNSolver}, mdp)    
    ğ’® = ğ’®GAIL.G # pull out the main solver
    @assert !(prioritized(ğ’®.buffer)) # not handled
    
    # Initialize minibatch buffers and sampler
    ğ’Ÿ_Ï€ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.batch_size, device = ğ’®.device)
    ğ’Ÿ_expert = deepcopy(ğ’Ÿ_Ï€)
    ğ’Ÿ_nda = isnothing(ğ’®GAIL.nda_buffer) ? nothing : deepcopy(ğ’Ÿ_Ï€)
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.S, max_steps = ğ’®.max_steps, exploration_policy = ğ’®.exploration_policy, rng = ğ’®.rng)
    
    # Log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, log_undiscounted_return(s, Neps = ğ’®.eval_eps))
    
    # Fill the buffer as needed
    ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i = ğ’®.i, explore = true)
    
    for ğ’®.i = range(ğ’®.i, stop = ğ’®.i + ğ’®.N - ğ’®.Î”N, step = ğ’®.Î”N)
        # Take Î”N steps in the environment
        push!(ğ’®.buffer, steps!(s, explore = true, i = ğ’®.i, Nsteps = ğ’®.Î”N))
        
        infos = []
        for _ in 1:ğ’®.epochs
            # Sample a minibatch
            rand!(ğ’®.rng, ğ’Ÿ_Ï€, ğ’®.buffer, i = ğ’®.i)
            rand!(ğ’®.rng, ğ’Ÿ_expert, ğ’®GAIL.expert_buffer, i = ğ’®.i)
            !isnothing(ğ’®GAIL.nda_buffer) && rand!(ğ’®.rng, ğ’Ÿ_nda, ğ’®GAIL.nda_buffer, i = ğ’®.i)
            
            # Train the discriminator
            info_D = train!(ğ’®GAIL.D, 
                            (;kwargs...) -> dqn_Lá´°(ğ’®GAIL.D, ğ’Ÿ_expert, ğ’Ÿ_Ï€, ğ’Ÿ_nda, ğ’®GAIL.Î»_nda; kwargs...), 
                            ğ’®GAIL.optD, 
                            loss_sym = :loss_D, 
                            grad_sym = :grad_norm_D)
            
            # Compute target and train the generato
            y = dqngail_target(ğ’®.Ï€â», ğ’®GAIL.D, ğ’Ÿ_Ï€, Î³)
            info_G = train!(ğ’®.Ï€, 
                            (;kwargs...) -> td_loss(ğ’®.Ï€, ğ’Ÿ_Ï€, y, ğ’®.loss; kwargs...), 
                            ğ’®.opt, loss_sym = :loss_G, 
                            grad_sym = :grad_norm_G)
            
            push!(infos, merge(info_D, info_G))
        end
        # Update target network
        elapsed(ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, ğ’®.Î”target_update) && copyto!(ğ’®.Ï€â», ğ’®.Ï€)
        
        # Log results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, log_undiscounted_return(s, Neps = ğ’®.eval_eps), 
                                            aggregate_info(infos),
                                            log_exploration(ğ’®.exploration_policy, ğ’®.i))
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.Ï€
end

## PG-GAIL stuff
function pg_Lá´°(D, ğ’Ÿ_expert, ğ’Ÿ_Ï€; info = Dict())
    LBCE(value(D, vcat(ğ’Ÿ_expert[:s], ğ’Ÿ_expert[:a])), 1.f0) + LBCE(value(D, vcat(ğ’Ÿ_Ï€[:s], ğ’Ÿ_Ï€[:a])), 0.f0)
end

# function pg_Lá´°_nda(D, ğ’Ÿ_expert, ğ’Ÿ_Ï€, ğ’Ÿ_nda, Î»_nda::Float32)
#     LBCE(q_predicted(D, ğ’Ÿ_expert), 1.f0) +  LBCE(q_predicted(D, ğ’Ÿ_Ï€), 0.f0) + Î»_nda*LBCE(q_predicted(D, ğ’Ÿ_nda), 0.f0)
# end

function POMDPs.solve(ğ’®GAIL::GAILSolver{PGSolver}, mdp)
    ğ’® = ğ’®GAIL.G # pull out the main solver
    
    # Construct the experience buffer and sampler
    ğ’Ÿ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.Î”N, ğ’®.required_columns, device = ğ’®.device)
    Î³, Î» = Float32(discount(mdp)), ğ’®.Î»_gae
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.S, required_columns = ğ’®.required_columns, Î» = ğ’®.Î»_gae, max_steps = ğ’®.max_steps, rng = ğ’®.rng, exploration_policy = ğ’®.Ï€)
    
    # Log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, log_undiscounted_return(s, Neps = ğ’®.eval_eps))
    
    for ğ’®.i = range(ğ’®.i, stop = ğ’®.i + ğ’®.N - ğ’®.Î”N, step = ğ’®.Î”N)
        # Sample transitions
        push!(ğ’Ÿ, steps!(s, Nsteps = ğ’®.Î”N, reset = true, explore = true))
        
        # Train the discriminator (using batches)
        if isnothing(ğ’®GAIL.nda_buffer)
            info_D = train!(ğ’®GAIL.D, pg_Lá´°, ğ’®.batch_size, ğ’®GAIL.optD, 
                                  ğ’®GAIL.expert_buffer, ğ’Ÿ,
                                  epochs = ğ’®.epochs, rng = ğ’®.rng,
                                  loss_sym = :loss_D, grad_sym = :grad_norm_D)
        else
            #TODO
            error("not implemented")
        end
        
        ğ’Ÿ[:advantage] .= value(ğ’®GAIL.D, vcat(ğ’Ÿ[:s], ğ’Ÿ[:a]))
        
        # Normalize the advantage
        ğ’®.normalize_advantage && (ğ’Ÿ[:advantage] .= whiten(ğ’Ÿ[:advantage]))
        
        # Train the policy (using batches)
        info_G = train!(ğ’®.Ï€, ğ’®.loss, ğ’®.batch_size, ğ’®.opt, ğ’Ÿ, 
                        epochs = ğ’®.epochs, 
                        rng = ğ’®.rng, 
                        regularizer = ğ’®.regularizer, 
                        early_stopping = ğ’®.early_stopping,
                        loss_sym = :policy_loss_G,
                        grad_sym = :policy_grad_norm_G)
        
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, log_undiscounted_return(s, Neps = ğ’®.eval_eps), 
                                        info_D, 
                                        info_G)
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.Ï€
end


