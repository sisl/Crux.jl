@with_kw mutable struct GAILSolver{T} <: Solver
    D
    G::T
    optD = deepcopy(G.opt)
    expert_buffer::ExperienceBuffer
    nda_buffer::Union{Nothing, ExperienceBuffer} = nothing
    Î»_nda::Float32 = 0.5f0
end

## Discriminator stuff
const LBCE = Flux.Losses.logitbinarycrossentropy

function dqn_Lá´°(D, ğ’Ÿ_expert, ğ’Ÿ_Ï€)
    LBCE(q_predicted(D, ğ’Ÿ_expert), 1.f0) + LBCE(q_predicted(D, ğ’Ÿ_Ï€), 0.f0)
end

function dqn_Lá´°_nda(D, ğ’Ÿ_expert, ğ’Ÿ_Ï€, ğ’Ÿ_nda, Î»_nda::Float32)
    LBCE(q_predicted(D, ğ’Ÿ_expert), 1.f0) +  LBCE(q_predicted(D, ğ’Ÿ_Ï€), 0.f0) + Î»_nda*LBCE(q_predicted(D, ğ’Ÿ_nda), 0.f0)
end

## DQN-GAIL stuff
dqngail_target(Q, D, ğ’Ÿ, Î³::Float32) = tanh.(q_predicted(D, ğ’Ÿ)) .+ Î³ .* (1.f0 .- ğ’Ÿ[:done]) .* maximum(Q(ğ’Ÿ[:sp]), dims=1)

function POMDPs.solve(ğ’®GAIL::GAILSolver{DQNSolver}, mdp)
    ğ’® = ğ’®GAIL.G # pull out the main solver
    
    # Initialize minibatch buffers and sampler
    ğ’Ÿ_Ï€ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.batch_size, device = ğ’®.device)
    ğ’Ÿ_expert = deepcopy(ğ’Ÿ_Ï€)
    ğ’Ÿ_nda = isnothing(ğ’®GAIL.nda_buffer) ? nothing : deepcopy(ğ’Ÿ_Ï€)
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.S, ğ’®.A, max_steps = ğ’®.max_steps, exploration_policy = ğ’®.exploration_policy, rng = ğ’®.rng)
    
    # Log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, log_undiscounted_return(s, Neps = ğ’®.eval_eps))
    
    # Fill the buffer as needed
    ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i = ğ’®.i)
    
    for ğ’®.i = range(ğ’®.i, stop = ğ’®.i + ğ’®.N - ğ’®.Î”train, step = ğ’®.Î”train)
        # Take Î”train steps in the environment
        push!(ğ’®.buffer, steps!(s, i = ğ’®.i, Nsteps = ğ’®.Î”train))
        
        # Sample a minibatch
        rand!(ğ’®.rng, ğ’Ÿ_Ï€, ğ’®.buffer, i = ğ’®.i)
        rand!(ğ’®.rng, ğ’Ÿ_expert, ğ’®GAIL.expert_buffer, i = ğ’®.i)
        !isnothing(ğ’®GAIL.nda_buffer) && rand!(ğ’®.rng, ğ’Ÿ_nda, ğ’®GAIL.nda_buffer, i = ğ’®.i)
        
        # train the discrimnator
        if isnothing(ğ’®GAIL.nda_buffer)
            lossD, gradD = train!(ğ’®GAIL.D, () -> dqn_Lá´°(ğ’®GAIL.D, ğ’Ÿ_expert, ğ’Ÿ_Ï€), ğ’®GAIL.optD)
        else
            lossD, gradD = train!(ğ’®GAIL.D, () -> dqn_Lá´°_nda(ğ’®GAIL.D, ğ’Ÿ_expert, ğ’Ÿ_Ï€, ğ’Ÿ_nda, ğ’®GAIL.Î»_nda), ğ’®GAIL.optD)
        end
        
        # Compute target, update priorities, and train the generator.
        y = dqngail_target(ğ’®.Ï€.Qâ», ğ’®GAIL.D, ğ’Ÿ_Ï€, Î³)
        prioritized(ğ’®.buffer) && update_priorities!(ğ’®.buffer, ğ’Ÿ_Ï€.indices, td_error(ğ’®.Ï€, ğ’Ÿ_Ï€, y))
        lossG, gradG = train!(ğ’®.Ï€, () -> td_loss(ğ’®.Ï€, ğ’Ÿ_Ï€, y, ğ’®.L), ğ’®.opt)
            
        # Update target network
        elapsed(ğ’®.i + 1:ğ’®.i + ğ’®.Î”train, ğ’®.Î”target_update) && copyto!(ğ’®.Ï€.Qâ», ğ’®.Ï€.Q)
        
        # Log results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”train, log_undiscounted_return(s, Neps = ğ’®.eval_eps), 
                                            log_loss(lossG, suffix = "G"),
                                            log_loss(lossD, suffix = "D"),
                                            log_gradient(gradG, suffix = "G"),
                                            log_gradient(gradD, suffix = "D"),
                                            log_exploration(ğ’®.exploration_policy, ğ’®.i))
    end
    ğ’®.i += ğ’®.Î”train
    ğ’®.Ï€
end

## PG-GAIL stuff
function pg_Lá´°(D, ğ’Ÿ_expert, ğ’Ÿ_Ï€)
    LBCE(value(D, vcat(ğ’Ÿ_expert[:s], ğ’Ÿ_expert[:a])), 1.f0) + LBCE(value(D, vcat(ğ’Ÿ_Ï€[:s], ğ’Ÿ_Ï€[:a])), 0.f0)
end

# function pg_Lá´°_nda(D, ğ’Ÿ_expert, ğ’Ÿ_Ï€, ğ’Ÿ_nda, Î»_nda::Float32)
#     LBCE(q_predicted(D, ğ’Ÿ_expert), 1.f0) +  LBCE(q_predicted(D, ğ’Ÿ_Ï€), 0.f0) + Î»_nda*LBCE(q_predicted(D, ğ’Ÿ_nda), 0.f0)
# end

function POMDPs.solve(ğ’®GAIL::GAILSolver{PGSolver}, mdp)
    ğ’® = ğ’®GAIL.G # pull out the main solver
    
    # Construct the experience buffer and sampler
    ğ’Ÿ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.Î”N, ğ’®.required_columns, device = ğ’®.device)
    Î³, Î» = Float32(discount(mdp)), ğ’®.Î»_gae
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.S, ğ’®.A, required_columns = ğ’®.required_columns, Î» = ğ’®.Î»_gae, max_steps = ğ’®.max_steps, rng = ğ’®.rng)
    
    # Log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, log_undiscounted_return(s, Neps = ğ’®.eval_eps))
    
    for ğ’®.i = range(ğ’®.i, stop = ğ’®.i + ğ’®.N - ğ’®.Î”N, step = ğ’®.Î”N)
        # Sample transitions
        push!(ğ’Ÿ, steps!(s, Nsteps = ğ’®.Î”N, reset = true))
        
        # Train the discriminator (using batches)
        if isnothing(ğ’®GAIL.nda_buffer)
            lossD, gradD = train!(ğ’®GAIL.D, 
                                  (Dexp, DÏ€) -> pg_Lá´°(ğ’®GAIL.D, Dexp, DÏ€), 
                                  ğ’®.batch_size, ğ’®GAIL.optD, 
                                  ğ’®GAIL.expert_buffer, ğ’Ÿ,
                                  epochs = ğ’®.epochs, rng = ğ’®.rng)
        else
            error("not implemented")
            # lossD, gradD = train!(ğ’®GAIL.D, 
                                  # (Dexp, DÏ€, Dnda) -> Lá´°_nda(ğ’®GAIL.D, Dexp, DÏ€, Dnda, ğ’®GAIL.Î»_nda), 
                                  # ğ’®.batch_size, ğ’®GAIL.optD, 
                                  # ğ’®GAIL.expert_buffer, ğ’Ÿ, ğ’®GAIL.nda_buffer, 
                                  # epochs = ğ’®.epochs, rng = ğ’®.rng)
        end
        
        ğ’Ÿ[:advantage] .= value(ğ’®GAIL.D, vcat(ğ’Ÿ[:s], ğ’Ÿ[:a]))
            
        
        # Train the policy (using batches)
        losses, grads = train!(ğ’®.Ï€, (D) -> ğ’®.loss(ğ’®.Ï€, D), ğ’®.batch_size, ğ’®.opt, ğ’Ÿ, epochs = ğ’®.epochs, rng = ğ’®.rng)
        
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, log_undiscounted_return(s, Neps = ğ’®.eval_eps), 
                                        log_loss(losses, suffix = "G"),
                                        log_gradient(grads, suffix = "G"),
                                        log_loss(lossD, suffix = "D"),
                                        log_gradient(gradD, suffix = "D"),)
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.Ï€
end


