@with_kw mutable struct VPGSolver <: Solver 
    Ï€::Policy
    S::AbstractSpace
    A::AbstractSpace = action_space(Ï€)
    baseline::Baseline
    N::Int64 = 1000
    Î”N::Int = 1000
    batch_size::Int = 32
    max_steps::Int64 = 100
    eval_eps::Int = 100
    opt = ADAM(1e-3)
    rng::AbstractRNG = Random.GLOBAL_RNG
    log = LoggerParams(dir = "log/vpg", period = 500)
    device = device(Ï€)
    i::Int64 = 0
end

vpg_loss(Ï€, ğ’Ÿ) = -mean(logpdf(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a]) .* ğ’Ÿ[:advantage])

function POMDPs.solve(ğ’®::VPGSolver, mdp)
    # Construct the experience buffer and sampler
    ğ’Ÿ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.Î”N, device = ğ’®.device, gae = true)
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.S, ğ’®.A, max_steps = ğ’®.max_steps, rng = ğ’®.rng)
    
    # Log the pre-train performance
    ğ’®.i == 0 && log(ğ’®.log, ğ’®.i, log_undiscounted_return(s, Neps = ğ’®.eval_eps))
    
    for ğ’®.i = range(ğ’®.i, stop = ğ’®.i + ğ’®.N - ğ’®.Î”N, step = ğ’®.Î”N)
        # Sample transitions
        push!(ğ’Ÿ, steps!(s, Nsteps = ğ’®.Î”N, baseline = ğ’®.baseline, Î³ = Î³, reset = true))
        
        # Train the baseline
        train!(ğ’®.baseline, ğ’Ÿ)
        
        # Train the policy (using batches)
        losses, grads = train!(ğ’®.Ï€, (D) -> vpg_loss(ğ’®.Ï€, D), ğ’Ÿ, ğ’®.batch_size, ğ’®.opt, ğ’®.device, rng = ğ’®.rng)
        
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, log_undiscounted_return(s, Neps = ğ’®.eval_eps), 
                                        log_loss(losses),
                                        log_gradient(grads))
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.Ï€
end

