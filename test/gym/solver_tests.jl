using Conda
Conda.add("gymnasium")
Conda.add("pygame")

using POMDPs
using POMDPGym
import POMDPModels
using Test
using Crux
using Flux
using Random
using BSON

function test_solver(ğ’®fn, mdp, Ï€...)
    # run it once
    Random.seed!(0)
    S1 = ğ’®fn(deepcopy.(Ï€)...)
    Ï€1 = solve(S1, deepcopy(mdp))

    # run it again on the cpu with the same rng
    Random.seed!(0)
    S2 = ğ’®fn(deepcopy.(Ï€)...)
    Ï€2 = solve(S2, deepcopy(mdp))

    # Run it on the gpu
    Random.seed!(0)
    S3 = ğ’®fn(gpu.(deepcopy.(Ï€))...)
    Ï€3 = solve(S3, deepcopy(mdp))

    # compare the results
    s = rand(Crux.dim(state_space(mdp))...)
    try
        value(Ï€[1], s)
        @test all(value(Ï€1, s) .â‰ˆ value(Ï€2, s))
        @test all(abs.(value(Ï€2, s) .- value(Ï€3, s)) .< 1e-2)
    catch
        if action(Ï€1, s)[1] isa Symbol
            @test all(action(Ï€1, s) .== action(Ï€2, s))
            @test all(action(Ï€2, s) .== action(Ï€3, s))
        else
            @test isapprox(action(Ï€1, s), action(Ï€2, s), atol=1e-1)
            @test isapprox(action(Ï€2, s), action(Ï€3, s), atol=1e-1)
            # @test all(abs.(action(Ï€2, s) .- action(Ï€3, s)) .< 1e-2)
        end
    end
end

## Training params
N = 100
Î”N = 50

## discrete RL
discrete_mdp = GridWorldMDP()
S = state_space(discrete_mdp)
A() = DiscreteNetwork(Chain(Dense(2, 32, relu), Dense(32, 4)), actions(discrete_mdp))
V() = ContinuousNetwork(Chain(Dense(2, 32, relu), Dense(32, 1)))
AC() = ActorCritic(A(), V())

test_solver((Ï€) -> REINFORCE(Ï€=Ï€, S=S, N=N, Î”N=Î”N), discrete_mdp, A())
test_solver((Ï€) -> A2C(Ï€=Ï€, S=S, N=N, Î”N=Î”N), discrete_mdp, AC())
test_solver((Ï€) -> PPO(Ï€=Ï€, S=S, N=N, Î”N=Î”N), discrete_mdp, AC())
test_solver((Ï€) -> DQN(Ï€=Ï€, S=S, N=N), discrete_mdp, A())
test_solver((Ï€) -> DQN(Ï€=Ï€, S=S, N=N, target_fn=Crux.dqn_target), discrete_mdp, A())

# test compatibility with non-POMDPGym MDPs
test_solver((Ï€) -> PPO(Ï€=Ï€, S=S, N=N, Î”N=Î”N), POMDPModels.SimpleGridWorld(), AC())

## Continuous RL
continuous_mdp = PendulumPOMDP()
S = state_space(continuous_mdp)
QSA() = ContinuousNetwork(Chain(Dense(3, 32, tanh), Dense(32, 1)))
V() = ContinuousNetwork(Chain( Dense(2, 32, relu), Dense(32, 1)))
A() = ContinuousNetwork(Chain(Dense(2, 32, relu), Dense(32, 1, tanh)), 1)
G() = GaussianPolicy(A(), zeros(Float32, 1))

test_solver((Ï€) -> REINFORCE(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, G())
test_solver((Ï€) -> A2C(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(G(), V()))
test_solver((Ï€) -> PPO(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(G(), V()))
test_solver((Ï€) -> DDPG(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(A(), QSA()))
test_solver((Ï€) -> TD3(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(A(), DoubleNetwork(QSA(), QSA())))
test_solver((Ï€) -> SAC(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(G(), DoubleNetwork(QSA(), QSA())))


# Continuous IL
Î³ = 0.95f0
exp_data_path = abspath(joinpath(@__DIR__, "..", "..", "examples", "il", "expert_data", "pendulum.bson"))
ğ’Ÿ_demo = expert_trajectories = BSON.load(exp_data_path)[:data]
D(output=1) = ContinuousNetwork(Chain(DenseSN(3, 12, relu), DenseSN(12, output)))

test_solver((Ï€, D) -> OnPolicyGAIL(D=D, ğ’Ÿ_demo=ğ’Ÿ_demo, Ï€=Ï€, S=S, N=N, Î”N=Î”N, Î³=Î³), continuous_mdp, ActorCritic(G(), V()), D())
test_solver((Ï€, D) -> OffPolicyGAIL(D=D, ğ’Ÿ_demo=ğ’Ÿ_demo, Ï€=Ï€, S=S, N=50, Î”N=Î”N), continuous_mdp, ActorCritic(G(), DoubleNetwork(QSA(), QSA())), D(2))
test_solver(Ï€ -> BC(Ï€=Ï€, ğ’Ÿ_demo=ğ’Ÿ_demo, S=S, opt=(epochs=1,), log=(period=50,)), continuous_mdp, A())
# NOTE: gradient penalty on the gpu only plays nicely with tanh, not relus in the discriminator?
test_solver((Ï€) -> AdVIL(ğ’Ÿ_demo=ğ’Ÿ_demo, Ï€=Ï€, S=S, a_opt=(epochs=1,), log=(period=50,)), continuous_mdp, ActorCritic(A(), QSA()))
test_solver((Ï€) -> SQIL(ğ’Ÿ_demo=ğ’Ÿ_demo, Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(G(), DoubleNetwork(QSA(), QSA())))
test_solver((Ï€) -> AdRIL(ğ’Ÿ_demo=ğ’Ÿ_demo, Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(G(), DoubleNetwork(QSA(), QSA())))
test_solver((Ï€) -> ASAF(ğ’Ÿ_demo=ğ’Ÿ_demo, Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, G())


# Batch RL
test_solver(Ï€ -> BatchSAC(Ï€=Ï€, ğ’Ÿ_train=ğ’Ÿ_demo, S=S, a_opt=(epochs=1,)), continuous_mdp, ActorCritic(G(), DoubleNetwork(QSA(), QSA())))
test_solver(Ï€ -> CQL(Ï€=Ï€, ğ’Ÿ_train=ğ’Ÿ_demo, S=S, a_opt=(epochs=1,)), continuous_mdp, ActorCritic(G(), DoubleNetwork(QSA(), QSA())))
