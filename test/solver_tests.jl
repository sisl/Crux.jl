using POMDPs, POMDPGym, Test, Crux, Flux, Random, BSON
function test_solver(ğ’®fn, mdp, Ï€...)
    # run it once
    Random.seed!(0)
    Ï€1 = solve(ğ’®fn(deepcopy.(Ï€)...), deepcopy(mdp))
    
    # run it again on the cpu with the same rng
    Random.seed!(0)
    Ï€2 = solve(ğ’®fn(deepcopy.(Ï€)...), deepcopy(mdp))
    
    # Run it on the gpu
    Random.seed!(0)
    Ï€3 = solve(ğ’®fn(gpu.(deepcopy.(Ï€))...), deepcopy(mdp))
    
    # compare the results
    s = rand(Crux.dim(state_space(mdp))...)
    try
        value(Ï€[1], s)
        @test all(value(Ï€1, s) .â‰ˆ value(Ï€2, s))
        @test all(abs.(value(Ï€2, s) .- value(Ï€3, s)) .< 1e-3)
    catch
        @test all(action(Ï€1, s) .â‰ˆ action(Ï€2, s))
        @test all(abs.(action(Ï€2, s) .- action(Ï€3, s)) .< 1e-3)
    end
end

## Training params
N = 100
Î”N = 50

## discrete RL
discrete_mdp = GridWorldMDP()
S = state_space(discrete_mdp)
A() = DiscreteNetwork(Chain(Dense(2, 32, relu), Dense(32, 4)), actions(discrete_mdp))
V() = ContinuousNetwork(Chain(Dense(2, 32, relu), Dense(32, 1)))
AC() = ActorCritic(A(), V())

test_solver((Ï€) -> REINFORCE(Ï€=Ï€, S=S, N=N, Î”N=Î”N), discrete_mdp, A())
test_solver((Ï€) -> A2C(Ï€=Ï€, S=S, N=N, Î”N=Î”N), discrete_mdp, AC())
test_solver((Ï€) -> PPO(Ï€=Ï€, S=S, N=N, Î”N=Î”N), discrete_mdp, AC())
test_solver((Ï€) -> DQN(Ï€=Ï€, S=S, N=N), discrete_mdp, A())


## Continuous RL 
continuous_mdp = PendulumMDP()
S = state_space(continuous_mdp)
QSA() = ContinuousNetwork(Chain(Dense(3, 32, tanh), Dense(32, 1)))
V() = ContinuousNetwork(Chain( Dense(2, 32, relu), Dense(32, 1)))
A() = ContinuousNetwork(Chain(Dense(2, 32, relu), Dense(32, 1, tanh)), 1)
G() = GaussianPolicy(A(), zeros(Float32, 1))

test_solver((Ï€) -> REINFORCE(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, G())
test_solver((Ï€) -> A2C(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(G(), V()))
test_solver((Ï€) -> PPO(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(G(), V()))
test_solver((Ï€) -> DDPG(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(A(), QSA()))
test_solver((Ï€) -> TD3(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(A(), DoubleNetwork(QSA(), QSA())))
test_solver((Ï€) -> SAC(Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(G(), DoubleNetwork(QSA(), QSA())))


# Continuous IL
ğ’Ÿ_expert = expert_trajectories = BSON.load("examples/il/expert_data/pendulum.bson")[:data]
D() = ContinuousNetwork(Chain(DenseSN(3, 32, relu), DenseSN(32, 1)))

test_solver((Ï€, D) -> GAIL(D=D, ğ’Ÿ_expert=ğ’Ÿ_expert, Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(G(), V()), QSA())
test_solver(Ï€ -> BC(Ï€=Ï€, ğ’Ÿ_expert=ğ’Ÿ_expert, S=S, opt=(epochs=1,)), continuous_mdp, A())
# NOTE: gradient penalty on the gpu only plays nicely with tanh, not relus in the discriminator?
test_solver((Ï€) -> AdVIL(ğ’Ÿ_expert=ğ’Ÿ_expert, Ï€=Ï€, S=S, a_opt=(epochs=1,) ), continuous_mdp, ActorCritic(A(), QSA()))
test_solver((Ï€) -> ValueDICE(ğ’Ÿ_expert=ğ’Ÿ_expert, Ï€=Ï€, S=S, N=N, Î”N=Î”N), continuous_mdp, ActorCritic(G(), QSA()))

